{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/mlp/image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgP2u3gKdVGo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from diffusers.utils import make_image_grid\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB7aI_8RjVuC"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "# Step 1 - inspect dataset\n",
        "ds = load_dataset(\"uoft-cs/cifar10\")\n",
        "\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "# Step 2 - Define transform\n",
        "validate_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # (H, W, C) PIL â†’ (C, H, W) float32 [0,1]\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(p=0.5),\n",
        "  transforms.RandomRotation(15),\n",
        "])\n",
        "\n",
        "# Step 3 - Wrap in dataset class\n",
        "def transform_batch(batch, transform):\n",
        "    imgs = [transform(img) for img in batch['img']]\n",
        "    labels = batch['label']\n",
        "    return {'pixel_values': imgs, 'labels': labels}\n",
        "\n",
        "\n",
        "ds_val = ds['test'].with_transform(partial(transform_batch, transform=validate_transform))\n",
        "ds_train = ds['train'].with_transform(partial(transform_batch, transform=validate_transform)) # Because we're creating a noisy dataset, doesn't make sense to add augs\n",
        "\n",
        "# Step 4 - Torch DataLoader\n",
        "train_loader = DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(ds_val, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8W2NMbPej28"
      },
      "outputs": [],
      "source": [
        "class NoisyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, original_dataset, feature_noise=0.0, label_noise=0.0, num_classes=10):\n",
        "        self.data = original_dataset\n",
        "        self.original_size = len(original_dataset)\n",
        "\n",
        "        # Label noise: which samples get random labels\n",
        "        self.noisy_labels = np.random.random(self.original_size) < label_noise\n",
        "        # NOTE: if you do 0, num_classes. the class distribution will match the existing distribution.\n",
        "        # But to make the test even harder, we will increase label noise as well as make the dataset more unbalanced\n",
        "        self.random_labels = np.random.randint(0, 4, self.original_size)\n",
        "\n",
        "        # Feature noise: duplicate some samples\n",
        "        num_dups = int(self.original_size * feature_noise)\n",
        "        self.dup_indices = np.random.choice(self.original_size, num_dups, replace=True) if num_dups > 0 else []\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.original_size + len(self.dup_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get original or duplicate sample\n",
        "        orig_idx = int(self.dup_indices[idx - self.original_size]) if idx >= self.original_size else idx\n",
        "        sample = self.data[orig_idx]\n",
        "\n",
        "        # Unpack\n",
        "        img = sample['pixel_values'] if isinstance(sample, dict) else sample[0]\n",
        "        label = sample['labels'] if isinstance(sample, dict) else sample[1]\n",
        "\n",
        "        # Apply noise\n",
        "        if idx >= self.original_size:  # Feature noise (duplicate)\n",
        "            img = img + torch.randn_like(img) * 0.01\n",
        "        elif self.noisy_labels[idx]:  # Label noise\n",
        "            label = int(self.random_labels[idx])\n",
        "\n",
        "        return {'pixel_values': img, 'labels': label} if isinstance(sample, dict) else (img, label)\n",
        "\n",
        "\n",
        "def inject_noise(dataloader, feature_noise: float, label_noise: float):\n",
        "    \"\"\"Inject label and feature noise into a DataLoader.\"\"\"\n",
        "    noisy_dataset = NoisyDataset(dataloader.dataset, feature_noise, label_noise)\n",
        "\n",
        "    return DataLoader(\n",
        "        noisy_dataset,\n",
        "        batch_size=dataloader.batch_size,\n",
        "        shuffle=isinstance(dataloader.sampler, torch.utils.data.sampler.RandomSampler),\n",
        "        num_workers=dataloader.num_workers,\n",
        "        pin_memory=dataloader.pin_memory,\n",
        "        drop_last=dataloader.drop_last\n",
        "    )\n",
        "\n",
        "# Usage:\n",
        "noisy_train_loader = inject_noise(train_loader, feature_noise=0.4, label_noise=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqEfSM28my9M"
      },
      "outputs": [],
      "source": [
        "def visualize_distribution(dataloader, title):\n",
        "   label_freq = torch.zeros(10)\n",
        "   image_mean = []\n",
        "   image_std = []\n",
        "   per_class_feature_distribution = torch.zeros(10)\n",
        "\n",
        "   for batch in dataloader:\n",
        "       imgs = batch['pixel_values']\n",
        "       labels = batch['labels']\n",
        "       label_freq += torch.bincount(labels, minlength=10)\n",
        "\n",
        "       for i in range(10):\n",
        "           idx = torch.where(labels == i)[0]\n",
        "           if len(idx) > 0:\n",
        "               samples = imgs[idx]\n",
        "               per_class_feature_distribution[i] += samples.mean()\n",
        "\n",
        "       batch_mean = imgs.mean(dim=(0, 2, 3))\n",
        "       batch_std = imgs.std(dim=(0, 2, 3))\n",
        "       image_mean.append(batch_mean)\n",
        "       image_std.append(batch_std)\n",
        "\n",
        "   per_class_feature_distribution /= len(dataloader)\n",
        "   image_mean = torch.vstack(image_mean).mean(0).numpy()\n",
        "   image_std = torch.vstack(image_std).mean(0).numpy()\n",
        "\n",
        "   fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "   fig.suptitle(title)\n",
        "\n",
        "   ax[0].bar([\"R\", \"G\", \"B\"], image_mean, yerr=image_std, capsize=5)\n",
        "   ax[0].set_title(\"Channel Distribution\")\n",
        "\n",
        "   ax[1].bar(np.arange(10), per_class_feature_distribution)\n",
        "   ax[1].set_title(\"Per class feature distribution\")\n",
        "\n",
        "   ax[2].bar(np.arange(10), label_freq)\n",
        "   ax[2].set_title(\"Label distribution\")\n",
        "\n",
        "   plt.tight_layout()\n",
        "   plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0I87z_Hh6cQ"
      },
      "outputs": [],
      "source": [
        "# def featurize(batch: torch.Tensor, out_size=8):\n",
        "#     # simple featurizer that just interpolates the image to a very small resolution 8 x 8\n",
        "#     gray = batch.mean(1, keepdim=True)\n",
        "#     small = F.interpolate(gray, size=(out_size, out_size), mode='bilinear', align_corners=False)\n",
        "#     feat  = small.flatten(1)\n",
        "#     return feat\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "resnet = models.resnet50(pretrained=True).to(device)\n",
        "resnet.eval()\n",
        "# Remove the final classification layer\n",
        "feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "def featurize(batch: torch.Tensor, out_size=None, mini_batch_size=32):\n",
        "    \"\"\"\n",
        "    Extract features using pretrained ResNet50 from torchvision\n",
        "    batch: (batch_size, 3, H, W) tensor with values in [0, 1]\n",
        "    returns: (batch_size, 2048) feature vectors\n",
        "    \"\"\"\n",
        "    batch_size = batch.shape[0]\n",
        "    features_list = []\n",
        "\n",
        "    # Normalize for ImageNet pretrained model\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(batch.device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(batch.device)\n",
        "    normalized = (batch - mean) / std\n",
        "\n",
        "    # Resize to 224x224 if needed (ResNet expects this size)\n",
        "    if batch.shape[-1] != 224:\n",
        "        normalized = F.interpolate(normalized, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "    # Process in mini-batches\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, batch_size, mini_batch_size):\n",
        "            mini_batch = normalized[i:i+mini_batch_size].to(device)\n",
        "            mini_features = feature_extractor(mini_batch)\n",
        "            mini_features = mini_features.squeeze(-1).squeeze(-1)  # Remove spatial dims\n",
        "            features_list.append(mini_features.cpu())  # Move to CPU to save GPU memory\n",
        "\n",
        "            # Clear CUDA cache after each mini-batch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Concatenate all features\n",
        "    features = torch.cat(features_list, dim=0).to(batch.device)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def distance(samples: torch.Tensor) -> torch.Tensor:\n",
        "  # samples = batch_size, d_embed\n",
        "  # Output: batch_size, batch_size\n",
        "  samples = samples / samples.norm(p=2, dim=-1, keepdim=True) # NOTE: we need to take the norm wrt to each row, i.e. norm(col)\n",
        "  out = samples @ samples.t()\n",
        "  eps = 1e-3\n",
        "  assert ((out >= -1 - eps) & (out <= 1 + eps)).all(), out # the bounds of cosine function are -1, 1\n",
        "  return out\n",
        "\n",
        "def batch_de_dup(batch, k: int, threshold: float, use_pbar: bool = False, can_vote: bool = False):\n",
        "  '''\n",
        "  This function will prune out all samples that are similar in feature space.\n",
        "  Algorithm:\n",
        "  1. Featurize dataset\n",
        "  2. Compute L2 norm - cosine similarity\n",
        "  3. Find all samples that are Ã© distance away from the centroid.\n",
        "  But for simplicity, we can just do it on a per sample basis - remove up to k nearest neighbors per sample\n",
        "  '''\n",
        "  images, labels = batch['pixel_values'], batch['labels']\n",
        "  embeddings = featurize(images) # (batch_size, d_embed)\n",
        "  indices_pruned = set()\n",
        "  batch_size = len(embeddings)\n",
        "\n",
        "  new_labels = labels.clone()\n",
        "\n",
        "  pbar = tqdm(range(batch_size)) if use_pbar else range(batch_size)\n",
        "  # Calculate the similarity matrix between each query and sample position\n",
        "  sim:torch.Tensor = distance(embeddings)  # (batch_size, batch_size)\n",
        "  sim.fill_diagonal_(-1)  # query with query should be non-sensical value since we do not want to return it from topk\n",
        "\n",
        "  labels_corrected = 0\n",
        "\n",
        "  for i in pbar:\n",
        "    if i in indices_pruned:\n",
        "      continue\n",
        "\n",
        "    # take top k samples and corresponding indices\n",
        "    val, idx = torch.topk(sim[i], min(k, batch_size - 1))\n",
        "\n",
        "    # 1. Apply de-duplication: reduces feature noise\n",
        "    # 2. Apply de-noise via majority voting: reduces label noise\n",
        "    # for (2), you'd need to look at all the labels for this batch and assign query's label to the majority\n",
        "    high_confident_sample_indices = idx[torch.where(val >= threshold)[0]]\n",
        "    if len(high_confident_sample_indices) == 0:\n",
        "      continue\n",
        "\n",
        "    if can_vote:\n",
        "      minibatch_labels = labels[torch.cat([high_confident_sample_indices, torch.tensor([i], device=idx.device, dtype=idx.dtype)])]\n",
        "      majority_label = torch.bincount(minibatch_labels.cpu()).argmax()\n",
        "      new_labels[i] = majority_label\n",
        "      labels_corrected += 1 * (new_labels[i] != labels[i])\n",
        "\n",
        "    # once you have the value and indices, the next step is to prune them out of the batch\n",
        "    for j in high_confident_sample_indices:\n",
        "      if j not in indices_pruned:\n",
        "        indices_pruned.add(j.item())\n",
        "\n",
        "  if indices_pruned:\n",
        "    indices_pruned = torch.tensor(np.array(list(indices_pruned)))\n",
        "    unique_idx_mask = ~torch.isin(torch.arange(batch_size), indices_pruned)\n",
        "  else:\n",
        "    unique_idx_mask = torch.ones(batch_size, dtype=torch.bool)\n",
        "\n",
        "  unique_idx = torch.where(unique_idx_mask)[0]\n",
        "\n",
        "  return {'pixel_values': images[unique_idx], 'labels': new_labels[unique_idx], 'labels_corrected': labels_corrected}\n",
        "\n",
        "\n",
        "def de_dup_across_all_batches(dataloader, k: int, threshold: float):\n",
        "  # we need to create some sort of a heap to ensure that we're only taking values that are the optimal\n",
        "  global_batch = {'pixel_values': [], 'labels': []}\n",
        "  total_labels_corrected = 0\n",
        "  for batch in tqdm(dataloader):\n",
        "    pruned_batch = batch_de_dup(batch, k, threshold, can_vote=False)\n",
        "    total_labels_corrected += pruned_batch['labels_corrected']\n",
        "    global_batch['pixel_values'].append(pruned_batch['pixel_values'])\n",
        "    global_batch['labels'].append(pruned_batch['labels'])\n",
        "\n",
        "  global_batch['pixel_values'] = torch.cat(global_batch['pixel_values'], dim=0)\n",
        "  global_batch['labels'] = torch.cat(global_batch['labels'], dim=0)\n",
        "\n",
        "  print(f\"Total labels corrected (first stage): {total_labels_corrected} / {len(dataloader.dataset)}\")\n",
        "\n",
        "  global_dataset = batch_de_dup(global_batch, k, threshold, use_pbar=True, can_vote=True)\n",
        "  total_labels_corrected += global_dataset['labels_corrected']\n",
        "\n",
        "  print(f\"Total labels corrected (second stage): {total_labels_corrected} / {len(dataloader.dataset)}\\nFinal dataset size: {len(global_dataset['labels'])}\")\n",
        "  hf_dataset = Dataset.from_dict({\n",
        "      \"pixel_values\": global_dataset[\"pixel_values\"].cpu().numpy(),\n",
        "      \"labels\": global_dataset[\"labels\"].cpu().numpy()\n",
        "  })\n",
        "  hf_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
        "  loader = DataLoader(hf_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "  return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGA6VeCi2CqP"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "clean_dataloader = de_dup_across_all_batches(noisy_train_loader, k=5, threshold=0.98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WKXjSaJrNmK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# visualize_distribution(train_loader, \"Original Train Set\")\n",
        "visualize_distribution(clean_dataloader, \"Pruned out noisy set\")\n",
        "visualize_distribution(noisy_train_loader, \"Synthetic Noisy Train Set\")\n",
        "# visualize_distribution(test_loader, \"Original Test Set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mM0e7xkmYM9"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1), # 32 x 32 -> 32 x 32\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2), # 32 x 32 -> 16 x 16\n",
        "\n",
        "        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # 16 x 16 -> 16 x 16\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2), # 16 x 16 -> 8 x 8\n",
        "\n",
        "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 8 x 8 -> 8 x 8\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2) # 8 x 8 -> 4 x 4\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),  # output shape: (N, 64, 1, 1)\n",
        "        nn.Flatten(), # (N, 64)\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, num_classes) # logits\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.feature_extractor(x)\n",
        "    logits = self.classifier(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VQXU-Z9tvMq"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(dataloader, model, criterion, optimizer):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  for batch in dataloader:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    imgs = batch['pixel_values'].to('cuda') if isinstance(batch, dict) else batch[0].to('cuda')\n",
        "    labels = batch['labels'].to('cuda') if isinstance(batch, dict) else batch[1].to('cuda')\n",
        "    logits = model(imgs)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader):\n",
        "  model.eval()\n",
        "  scores = []\n",
        "  all_pred, all_true = [], []\n",
        "  for batch in dataloader:\n",
        "      imgs = batch[\"pixel_values\"].to('cuda') if isinstance(batch, dict) else batch[0].to('cuda')\n",
        "      labels = batch[\"labels\"] if isinstance(batch, dict) else batch[1]\n",
        "      with torch.no_grad():\n",
        "        preds = torch.softmax(model(imgs), dim=-1).argmax(dim=-1)\n",
        "      all_pred.append(preds.cpu())\n",
        "      all_true.append(labels.cpu())\n",
        "  y_pred = torch.cat(all_pred).numpy()\n",
        "  y_true = torch.cat(all_true).numpy()\n",
        "  cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "  return metrics.f1_score(y_true, y_pred, average=\"macro\"), cm\n",
        "\n",
        "\n",
        "def calibration(logits, y_true, bin_count: int = 10):\n",
        "  '''\n",
        "  Implements ECE calibration technique\n",
        "  Algorithm:\n",
        "  1. Compute confidence (probs) and correct predictions for each item in the batch\n",
        "  2. Create n bins to discritize your model predictions into them.\n",
        "  Every bin represents a confidence bound, i.e. how many correct predictions were in that bin?\n",
        "  3. For each bin, compute:\n",
        "   1) Accuracy - proportion of correct predictions in that bin\n",
        "   2) Conf - average confidence for that bin\n",
        "  4. Apply the ECE formula for each bin: |conf - acc| * bin_size / total_size\n",
        "  The total sum represents the error, expected calibration error.\n",
        "  '''\n",
        "  probs = torch.softmax(logits, dim=-1)\n",
        "  conf, preds = probs.max(dim=1) # torch.max returns both the argmax and the value corresponding to that. output shape = batch_size\n",
        "\n",
        "  correct = 1 * (y_true == preds)\n",
        "\n",
        "  bins = torch.linspace(0, 1, bin_count + 1)\n",
        "\n",
        "  total_ece = 0.0\n",
        "  individual_confidence_in_bin = []\n",
        "  individual_accuracy_in_bin = []\n",
        "\n",
        "  for i in range(bin_count):\n",
        "    mask = (conf >= bins[i]) & (conf < bins[i + 1])\n",
        "\n",
        "    if mask.sum() == 0: # no predictions in bin\n",
        "      individual_accuracy_in_bin.append(0.0)\n",
        "      individual_confidence_in_bin.append(0.0)\n",
        "      continue\n",
        "\n",
        "    average_accuracy_in_bin = correct[mask].mean()\n",
        "    average_confidence_in_bin = conf[mask].mean()\n",
        "\n",
        "    # Update statistics\n",
        "    # ECE = abs(bin accuracy - bin confidence) * (num elements in bin / batch_size)\n",
        "    bin_ece = torch.abs(average_accuracy_in_bin - average_confidence_in_bin) * mask.sum().item() / len(conf)\n",
        "    individual_accuracy_in_bin.append(average_accuracy_in_bin)\n",
        "    individual_confidence_in_bin.append(average_confidence_in_bin)\n",
        "    total_ece += bin_ece\n",
        "\n",
        "  return total_ece, individual_accuracy_in_bin, individual_confidence_in_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igg4diNfvoN7"
      },
      "outputs": [],
      "source": [
        "clean_model = SimpleCNN(10).to('cuda')\n",
        "clean_optimizer = optim.AdamW(clean_model.parameters(), lr=1e-3)\n",
        "\n",
        "noisy_model = SimpleCNN(10).to('cuda')\n",
        "noisy_optimizer = optim.AdamW(noisy_model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # true class gets 0.9 while all the other classes get 0.1 / 10 = 0.011. prevents from overconfident predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzFzVcQjtUWy"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "NUM_EPOCHS = 50\n",
        "results = {\"noisy_loss\": [], \"clean_loss\": [], \"noisy_f1\": [], \"clean_f1\": []}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    noisy_loss = train_one_epoch(noisy_train_loader, noisy_model, criterion, noisy_optimizer)\n",
        "    clean_loss = train_one_epoch(clean_dataloader, clean_model, criterion, clean_optimizer)\n",
        "\n",
        "    noisy_f1 = validate(noisy_model, test_loader)[0]\n",
        "    clean_f1 = validate(clean_model, test_loader)[0]\n",
        "\n",
        "    results[\"noisy_loss\"].append(noisy_loss)\n",
        "    results[\"clean_loss\"].append(clean_loss)\n",
        "    results[\"noisy_f1\"].append(noisy_f1)\n",
        "    results[\"clean_f1\"].append(clean_f1)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] \"\n",
        "          f\"Noisy Loss: {noisy_loss:.3f}, F1: {noisy_f1:.3f} | \"\n",
        "          f\"Clean Loss: {clean_loss:.3f}, F1: {clean_f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sayjn0M3HYAE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Loss subplot\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(results[\"clean_loss\"], label=\"Clean\", marker='o')\n",
        "plt.plot(results[\"noisy_loss\"], label=\"Noisy\", marker='o')\n",
        "plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(alpha=0.3)\n",
        "\n",
        "# F1 subplot\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(results[\"clean_f1\"], label=\"Clean\", marker='o')\n",
        "plt.plot(results[\"noisy_f1\"], label=\"Noisy\", marker='o')\n",
        "plt.title(\"F1 Score\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"F1\"); plt.legend(); plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6jjwxnoywcd"
      },
      "outputs": [],
      "source": [
        "# Validation\n",
        "\n",
        "# Build a confusion matrix to understand which class is performing the best\n",
        "\n",
        "def visualize_outputs(model, dataloader):\n",
        "  cm = torch.tensor(validate(model, train_loader)[1])\n",
        "  # Per class accuracy\n",
        "  import seaborn as sns\n",
        "  plt.title(\"Confusion Matrix visualized\")\n",
        "  plt.xlabel(\"Predicted Class\")\n",
        "  plt.ylabel(\"True Class\")\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Per class accuracy\")\n",
        "  plt.bar(np.arange(10), (cm.diag() / cm.sum(dim=1)).numpy())\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2xneErZ7zeB"
      },
      "outputs": [],
      "source": [
        "# Training output visualization\n",
        "%%time\n",
        "\n",
        "print(\"CLEAN STATS\")\n",
        "visualize_outputs(clean_model, clean_dataloader)\n",
        "visualize_outputs(clean_model, test_loader)\n",
        "\n",
        "print(\"NOISY STATS\")\n",
        "visualize_outputs(noisy_model, noisy_train_loader)\n",
        "visualize_outputs(noisy_model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyNnRzaWGEeTyg2Jsc1qWkgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}