{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa2aXDZJyXKyQmiuS5u3zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/transformer_debugging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crack these 10 transfromer debugging exercises and you shall pass any transformer debugging coding interview.\n",
        "# Note: Each block, once ran will error out. Try not to look at the solution (commented out)\n",
        "\n",
        "# Goal: Solve each problem in <10m\n",
        "\n",
        "# Bug tier:\n",
        "# Tier 1 - runtime bugs\n",
        "# Tier 2 - structural omissions, not runtime but missing features in a GPT-like model. Includes best-practices\n"
      ],
      "metadata": {
        "id": "nV091gaUmD8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "HP-B_UWvmQOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIgOq3VIl-f8"
      },
      "outputs": [],
      "source": [
        "# Problem 1 (Easy)\n",
        "# Tier 1 errors: 2\n",
        "# Tier 2 errors: 3\n",
        "# Time taken: 5m\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.q_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = embed_dim ** 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.q_lin(x)\n",
        "        K = self.k_lin(x)\n",
        "        V = self.v_lin(x)\n",
        "        # scores = torch.matmul(Q, K) # Error 1\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) # Solution 1\n",
        "        attn = F.softmax(scores / self.scale, dim=-1)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.attn = SimpleSelfAttention(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.attn(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 2\n",
        "embed_dim = 32\n",
        "batch_size = 8\n",
        "seq_len = 10\n",
        "model = SimpleDecoder(vocab_size, embed_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "for epoch in range(2):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(inp)\n",
        "    # loss = criterion(output.view(-1, vocab_size), inp.view(-1)) # Error 2\n",
        "    loss = criterion(output[:, :-1, :].contiguous().view(-1, vocab_size), inp[:, 1:].contiguous().view(-1)) # Solution 2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Tier 2 errors:\n",
        "# 1. Missing causal mask\n",
        "# 2. Missing positional encodings\n",
        "# 3. Missing Layernorm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2 (Easy)\n",
        "# Tier 1 errors: 2\n",
        "# Tier 2 errors: 3\n",
        "# Time taken: 8m 12s\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, f\"{embed_dim} needs to be divisible by {num_heads}\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = embed_dim // num_heads\n",
        "        self.q_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.shape\n",
        "        Q = self.q_lin(x).view(B, T, self.num_heads, self.d_k) # b, t, h, d_k\n",
        "        K = self.k_lin(x).view(B, T, self.num_heads, self.d_k)\n",
        "        V = self.v_lin(x).view(B, T, self.num_heads, self.d_k)\n",
        "\n",
        "        Q = Q.transpose(1, 2)  # (B, H, T, d_k)\n",
        "        K = K.transpose(1, 2)  # (B, H, T, d_k)\n",
        "        V = V.transpose(1, 2)  # (B, H, T, d_k)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) # (b, h, t, d_k) x (b, h, d_k, t) = b, h, tq, tk\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn, V) # b, h, tq, d_k\n",
        "\n",
        "        context = context.transpose(1, 2).contiguous().view(B, T, E) # b, tq, h, d_k -> b, tq, E=h*d_k\n",
        "        return self.fc(context)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, heads):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, heads)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.attn(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 1000\n",
        "embed_dim = 32\n",
        "heads = 6 # this isn't even divisible! Change to 4 (Error 1)\n",
        "# heads = 4 # Solution 1\n",
        "model = Decoder(vocab_size, embed_dim, heads)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 4\n",
        "seq_len = 10\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(inp)\n",
        "    loss = criterion(out.view(-1, vocab_size), inp.contiguous().view(-1)) # Error 2\n",
        "    # loss = criterion(out[:, :-1, :].contiguous().view(-1, vocab_size), inp[:, 1:].contiguous().view(-1)) # Solution 2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Tier 2 Errors:\n",
        "# 1. Missing Causal mask\n",
        "# 2. Missing positional encodings\n",
        "# 3. Missing Layernorm"
      ],
      "metadata": {
        "id": "B3vkxT0JnwPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3 (Medium)\n",
        "# Tier 1 bugs - 3\n",
        "# Tier 2 bugs - 2\n",
        "# Time taken: 10m\n",
        "\n",
        "class PositionalDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len)) # Error 1\n",
        "        # Okay, so the error is that positional embeddings is just zeros everywhere\n",
        "        # [1, max_seq_len] = pos_emb\n",
        "        # self.pos_emb = nn.Parameter(torch.zeros(1, max_len)) # Error 1\n",
        "        # Solution #1 -> Learned positional encodings\n",
        "        # self.pos_emb = nn.Embedding(max_len, embed_dim) # Learned positional embeddings\n",
        "\n",
        "        # Solution #2 -> Fixed positional embeddings\n",
        "        # div_term = 1/(10_000 ** (torch.arange(0, embed_dim // 2) / embed_dim)).unsqueeze(0) # [1, embed_dim/2]\n",
        "        # positions = torch.zeros(max_len, embed_dim)\n",
        "        # positions[:, 0::2] = torch.sin(torch.arange(max_len)[:, None] * div_term)\n",
        "        # positions[:, 1::2] = torch.cos(torch.arange(max_len)[:, None] * div_term)\n",
        "        # self.register_buffer('pos_emb', positions)\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = x + self.pos_emb[:, :x.size(1)] # Error 2\n",
        "        # x = x + self.pos_emb(torch.arange(0, x.size(1))) # Solution 2 (using Learned positional encodings)\n",
        "        # x = x + self.pos_emb[None, :x.size(1), :] # Solution 2 (using Fixed positional encodings)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 500\n",
        "embed_dim = 32\n",
        "max_len = 50\n",
        "model = PositionalDecoder(vocab_size, embed_dim, max_len)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 5\n",
        "seq_len = 20\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(inp)\n",
        "    loss = criterion(out.view(-1, vocab_size), inp.view(-1)) # Error 3\n",
        "    # loss = criterion(out[:, :-1, :].contiguous().view(-1, vocab_size), inp[:, 1:].contiguous().view(-1)) # Solution 3\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Tier 2 Bugs:\n",
        "# 1. Missing causal mask\n",
        "# 2. Missing Layernorm"
      ],
      "metadata": {
        "id": "j52LaP-Ks_xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4 (Easy)\n",
        "# Tier 1 bugs - 1\n",
        "# Time taken: 4m 38s\n",
        "\n",
        "class SimpleDecoder2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 100\n",
        "embed_dim = 16\n",
        "model = SimpleDecoder2(vocab_size, embed_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 4\n",
        "seq_len = 10\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "for epoch in range(2):\n",
        "    optimizer.zero_grad()\n",
        "    # out = model(inp)\n",
        "    # loss = criterion(out, inp) # Error 1\n",
        "    # Solution 1\n",
        "    # loss = criterion(out[:, 1:].contiguous().view(-1, vocab_size), inp[:, 1:].contiguous().view(-1)) # Error 1 -> target needs to be a 1-d tensor, flatten out batch_size x seq_len\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "yUKrJ6Ij1VEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 5 (Easy)\n",
        "# Tier 1 bugs - 1\n",
        "# Time taken: 2m 56s\n",
        "\n",
        "class TeacherForcingDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 200\n",
        "embed_dim = 32\n",
        "model = TeacherForcingDecoder(vocab_size, embed_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 3\n",
        "seq_len = 7\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(inp) # b, m, v\n",
        "    loss = criterion(out.view(-1, vocab_size), inp[:,1:].view(-1)) # Error 1\n",
        "    # loss = criterion(out[:, :-1, :].contiguous().view(-1, vocab_size), inp[:, 1:].contiguous().view(-1)) # Solution 1\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "gwK9h-An7sFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 7 (Easy)\n",
        "\n",
        "# Tier 1 bugs - 2\n",
        "# Time taken - 2m 18s\n",
        "\n",
        "class MismatchDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Embedding(vocab_size, hidden_size - 2) # Error 1\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size) # Solution\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 500\n",
        "hidden_size = 20\n",
        "model = MismatchDecoder(vocab_size, hidden_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "for epoch in range(2):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(inp)\n",
        "    loss = criterion(out.view(-1, vocab_size), inp.view(-1)) # Error 2\n",
        "    # Solution 2\n",
        "    # out = model(inp[:, :-1])\n",
        "    # loss = criterion(out.view(-1, vocab_size), inp[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "ONUjB4aO9uQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 8 (Medium)\n",
        "# Tier 1 bugs - 2\n",
        "# Tier 2 bugs - 2\n",
        "# Time taken - 15m\n",
        "\n",
        "class SlidingWindowDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, window_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def forward_buggy(self, x):\n",
        "        B, T = x.shape\n",
        "        x = self.embed(x)\n",
        "        pad = self.window_size // 2\n",
        "        padded = torch.cat([x.new_zeros(B, pad, x.size(2)), x, x.new_zeros(B, pad, x.size(2))], dim=1)\n",
        "        windows = padded.unfold(1, self.window_size, 1)   # (B, T, window, D)\n",
        "\n",
        "        Q = self.q(windows)\n",
        "        K = self.k(windows)\n",
        "        V = self.v(windows)\n",
        "\n",
        "        attn_scores = torch.einsum('b i j d, b i k d -> b i j k', Q, K)\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "        context = torch.einsum('b i j k, b i k d -> b i j d', attn, V)\n",
        "        return self.fc(context)\n",
        "\n",
        "    def forward_correct(self, x):\n",
        "        # Correct implementation\n",
        "        # The original buggy version had incorrectly applied causal mask with local attention. Much easier to just build a normal causal mask from scratch\n",
        "        # that encodes for a fixed window\n",
        "        B, T = x.shape\n",
        "        x = self.embed(x) # b, m, d\n",
        "        pad = self.window_size // 2 # 1 - causal attention with a window\n",
        "        # 1- b, 1, d\n",
        "        # 2- b, m, d\n",
        "        # 2- b, 1, d\n",
        "        # final = b, m+2, d = b, 7, d\n",
        "\n",
        "        # x.shape = b, m, d = 2, 5, 16\n",
        "        # windows.shape = b, 7\n",
        "        # padded.shape = b\n",
        "        # Error 2 - Should make this into a mask instead of directly passing into the model\n",
        "        Q = self.q(x)\n",
        "        K = self.k(x)\n",
        "        V = self.v(x)\n",
        "\n",
        "        attn_scores = torch.einsum('b q d, b k d -> b q k', Q, K)\n",
        "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool() | torch.tril(torch.ones(T, T), diagonal=-(pad)).bool()\n",
        "        attn_scores.masked_fill_(mask, float('-inf'))\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "        context = torch.einsum('b q k, b k d -> b q d', attn, V)\n",
        "        return self.fc(context)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_correct(x)\n",
        "\n",
        "vocab_size = 1000\n",
        "embed_dim = 16\n",
        "window_size = 3\n",
        "model = SlidingWindowDecoder(vocab_size, embed_dim, window_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "for epoch in range(2):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(inp)\n",
        "    # loss = criterion(out.view(-1, vocab_size), inp.view(-1)) # Error 1\n",
        "    loss = criterion(out[:, :-1].contiguous().view(-1, vocab_size), inp[:, 1:].contiguous().view(-1)) # Solution 1\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Tier 2 bugs\n",
        "# 1. No positional encoding\n",
        "# 2. No LayerNorm"
      ],
      "metadata": {
        "id": "vnIehp9Y-mTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 9 (Easy)\n",
        "# Tier 1 bugs - 1\n",
        "\n",
        "# Time Taken: 2m 2s\n",
        "\n",
        "class DetachDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "vocab_size = 150\n",
        "embed_dim = 16\n",
        "model = DetachDecoder(vocab_size, embed_dim)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # fine to use SGD, but better to use AdamW to make use of modentum and velocity parameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 4\n",
        "seq_len = 6\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    # out = model(inp)\n",
        "    # loss = criterion(out.view(-1, vocab_size), inp.view(-1))\n",
        "    out = model(inp[:, :-1])\n",
        "    loss = criterion(out.view(-1, vocab_size), inp[:, :-1].contiguous().view(-1))\n",
        "    # loss_det = loss.detach() # hmm this will get rid of gradients for the loss which we need for gradient flow. Error 1\n",
        "    loss_det = loss # Solution 1. don't remove gradients from computational graph\n",
        "    loss_det.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "FjZGKEVJG1nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 10 (Medium)\n",
        "# Tier 1 bugs - 2\n",
        "# Tier 2 bugs - 3\n",
        "# Time Taken: 5m\n",
        "\n",
        "class GlobalQueryDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward_buggy(self, x):\n",
        "        x = self.embed(x)\n",
        "        global_q = self.q(x[:, :1, :])            # (B, 1, D)\n",
        "        q = self.q(x)                             # (B, T, D)\n",
        "        k = self.k(x)                             # (B, T, D)\n",
        "        v = self.v(x)                             # (B, T, D)\n",
        "\n",
        "        global_scores = torch.matmul(global_q, k.transpose(-2, -1))  # (B, 1, T)\n",
        "        local_scores = torch.matmul(q, k.transpose(-2, -1))          # (B, T, T)\n",
        "\n",
        "        scores = F.softmax(global_scores + local_scores, dim=-1)     # (??)\n",
        "        context = torch.matmul(scores, v)\n",
        "        return self.fc(context)\n",
        "\n",
        "    def forward_correct(self, x):\n",
        "        # Problem: we're broadcasting across the key dimension which means we're passing the same bias across the query dimension\n",
        "        # Solution: Broadcast across the key dimension. Better to build a full-attention mask\n",
        "        x = self.embed(x)\n",
        "        b, m, d = x.shape\n",
        "\n",
        "        global_q = self.q(x[:, :1, :]).expand(-1, m, -1) # (B, T, D)\n",
        "        q = self.q(x)                             # (B, T, D)\n",
        "        k = self.k(x)                             # (B, T, D)\n",
        "        v = self.v(x)                             # (B, T, D)\n",
        "\n",
        "        global_scores = torch.matmul(global_q, k.transpose(-2, -1))  # (B, T, T)\n",
        "        local_scores = torch.matmul(q, k.transpose(-2, -1))          # (B, T, T)\n",
        "\n",
        "        scores = F.softmax(global_scores/(d**0.5) + local_scores/(d**0.5), dim=-1)\n",
        "        context = torch.matmul(scores, v)\n",
        "        return self.fc(context)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.forward_correct(x)\n",
        "\n",
        "vocab_size = 300\n",
        "embed_dim = 16\n",
        "model = GlobalQueryDecoder(vocab_size, embed_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "inp = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "for epoch in range(2):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(inp)\n",
        "    # loss = criterion(out.view(-1, vocab_size), inp.view(-1)) # Error 2\n",
        "    loss = criterion(out[:, 1:].contiguous().view(-1, vocab_size), inp[:, :-1].contiguous().view(-1)) # Solution 2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# Tier 2 bugs:\n",
        "# 1. Missing LayerNorm\n",
        "# 2. Missing CausalMask\n",
        "# 3. Missing positional encoding"
      ],
      "metadata": {
        "id": "nmPxm9M3Kmq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}