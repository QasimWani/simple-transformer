{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU9qMenHvZkGD9z5CC/zqg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/postional_encodings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjH7_Cp-bmaF"
      },
      "outputs": [],
      "source": [
        "# Implements different types of positional encoding strategies:\n",
        "\n",
        "# 1. Fixed encodings - Encodes sinusoidal positions\n",
        "# 2. Learned embeddings - Learns positions\n",
        "# 3. RoPE embeddings - Encodes absolute and relative positions through rotation matrices\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedPositionalEncodings(nn.Module):\n",
        "  '''\n",
        "  Fixed positions were introduced in Attention Is All you Need paper.\n",
        "\n",
        "  Unlike RNNs and LSTMs where relative positions are baked into model architecture, a transformer\n",
        "  has to learn that token1 is next to token 2. If we do not apply positional encodings into the network,\n",
        "  the model learns a bag of words representation where it loses all the order/positional info. See LDA and TD-IDF.\n",
        "  1. NLP Classification: https://github.com/QasimWani/Classification/blob/master/Xenophobia/xenophobia%20classifier.ipynb\n",
        "  2. HitcHiqe: https://github.com/QasimWani/hitchHiqe/blob/121fa21390a41d7e71c58d1f8002b1b7cefa0c2e/middleware/recommender.js#L294\n",
        "\n",
        "  We want the model to learn the relative positions and have semantically different otuputs between [cat, sat, on, the, mat] and [sat, cat, on, the, mat]\n",
        "\n",
        "  This is where positional encodings come into play. Fixed positional encodings compute frequencies to control the oscillation of different tokens and corresponding elements\n",
        "  in the embedding (d_embed) for the model to learn the relative positions. A faster oscillation\n",
        "\n",
        "  Design choice #1 - apply a sin/cos function to encode for relative position. Tokens that are next to one another will be closer in representation, and tokens that are farther apart might be farther. Range [-1, 1]\n",
        "  Problem with this is that there will be a lot of repetition across the embedding dimension. In fact, every single value in d_embed will be equal so the model doesn't have enough info to learn and turns into a low-rank representation.\n",
        "\n",
        "  Design choice #2 - instead of just applying a single sin/cos, we apply sine function for even positions in d_embed and cosine function for odd positions in d_embed.\n",
        "  While this does help with reduced repetition, 50% of all values are still identical since the cos/sin is a function of range(0, max_len) and for each element in seq_len (token id) the sin/cos value is the same across d_embed.\n",
        "\n",
        "  Design choice #3 - control the oscillation. To further let the model learn relative positions and reduce repetition across the embedding dimension, d_embed, let's use a scaling factor to change how fast sine and cosine function oscillates across d_embed.\n",
        "  Scaling factor: e^(idx * (- log(10k) / d_embed)), where idx in range(0, d_embed, step=2).\n",
        "  This is equivalent to 10k ^ (-idx / d_embed), where idx is in range(0, d_embed, step=2). Law of exponents\n",
        "  Note: we need to skip half of all postions because sin/cos is applied to 50% of the data individually.\n",
        "\n",
        "  In the earlier positions of d_embed the oscillation is pretty high, exp(-0 * log(10k) / d_embed) >>  exp(-766 * log(10k) / d_embed).\n",
        "  You can think of it as the initial positions encode fine-grained position info, while the latter half might encode more general/coarse positional info.\n",
        "\n",
        "\n",
        "  Pros:\n",
        "  1. Encoding both relative and absolute positions to tokens is how we're able to solve the bag of words problem.\n",
        "  2. Because it's not learned and is fixed, it extrapolates by design\n",
        "  3. No parameters to learn. Fixed based on max_seq_len and d_embed\n",
        "  4. Bounded values [-1, 1] prevents gradient overflows\n",
        "\n",
        "  Cons:\n",
        "  1. Transformer model needs to learn sin/cos relationships to be able to understand the relative encodings. Harder task than just baking it in\n",
        "  somehow (see RoPE). For example, if a sentence is shuffled yet contains the same semantic information, the positional encodings will be different.\n",
        "  In particular if you have 'I ate an apple by the bench', this will have vastly different positional encodings than 'by the bench I ate an apple'.\n",
        "  Being able to bake in both absolute and relative positions is important, but at the same time we should not let the model learn complex sin/cos\n",
        "  relationships in the original embedding vector since it complicates learning.\n",
        "  2. Way less interpretable than learned embeddings or geometric approaches like RoPE.\n",
        "  3. Only applied once per input. Further attention layers don't have positional information baked in d_embed\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_len, d_embed):\n",
        "    super().__init__()\n",
        "    assert d_embed % 2 == 0, f\"Embedding dimension needs to be even, other sin/cos will cause dim mismatch\"\n",
        "\n",
        "    positional_encodings = torch.zeros(max_len, d_embed) # (max_len, d_embed)\n",
        "    positions = torch.arange(0, max_len).unsqueeze(1).float() # (max_len, 1)\n",
        "\n",
        "    div_term = torch.exp(torch.arange(0, d_embed, 2) * (-math.log(1e4) / d_embed)) # equivalent to: 1 / 1e4 ** (-torch.arange(0, d_embed, 2) / d_embed)\n",
        "\n",
        "    positional_encodings[:, 0::2] = torch.sin(positions * div_term) # (max_len, d_embed)\n",
        "    positional_encodings[:, 1::2] = torch.cos(positions * div_term) # (max_len, d_embed)\n",
        "\n",
        "    self.register_buffer('positional_encodings', positional_encodings)\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    x.shape = batch_size, seq_len, d_embed\n",
        "    '''\n",
        "    seq_len = x.size(1)\n",
        "    return x + self.positional_encodings[:seq_len, :].unsqueeze(0) # Note: seq_len <= max_len. To prevent mismatch errors, we should truncate the positional encodings to only use the value up until current seq_len that came from the input\n"
      ],
      "metadata": {
        "id": "oSFvRJVbgOkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnedPositionalEncodings(nn.Module):\n",
        "  '''\n",
        "  Instead of fixing positional encodings, why not just learn it? Simple weight matrix (no bias, hence embedding)\n",
        "  to learn the encodings over training.\n",
        "\n",
        "  Pros:\n",
        "  1. Super simple\n",
        "  2. May learn flexible encodings that may not be baked in by some feature engineered design.\n",
        "  3. Fine for models with small context lengths\n",
        "\n",
        "  Cons:\n",
        "  1. Parameter count can be massive - max_len x d_embed is huge if large context window.\n",
        "  2. No extrapolation. Model can't learn positional encodings beyond max_len.\n",
        "  2. No inductive bias - the model has to learn this representation from scratch. Will learn to memorize data statistics, prone to data noise/imbalance\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_len, d_embed):\n",
        "    super().__init__()\n",
        "\n",
        "    self.positional_encodings = nn.Embedding(max_len, d_embed)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "    positions = torch.arange(0, seq_len, device=x.device)\n",
        "    return x + self.positional_encodings(positions).unsqueeze(0)"
      ],
      "metadata": {
        "id": "ur4rK29Tpwhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbeddings(nn.Module):\n",
        "  '''\n",
        "  One of the major challenges with Fixed encoding like the one proposed in Attention Is All you Need is that the model needs to learn trig functions to truly\n",
        "  understand the positional relationship. For example, 'I ate an apple by the bench' will have vastly different positional encodings than\n",
        "  'by the bench I ate an apple' and the model needs to understand why.\n",
        "\n",
        "  To solve for this we make use of a rotation matrix where the dot product between two vectors, query and key preserve the distance between tokens m and n.\n",
        "  A rotation matrix is defined as:\n",
        "  [cos -sin]\n",
        "  [sin  cos]\n",
        "\n",
        "  This comes from polar coordinates and making use of the fact that in a unit circle, hypotenuse is 1, which means that to transform a vector [1, 0] (x0, x1)\n",
        "  in the x1 direction by some angle, theta, we need to transform it by cos in the x0 direction and sin in the x1. In other words, T([1, 0]) = [cos theta, sin theta].\n",
        "  Similarly, to transform a vector [0, 1] (x0, x1) by some angle, theta, we get T([0, 1]) = [-sin theta, cos theta].\n",
        "\n",
        "  One nice property of this rotation matrix is that it doesn't change the values between two vectors since it's relative. So we can encode relative positions\n",
        "  into our vectors implicitly. Now, we want to also bake in absolute positional information. This is done by simply scaling the theta by the position in seq_len.\n",
        "  identical to `FixedPositionalEncodings`.\n",
        "\n",
        "  An important distinction between RoPE and other positional encoding techniques is that we're applying it directly to Q and K projections so we can bake in the\n",
        "  relative property, otherwise we learn just the absolute positional information and is not that much different than the `FixedPositionalEncodings` formulation.\n",
        "\n",
        "  Formula: Attention_m,n = (X_m W_q R_theta^m) @ (X_n W_k R_theta^n).T, where m and n are two positions\n",
        "\n",
        "  Pros:\n",
        "  1. Relative position emerges directly from dot product (no learning trig identities needed)\n",
        "  2. Applied at every attention layer, not just the first/pre-processing. This means that each layer of the network has positional info baked in.\n",
        "  3. No parameters to learn\n",
        "  4. Extrapolates to longer sequences by continuing to rotate by position index and frequency term\n",
        "\n",
        "  Cons:\n",
        "  1. Complex, especially the implementation - there's ones that make use of half rotation to get rid of sparsity in the `rotation_matrix`\n",
        "  2. Frequency needs to be tuned for super long context length. NTK and position interpolation help\n",
        "\n",
        "  '''\n",
        "  def __init__(self, max_len, d_embed):\n",
        "    super().__init__()\n",
        "    # earlier positions in d_embed receive higher frequency while later positions receive lower frequency, meaning they move much slower\n",
        "    # there are ways of increasing context window at inference through NTK and position interpolation constant scaling parameter (tr_seq_len / ts_seq_len)\n",
        "    freqs = 1.0 / (1e4 ** (torch.arange(0, d_embed, 2).float() / d_embed)) # d_embed / 2\n",
        "\n",
        "    self.register_buffer('frequency', freqs)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    x.shape = batch_size, seq_len, d_embed\n",
        "    '''\n",
        "    # NOTE: Phase dictates positions in seq_len while frequence dictates rate of oscillation in d_embed\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "\n",
        "    x = x.reshape(batch_size, seq_len, d_embed // 2, 2) # first component of last dimension will be scaled with cos (x0) and second component will be scaled with sin (x1)\n",
        "\n",
        "    positions = torch.arange(0, seq_len, device=x.device).unsqueeze(-1) # seq_len, 1\n",
        "    angles = positions * self.freqs # seq_len, d_embed / 2\n",
        "\n",
        "    # Create rotation matrix components\n",
        "    # [cos -sin] [x0]   [x0 * cos  -  x1 * sin]\n",
        "    # [sin  cos] [x1] = [x0 * sin  +  x1 * cos]\n",
        "    cos = torch.cos(angles).reshape(1, seq_len, d_embed // 2) # 1, seq_len, d_embed / 2\n",
        "    sin = torch.sin(angles).reshape(1, seq_len, d_embed // 2) # 1, seq_len, d_embed / 2\n",
        "\n",
        "    x_rotated = torch.stack([\n",
        "        x[..., 0] * cos - x[..., 1] * sin,\n",
        "        x[..., 0] * sin + x[..., 1] * cos\n",
        "    ], dim=-1) # batch_size, seq_len, d_embed / 2, 2\n",
        "\n",
        "    return x_rotated.view(batch_size, seq_len, d_embed)\n",
        "\n",
        "\n",
        "# usage - apply to q and k, not to embeddings directly!\n",
        "def apply_rope(Q, K, max_len, d_embed):\n",
        "  rope = RotaryPositionalEmbeddings(max_len, d_embed) # NOTE: in production, initialize just once\n",
        "  Q_rot = rope(Q)\n",
        "  K_rot = rope(K)\n",
        "  # attention uses rotated q,k\n",
        "  scores = torch.matmul(Q_rot, K_rot.transpose(-2, -1))\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "4Imy3Dj5qr5D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}