{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA4wCCyTAfeJX8t6D8eAqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e3magc2ay7w"
      },
      "outputs": [],
      "source": [
        "# Deepdive into the mystical world of tokenization. This is fundamental to making LLMs work\n",
        "\n",
        "# Topics:\n",
        "# 1. Character tokenization\n",
        "# 2. Byte tokenization\n",
        "# 3. Bypte-pair encoding - BPE\n",
        "# 4. WordPiece tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "u-2pkdePbq54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "â€œIf youâ€™re going to try, go all the way.\n",
        "\n",
        "Otherwise, donâ€™t even start.\n",
        "\n",
        "This could mean losing girlfriends, wives, relatives and maybe even your mind.\n",
        "\n",
        "It could mean not eating for three or four days.\n",
        "\n",
        "It could mean freezing on a park bench.\n",
        "\n",
        "It could mean jail.\n",
        "\n",
        "It could mean derision.\n",
        "\n",
        "It could mean mockery â€” isolation.\n",
        "\n",
        "Isolation is a gift.\n",
        "\n",
        "All the others are a test of your endurance, of how much you really want to do it.\n",
        "\n",
        "And, youâ€™ll do it, despite rejection and the worst odds.\n",
        "\n",
        "And it will be better than anything else you can imagine.\n",
        "\n",
        "If youâ€™re going to try, go all the way.\n",
        "\n",
        "There is no other feeling like that.\n",
        "\n",
        "You will be alone with the gods, and the nights will flame with fire.\n",
        "\n",
        "You will ride life straight to perfect laughter.\n",
        "\n",
        "Itâ€™s the only good fight there is.â€\n",
        "\n",
        "- Charles Bukowski (1920 â€“ 1994) ðŸª¦\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7PjW49-blHc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseTokenizer(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def encode(self, text: str):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def decode(self, tokens: list):\n",
        "    pass\n",
        "\n",
        "  def is_lossless(self, text):\n",
        "    tokens = self.encode(text)\n",
        "    decoded_text = self.decode(tokens)\n",
        "    assert decoded_text == text, f\"{text} != {decoded_text}\"\n",
        "\n",
        "  def compression_factor(self, text):\n",
        "    self.is_lossless(text)\n",
        "    num_tokens = len(self.encode(text))\n",
        "    return len(text) / num_tokens # ratio of uncompressed to compressed representation\n"
      ],
      "metadata": {
        "id": "gFIFhqo6lLFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer(BaseTokenizer):\n",
        "\n",
        "  def encode(self, text):\n",
        "      '''\n",
        "      This is a very simple tokenizer that makes use of Unicode code points.\n",
        "      There are two issues with using this as our default tokenizer.\n",
        "      1) Vocab size too large - up to a 1M depending on what type of encoding you choose.\n",
        "      2) Expanding vocab size. The unicode encoding schema keeps adding new character\n",
        "      so lack of consistency. You want a frozen tokenizer.\n",
        "      '''\n",
        "      return [ord(char) for char in text]\n",
        "\n",
        "\n",
        "  def decode(self, tokens: list):\n",
        "    return ''.join([chr(token) for token in tokens])\n"
      ],
      "metadata": {
        "id": "uCmGbB1raTWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ByteTokenizer(BaseTokenizer):\n",
        "\n",
        "  def encode(self, text: str):\n",
        "    '''\n",
        "    The advantage of using a byte tokenizer is that the vocab_size\n",
        "    is bounded to 256. Each character can be represented as a list\n",
        "    of bytes, where each byte is in range(0, 256).\n",
        "    The advantage of this is we have a tiny vocab_size, but the\n",
        "    major disadvantage is that our sequence length will be very\n",
        "    long. As you can tell, the upper bound of `ByteTokenizer.compression_factor`\n",
        "    is 1.0 because a single character can be composed of multiple\n",
        "    tokens.\n",
        "\n",
        "    A better alternative to this is using BPE.\n",
        "    '''\n",
        "    return list(text.encode('utf-8'))\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return bytes(tokens).decode('utf-8')\n"
      ],
      "metadata": {
        "id": "XO7Yyq04wdPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPE(BaseTokenizer):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    self.merge_mapping = {}\n",
        "    self.vocab_size = vocab_size\n",
        "    assert vocab_size >= 256, f\"BPE requires the vocab size to be larger than 256\"\n",
        "\n",
        "  def most_frequent_pair(self, tokens):\n",
        "    # list of tokens.\n",
        "    # Output: the specific token pair that occurs the most amount of times\n",
        "    freq = {}\n",
        "    for pair in zip(tokens, tokens[1:]): # consecutive indices search\n",
        "      freq[pair] = freq.get(pair, 0) + 1\n",
        "\n",
        "    # Get the most common\n",
        "    max_pair = max(freq, key=freq.get) # gets the highest value\n",
        "    max_value = freq[max_pair]\n",
        "    if max_value > 1:\n",
        "      return max_pair\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def merge(self, tokens, pair, token_id):\n",
        "    # Everytime you see the desired pair in tokens, replace it with new_token\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    num_tokens = len(tokens)\n",
        "    while i < num_tokens:\n",
        "      if i < num_tokens - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
        "        new_tokens.append(token_id)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return new_tokens\n",
        "\n",
        "  def join(self, tokens, pair, token_id):\n",
        "    # Perform the opposite of the merge function\n",
        "    num_tokens = len(tokens)\n",
        "    new_tokens = []\n",
        "\n",
        "    for i in range(num_tokens):\n",
        "      if tokens[i] == token_id:\n",
        "        new_tokens.extend(pair)\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "    return new_tokens\n",
        "\n",
        "\n",
        "  def encode_once(self, text: str):\n",
        "    '''\n",
        "    Proposed by Philip Gage in 1994 for data compression.\n",
        "    The idea is as follows (https://en.wikipedia.org/wiki/Byte-pair_encoding):\n",
        "    Suppose you have the string: aaabdaaabac\n",
        "\n",
        "    We first identify the most frequent byte-pair and replace it with a\n",
        "    byte code not represented in the string.\n",
        "\n",
        "    ZabdZabac ; Z = aa\n",
        "\n",
        "    Repeat the process with the most frequent byte-pair: ab\n",
        "\n",
        "    ZYdZYac ; Y = ab, Z = aa\n",
        "\n",
        "    Because the remainder of the characters only repeat once,\n",
        "    we can either end here OR we can go further to optimize it\n",
        "    even more:\n",
        "\n",
        "    XdXac ; X = ZY, Y = ab, Z = aa\n",
        "\n",
        "    Decoding is simple, just move in reverse order.\n",
        "\n",
        "    1. The disadvantages of BPE is that there's no sense of true word/byte tokenization.\n",
        "    Because most frequent parings are tokenized, we could have weird tokens like x representing facti (assuming the word is faction).\n",
        "    Additionally there's no notion of capturing semantics between tokens. 'dog' and 'dog.' may have very different tokens.\n",
        "    # To solve for this, GPT-2 implemented a regex expression: https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53\n",
        "\n",
        "    This processes the string by splitting the punctuation into a separate item in the list.\n",
        "    So, if you have an input string: \"I love dogs. They're 2 nice\" --> [I, love, dogs,., They, 're, 2, nice] // This solves for the first issue I brought up\n",
        "\n",
        "    2. Data distribution MATTERS a lot. Frequent words have very high compression ratios because they occur more frequently.\n",
        "    But languages that don't occur frequently in the train set have a much lower compression ratio and are usually quite fragmented.\n",
        "    '''\n",
        "\n",
        "    # we have a base vocab size of 256. that just comes from the fact that a\n",
        "    # every character is represented by a series of bytes and there are 256\n",
        "    # possible values. i.e. [0, 255]\n",
        "    # So the very first thing we need to do is calculate the number of merges\n",
        "    # we need to make.\n",
        "\n",
        "    num_merges = self.vocab_size - 256\n",
        "\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "\n",
        "    merge_mapping = {} # pair as key, new_token_id as value\n",
        "\n",
        "    i = 0\n",
        "    for i in range(num_merges):\n",
        "      # Step 1. get the most frequent byte pair\n",
        "      pair = self.most_frequent_pair(tokens)\n",
        "      if pair is None: # if the most frequent pair is 1, we can just stop here.\n",
        "        print(f'Merging terminated early at step: {i}')\n",
        "        break\n",
        "      # Step 2. Merge\n",
        "      tokens = self.merge(tokens, pair, i + 256)\n",
        "      merge_mapping[pair] = i + 256\n",
        "\n",
        "    self.merge_mapping = merge_mapping\n",
        "    print(f'Final vocab size: {256 + i}')\n",
        "    return tokens\n",
        "\n",
        "  def encode(self, text):\n",
        "    if len(self.merge_mapping) == 0:\n",
        "      return self.encode_once(text)\n",
        "\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    for pair, token_id in self.merge_mapping.items():\n",
        "      tokens = self.merge(tokens, pair, token_id)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    if self.merge_mapping is None:\n",
        "      # No merge dictionary, this will be equivalent to Byte decoding\n",
        "      return bytes(tokens).decode('utf-8')\n",
        "\n",
        "    new_tokens = list(tokens) # create a copy\n",
        "    for pair, token_id in reversed(self.merge_mapping.items()): # Python 3.7+ preverses insertion order, reverse makes it sorted\n",
        "      # At each step, join the highest value merge\n",
        "      new_tokens = self.join(new_tokens, pair, token_id)\n",
        "\n",
        "    return bytes(new_tokens).decode('utf-8')"
      ],
      "metadata": {
        "id": "LBU_ypI6z7GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class WordPiece(BaseTokenizer):\n",
        "\n",
        "  def __init__(self, vocab_size: int):\n",
        "    self.merge_mapping = {}\n",
        "    self.vocab_size = vocab_size\n",
        "    assert vocab_size >= 256, f\"WordPiece requires the vocab size to be larger than 256\"\n",
        "\n",
        "  def compute_maximum_likelihood(self, tokens):\n",
        "    if len(tokens) <= 1:\n",
        "      return None\n",
        "\n",
        "    unigram = Counter(tokens)\n",
        "    bigram = Counter(zip(tokens, tokens[1:]))\n",
        "\n",
        "    # Max. likelihood can be deceptive in that rare tokens will have a likelihood score of 1 since freq(pair) = 1 and freq(token1) and freq(token2) = 1\n",
        "    # To circumvent this, we just impose a simple filter where we only take bigrams that have a frequency of at least 2\n",
        "    candidates = {pair: score for pair, score in bigram.items() if score > 1}\n",
        "    if not candidates: # terminate early\n",
        "      return None\n",
        "\n",
        "    scores = {}\n",
        "    eps = 1 # Smoothing factor to prevent rare tokens from having too high of PMI (pointwise mutual information)\n",
        "    for pair, count in candidates.items():\n",
        "      scores[pair] = count / ((eps + unigram[pair[0]]) * (eps + unigram[pair[1]]))\n",
        "\n",
        "    max_score = max(scores, key=lambda p: (scores[p], candidates[p])) # two-tier tiebreaker. if two pairs have the same likelihood, pick the one that has higher joint frequency\n",
        "    return max_score\n",
        "\n",
        "\n",
        "  def merge(self, tokens, pair, new_token_id):\n",
        "    new_tokens = []\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    i = 0\n",
        "    while i < num_tokens:\n",
        "      if i < num_tokens - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
        "        new_tokens.append(new_token_id)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return new_tokens\n",
        "\n",
        "  def encode_once(self, text):\n",
        "    '''\n",
        "    The algorithm is exactly identical to BPE, except instead of computing the pair with highest frequency,\n",
        "    we compute the pair with highest likelihood, i.e. score(x, y) = frequency(x, y) / frequency(x) * frequency(y)\n",
        "    We assume Markov property, i.e. each token is independent of every other token.\n",
        "    '''\n",
        "\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    num_merges = self.vocab_size - 256\n",
        "\n",
        "    merge_mapping = {} # pair, token_id\n",
        "    i = 0\n",
        "    for i in range(num_merges):\n",
        "      # Step 1. find pair with highest likelihood\n",
        "      pair = self.compute_maximum_likelihood(tokens)\n",
        "      if pair is None:\n",
        "        print(f\"Terminating early with {i} merges\")\n",
        "        break\n",
        "      # Step 2. merge\n",
        "      tokens = self.merge(tokens, pair, 256 + i)\n",
        "      merge_mapping[pair] = 256 + i\n",
        "\n",
        "    print(f\"Final vocab size: {256 + i}\")\n",
        "    self.merge_mapping = merge_mapping\n",
        "    return tokens\n",
        "\n",
        "  def encode(self, text):\n",
        "    if len(self.merge_mapping) == 0:\n",
        "      return self.encode_once(text)\n",
        "\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    for pair, token_id in self.merge_mapping.items():\n",
        "      tokens = self.merge(tokens, pair, token_id)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "  def join(self, tokens, pair, token_id):\n",
        "    num_tokens = len(tokens)\n",
        "    new_tokens = []\n",
        "\n",
        "    for i in range(num_tokens):\n",
        "      if tokens[i] == token_id:\n",
        "        new_tokens.extend(pair)\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "    return new_tokens\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    new_tokens = list(tokens)\n",
        "    for pair, token_id in reversed(self.merge_mapping.items()):\n",
        "      new_tokens = self.join(new_tokens, pair, token_id)\n",
        "    return bytes(new_tokens).decode('utf-8')\n"
      ],
      "metadata": {
        "id": "C1WRQ--W_dtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_tokenizer = CharTokenizer()\n",
        "byte_tokenizer = ByteTokenizer()\n",
        "bpe_tokenizer = BPE(vocab_size=400)\n",
        "wordpiece_tokenizer = WordPiece(vocab_size=400)\n",
        "\n",
        "char_compression_factor = char_tokenizer.compression_factor(data)\n",
        "byte_compression_factor = byte_tokenizer.compression_factor(data)\n",
        "bpe_compression_factor = bpe_tokenizer.compression_factor(data)\n",
        "wordpiece_compression_factor = wordpiece_tokenizer.compression_factor(data)\n",
        "\n",
        "print(\"\\nCompression factors - higher is better\")\n",
        "print(f\"Char compression factor: {round(char_compression_factor, 3)}\")\n",
        "print(f\"Byte compression factor: {round(byte_compression_factor, 3)}\")\n",
        "print(f\"BPE compression factor: {round(bpe_compression_factor, 3)}\")\n",
        "print(f\"WordPiece compression factor: {round(wordpiece_compression_factor, 3)}\")"
      ],
      "metadata": {
        "id": "53Pg20puzBGF",
        "outputId": "b22fcd9c-eeef-42a6-f88c-a26739409d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging terminated early at step: 111\n",
            "Final vocab size: 367\n",
            "Final vocab size: 399\n",
            "\n",
            "Compression factors - higher is better\n",
            "Char compression factor: 1.0\n",
            "Byte compression factor: 0.975\n",
            "BPE compression factor: 2.465\n",
            "WordPiece compression factor: 2.495\n"
          ]
        }
      ]
    }
  ]
}