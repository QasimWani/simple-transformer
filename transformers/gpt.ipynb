{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqzZj544rjPFkEcKKkppLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6gx25vEfiX8p"
      },
      "outputs": [],
      "source": [
        "# Barebones implementation of GPT\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadedAttention(nn.Module):\n",
        "  ''' Applies SHA with causal mask '''\n",
        "\n",
        "  def __init__(self, max_seq_len, d_embed):\n",
        "    super().__init__()\n",
        "\n",
        "    self.q_proj = nn.Linear(d_embed, d_embed)\n",
        "    self.k_proj = nn.Linear(d_embed, d_embed)\n",
        "    self.v_proj = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "    self.w_out = nn.Sequential(\n",
        "        nn.Linear(d_embed, d_embed),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(0.1)\n",
        "\n",
        "    # Construct a one-time causal mask\n",
        "    # max_seq_len, max_seq_len. Token at index i will only attend to tokens from 0 to i. This avoids learning from future positions\n",
        "    mask = torch.triu(torch.ones(1, max_seq_len, max_seq_len), diagonal=1).bool()\n",
        "    self.register_buffer('causal_mask', mask)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, pad_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "\n",
        "    # Step 1. Project QKV\n",
        "    Q = self.q_proj(x) # batch_size, seq_len, d_embed\n",
        "    K = self.k_proj(x) # batch_size, seq_len, d_embed\n",
        "    V = self.v_proj(x) # batch_size, seq_len, d_embed\n",
        "\n",
        "    # Step 2. attention calculation\n",
        "    scores = (Q @ K.transpose(-2, -1)) / (d_embed ** 0.5) # seq_len, d_embed x d_embed, seq_len -> batch_size, seq_len, seq_len (how different positions attend to each other)\n",
        "    mask = self.causal_mask[:, :seq_len, :seq_len]\n",
        "\n",
        "    if pad_mask is not None:\n",
        "      # pad_mask.shape = batch_size, seq_len. 1 indicates real and 0 indicates <pad>\n",
        "      pad_mask = pad_mask[:, None, :].bool() # batch_size, 1, seq_len\n",
        "      combined_mask = mask | ~pad_mask\n",
        "      scores = scores.masked_fill(combined_mask, float('-inf')) # no need to load to a particular device since model and x assumes same device\n",
        "    else:\n",
        "      scores = scores.masked_fill(mask, float('-inf')) # no need to load to a particular device since model and x assumes same device\n",
        "\n",
        "    attention_weights = torch.softmax(scores, dim=-1) # batch_size, seq_len, seq_len. What happens if seq_len is very large? Lookup: Online Softmax\n",
        "    attention_weights = self.attn_dropout(attention_weights)\n",
        "\n",
        "    attention = attention_weights @ V # seq_len, seq_len x seq_len, d_embed -> batch_size, seq_len, d_embed relevance calculation\n",
        "\n",
        "    out = self.w_out(attention) # batch_size, seq_len, d_embed\n",
        "    return out # batch_size, seq_len, d_embed"
      ],
      "metadata": {
        "id": "hROrFP9Awf2x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "  ''' Applies MHA with causal mask '''\n",
        "  def __init__(self, max_seq_len, d_embed, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_embed % num_heads == 0, f\"d_embed needs to be divisible by num_heads. Recommend parameters: d_embed = 768, num_heads = 12.\"\n",
        "\n",
        "    self.qkv = nn.Linear(d_embed, d_embed * 3)\n",
        "    self.w_out = nn.Sequential(\n",
        "        nn.Linear(d_embed, d_embed),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(0.1)\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_embed // num_heads\n",
        "\n",
        "    # Register causal mask\n",
        "    mask = torch.triu(torch.ones(1, 1, max_seq_len, max_seq_len, dtype=torch.bool), diagonal=1) # batch_size, num_heads, seq_len, seq_len\n",
        "    self.register_buffer('causal_mask', mask)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, pad_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "\n",
        "    # Step 1. Get QKV matrices, each has dimension of batch_size, seq_len, num_heads, head_dim\n",
        "    qkv = self.qkv(x) # batch_size, seq_len, 3 * d_embed\n",
        "    Q, K, V = rearrange(qkv,\n",
        "                        'batch_size seq_len (three num_heads head_dim) -> three batch_size seq_len num_heads head_dim',\n",
        "                        three=3,\n",
        "                        num_heads=self.num_heads,\n",
        "                        head_dim=self.head_dim).unbind(0) # Note: We do not need to explicitly unbind it since PyTorch uses the iterator to split across Q, K, V\n",
        "\n",
        "    # Step 2. Compute Attention\n",
        "    scores = torch.einsum('b q h d, b k h d -> b h q k', Q, K) / (self.head_dim ** 0.5) # batch_size, num_heads, seq_len, seq_len\n",
        "    mask = self.causal_mask[..., :seq_len, :seq_len] # use elipses operator to keep the leading dimensions without defining them explicitly\n",
        "\n",
        "    if pad_mask is not None:\n",
        "        # Mask out pad tokens so real tokens don't attend to pad tokens.\n",
        "        # This is called key pad. We don't want to pad query tokens because then we'll get an nan due to entire row being -inf\n",
        "        # Pad rows still attend, but it's irrelevant because during loss we'll ignore them\n",
        "        # For example, if we have an input: [cat sat on <pad> <pad>]\n",
        "        # The causal mask would be:\n",
        "        # [0 1 1 1 1] cat\n",
        "        # [0 0 1 1 1] sat\n",
        "        # [0 0 0 1 1] on\n",
        "        # [0 0 0 0 1] <pad>\n",
        "        # [0 0 0 0 0] <pad>\n",
        "        # In this case, we are still attending to the last two columns. We need to explicitly set <pad> tokens to 1 so we mask it out.\n",
        "        # Desired causal mask that includes masking pad tokens:\n",
        "        # [0 1 1 1 1] cat\n",
        "        # [0 0 1 1 1] sat\n",
        "        # [0 0 0 1 1] on\n",
        "        # [0 0 0 1 1] <pad>\n",
        "        # [0 0 0 1 1] <pad>\n",
        "        # NOTE: For right-padded attention masks, you technically don't need to inject a pad mask since the causal mask will take care of the rest\n",
        "        pad_mask = pad_mask[:, None, None, :].bool() # # batch_size, 1, 1, seq_len\n",
        "        combined_mask = mask | ~pad_mask\n",
        "        scores = scores.masked_fill(combined_mask, float('-inf')) # broadcasts to batch_size, num_heads, seq_len, seq_len\n",
        "    else:\n",
        "        scores = scores.masked_fill(mask, float('-inf')) # broadcasts to batch_size, num_heads, seq_len, seq_len\n",
        "\n",
        "    attention_weights = torch.softmax(scores, dim=-1) # batch_size, num_heads, seq_len, seq_len\n",
        "    attention_weights = self.attn_dropout(attention_weights)\n",
        "\n",
        "    attention = torch.einsum('b h q k, b k h d -> b q h d', attention_weights, V) # batch_size, seq_len, d_embed\n",
        "    attention = rearrange(attention, 'b q h d -> b q (h d)')\n",
        "\n",
        "    out = self.w_out(attention) # batch_size, seq_len, d_embed\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "i2_IskiSrm0t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_size, out_channels):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(in_channels, hidden_size),\n",
        "        nn.ReLU(), # Replace with GELU\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(hidden_size, out_channels),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.network(x)"
      ],
      "metadata": {
        "id": "QZg5hrc2rvs7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, num_features, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(num_features)) # d_embed\n",
        "    self.beta = nn.Parameter(torch.zeros(num_features)) # d_embed\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "    mu = x.mean(dim=-1, keepdim=True) # batch_size, seq_len, 1\n",
        "    var = x.var(dim=-1, unbiased=False, keepdim=True) # batch_size, seq_len, 1. Note: unbiased = False divides by true population (instead of N - 1).\n",
        "    # But for large enough d_embed, doesn't matter\n",
        "\n",
        "    x = (x - mu) / torch.sqrt(var + self.eps) # batch_size, seq_len, d_embed\n",
        "    return x * self.gamma + self.beta # automatic broadcasting: batch_size, seq_len, d_embed\n"
      ],
      "metadata": {
        "id": "0JTUaH_Bsool"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, max_seq_len, d_embed, num_heads):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadedAttention(max_seq_len, d_embed, num_heads)\n",
        "    self.ffn = FFN(d_embed, d_embed * 4, d_embed)\n",
        "\n",
        "    self.ln1 = LayerNorm(d_embed)\n",
        "    self.ln2 = LayerNorm(d_embed)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, pad_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    x = x + self.mha(self.ln1(x), pad_mask) # Pre-norm\n",
        "    x = x + self.ffn(self.ln2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ERSoxuEYk-H9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Byte Tokenizer for simplicity, but I refer the reader to my in-depth implementation\n",
        "# of several tokenizers here: https://github.com/QasimWani/simple-transformer/blob/main/transformers/tokenization.ipynb\n",
        "\n",
        "class ByteTokenizer():\n",
        "  def encode(self, text: str):\n",
        "    '''\n",
        "    The advantage of using a byte tokenizer is that the vocab_size\n",
        "    is bounded to 256. Each character can be represented as a list\n",
        "    of bytes, where each byte is in range(0, 256).\n",
        "    The advantage of this is we have a tiny vocab_size, but the\n",
        "    major disadvantage is that our sequence length will be very\n",
        "    long. As you can tell, the upper bound of `ByteTokenizer.compression_factor`\n",
        "    is 1.0 because a single character can be composed of multiple\n",
        "    tokens.\n",
        "\n",
        "    A better alternative to this is using BPE.\n",
        "    '''\n",
        "    return list(text.encode('utf-8'))\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return bytes(tokens).decode('utf-8')\n"
      ],
      "metadata": {
        "id": "UqecLYzFCM3O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedPositionalEncodings(nn.Module):\n",
        "\n",
        "  def __init__(self, max_seq_len, d_embed):\n",
        "    super().__init__()\n",
        "    assert d_embed % 2 == 0\n",
        "    positional_encodings = torch.zeros(max_seq_len, d_embed) # max_seq_len, d_embed\n",
        "    positions = rearrange(torch.arange(0, max_seq_len), 'seq_len->seq_len 1') # max_seq_len, 1\n",
        "    # 1 / 10_000 ** (-index / d_embed) -or- exp(index * (-math.log(1e4) / d_embed))\n",
        "    div_term = 1 / 10_000 ** (torch.arange(0, d_embed, 2) / d_embed) # d_embed // 2\n",
        "\n",
        "    positional_encodings[..., 0::2] = torch.sin(positions * div_term) # max_seq_len, d_embed\n",
        "    positional_encodings[..., 1::2] = torch.cos(positions * div_term) # max_seq_len, d_embed\n",
        "\n",
        "    self.register_buffer('positions', positional_encodings)\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "    return x + self.positions[:seq_len, :][None, ...] # batch_size, seq_len, d_embed"
      ],
      "metadata": {
        "id": "kNTW2GfSuqqA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "  def __init__(self, max_seq_len=1024, d_embed=64, vocab_size=256, num_heads=4, num_transformer_blocks=8):\n",
        "    super().__init__()\n",
        "\n",
        "    self.config = {'max_seq_len': max_seq_len, 'd_embed': d_embed, 'vocab_size': vocab_size, 'num_heads': num_heads, 'num_transformer_blocks': num_transformer_blocks}\n",
        "\n",
        "    self.transformer = nn.Sequential(*[TransformerBlock(max_seq_len, d_embed, num_heads) for _ in range(num_transformer_blocks)])\n",
        "    self.add_positional_encodings = FixedPositionalEncodings(max_seq_len, d_embed)\n",
        "\n",
        "    self.token_embeddings = nn.Embedding(vocab_size, d_embed) # We learn the representation to d_embed. Note: this is a giant lookup table.\n",
        "    # You may ask yourself, learning a representation from large vocab_size of say 50,256 to small d_embed (766) is pretty challenging.\n",
        "    # This is where the transformer network comes into play, whos main job is to learn this representation. So we keep this dead-simple weight preserving look-up table\n",
        "\n",
        "    self.ln_final = LayerNorm(d_embed)\n",
        "    self.lm_head = nn.Linear(d_embed, vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.token_embeddings.weight # NOTE: we do not need to transpose it because Linear layer will internally call: x @ W.T + b\n",
        "\n",
        "  def forward(self, token_ids: torch.Tensor, pad_mask: torch.Tensor = None):\n",
        "    # Step 1. tokenization\n",
        "    token_embeddings = self.token_embeddings(token_ids) # batch_size, seq_len, d_embed\n",
        "\n",
        "    # Step 2. apply positional embeddings\n",
        "    embeddings = self.add_positional_encodings(token_embeddings) # batch_size, seq_len, d_embed (Fixed sinusoidal positions)\n",
        "\n",
        "    # Step 3. Pass it through a transformer block\n",
        "    for block in self.transformer:\n",
        "      embeddings = block(embeddings, pad_mask) # you can't pass in two inputs to Sequential. So need to do it iteratively\n",
        "\n",
        "    # Step 4. Pass it through final linear layer to project down to vocab size (seq_len, d_embed) -> (seq_len, vocab_size)\n",
        "    out = self.lm_head(self.ln_final(embeddings)) # batch_size, seq_len, vocab_size\n",
        "\n",
        "    return out\n",
        "\n",
        "  def get_parameter_count(self):\n",
        "    get_parameter_count = lambda param_name: sum(p.numel() for p in param_name.parameters())\n",
        "\n",
        "    num_transformer_blocks = get_parameter_count(self.transformer)\n",
        "    num_token_embeddings = get_parameter_count(self.token_embeddings)\n",
        "    num_lm_head = get_parameter_count(self.lm_head)\n",
        "\n",
        "    num_qkv_block = get_parameter_count(self.transformer[0].mha)\n",
        "    num_ffn = get_parameter_count(self.transformer[0].ffn)\n",
        "    num_layernorm = get_parameter_count(self.transformer[0].ln1) + get_parameter_count(self.transformer[0].ln2)\n",
        "\n",
        "    num_final_ln = get_parameter_count(self.ln_final)\n",
        "\n",
        "    # Get parameter count for transformer block\n",
        "    return {\n",
        "        'config': self.config,\n",
        "        'transformer_blocks': num_transformer_blocks,\n",
        "        'per_block_mha': num_qkv_block,\n",
        "        'per_block_ffn': num_ffn,\n",
        "        'per_block_layernorm': num_layernorm,\n",
        "        'token_embeddings': num_token_embeddings,\n",
        "        'final_ln': num_final_ln,\n",
        "        'lm_head': num_lm_head, # NOTE: weight sharing, so we don't double count the parameters\n",
        "        'total': num_transformer_blocks + num_token_embeddings + num_final_ln\n",
        "    }\n"
      ],
      "metadata": {
        "id": "VIwjOS87iupy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(max_seq_len=256, d_embed=128, vocab_size=256, num_heads=4, num_transformer_blocks=2).to('cuda')"
      ],
      "metadata": {
        "id": "KJpcjSfbEtaz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nice illustrations showcasing scaling laws:\n",
        "# 1. Parameter count: https://claude.ai/public/artifacts/af13cc77-c008-4436-b61c-129d4e9c66f2\n",
        "# 2. FLOPs: screenshot at https://youtu.be/SQ3fZ1sAqXI?si=dnzKwbDe9ArzvfFz&t=334\n",
        "\n",
        "model.get_parameter_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8e5z59MIMLB",
        "outputId": "d6d1ca16-0621-47fd-f830-10cc0f3406fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'config': {'max_seq_len': 256,\n",
              "  'd_embed': 128,\n",
              "  'vocab_size': 256,\n",
              "  'num_heads': 4,\n",
              "  'num_transformer_blocks': 2},\n",
              " 'transformer_blocks': 396544,\n",
              " 'per_block_mha': 66048,\n",
              " 'per_block_ffn': 131712,\n",
              " 'per_block_layernorm': 512,\n",
              " 'token_embeddings': 32768,\n",
              " 'final_ln': 256,\n",
              " 'lm_head': 32768,\n",
              " 'total': 429568}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's train the model\n",
        "\n",
        "def train_one_epoch(dataloader, model, optimizer):\n",
        "  model.train()\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=-100) # we will make the padding token id to be -100 s.t. we don't take that into account for loss calculation\n",
        "\n",
        "  for batch in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    token_ids, attention_mask = batch\n",
        "\n",
        "\n",
        "    logits = model(token_ids, attention_mask) # batch_size, seq_len, vocab_size\n",
        "\n",
        "    # Loss calculation. Main idea is to shift by one.\n",
        "    # Suppose current tokens that we feed into the model are [the cat sat]\n",
        "    # The targets (just the tokens extended by one) would be: [cat sat on]. assume the original sequence is [the cat sat on the mat]\n",
        "    # So at each step, we're just shifting the index by one so we explicitly teach the model to predict the next token\n",
        "    targets = token_ids[:, 1:].clone() # batch_size, seq_len - 1 (start with the next token to ensure that the first token our model learns to predict is the second token in true sequence, first token in target)\n",
        "    logits = logits[:, :-1, :] # batch_size, seq_len - 1, vocab_size\n",
        "    mask_shifted = attention_mask[:, 1:] # batch_size, seq_len - 1\n",
        "    targets[mask_shifted == 0] = -100 # make padded tokens -100\n",
        "\n",
        "    loss = criterion(rearrange(logits, 'B T V -> (B T) V'), rearrange(targets, 'B T -> (B T)'))\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # MISSING\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "cGafRFnzthVF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frhiffMOXudP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}