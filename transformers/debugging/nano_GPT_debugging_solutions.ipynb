{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM4GlqTt+RiAlpbfCMxIzsl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/debugging/nano_GPT_debugging_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target: do each one in less than 30 minutes\n",
        "# This contains solutions to https://github.com/QasimWani/simple-transformer/blob/main/transformers/debugging/nano_GPT_debugging_problems.ipynb"
      ],
      "metadata": {
        "id": "ywngg5Og4mb9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1: Difficulty Medium\n",
        "# Time taken: 35m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Error 1 - weights initialized to zeros\n",
        "        # nn.init.zeros_(self.q_linear.weight)\n",
        "        # nn.init.zeros_(self.k_linear.weight)\n",
        "        # nn.init.zeros_(self.v_linear.weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dk = x.shape\n",
        "        # b, m, d -> b, m, n, h -> (b, n, m, h)\n",
        "        # Better to just be safe and use einops\n",
        "        q = rearrange(self.q_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        k = rearrange(self.k_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        v = rearrange(self.v_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # (b, n, q, h) x (b, n, h, k) -> (b, n, q, k) Correct!\n",
        "\n",
        "        if mask is not None:\n",
        "            # Erorr 2: masked_fill will replace all True with float('inf'). Change to -inf\n",
        "            # mask = 1 is the future tokens --> we need to replace these\n",
        "            # mask = 0 is past and current --> These must stay as-is\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v) # b, n, q, h\n",
        "        # Error 3 - dangeorous broadcasting happening inside view tensor\n",
        "        # out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim) # b, q, n, k -> b, q, d (doing view before transpose is dangerous)\n",
        "        out = rearrange(out, 'b n q h -> b q (n h)', n=self.num_heads, h=self.head_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU() # 1 - Not really a bug, but maybe replace with a GeLU?\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # attn_out = self.self_attn(x, mask)\n",
        "        # x = x + self.dropout(attn_out)\n",
        "        # x = self.norm1(x)\n",
        "        # ff_out = self.ff(x)\n",
        "        # x = x + self.dropout(ff_out)\n",
        "        # x = self.norm2(x)\n",
        "        # return x\n",
        "\n",
        "        # Correct usage of pre-norm\n",
        "        attn_out = self.self_attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ffn_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "# PositionalEncoding\n",
        "# Bugs: 1\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000): # Fair warning: this might be too much and is not a power of 2 which has worse lookup times in memory\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim) # m, d\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float() # m, 1\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim)).unsqueeze(0) # k=d/2. This might lead to unwanted broadcasting. making it explicit.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # (m, 1) x (1, d/2).  = m, d/2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # 1, m, d good idea to avoid broadcasting issues.\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)] # b, m, d + 1, m, d. Correct!\n",
        "        return x\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim) # weight sharing with out_linear\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size, bias=False)  # Off-by-one vocab size. Fixed ;)\n",
        "        # well i'll do you one double here. let's impose a weight sharing\n",
        "        self.out_linear.weight = self.embedding.weight # note: we do not need to explicitly transpose since the linear layer will do: x @ W.T + b\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        # mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) # Error 2\n",
        "        # Well the causal mask is applied to query positions. so the final form must be: [b, n, m, m] -> [1, 1, m, m] (assumes MHA)\n",
        "        # But the bigger problem is that we're masking out all previous token positions because triu will place zeros in all current and previous positions.\n",
        "        # So either get rid of == 0 or do != 0\n",
        "        # Remmber: triu will place zeros in current and previous positions and masked_fill will replace all True values!\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() # Solution 2\n",
        "        return mask.unsqueeze(0).unsqueeze(1)  # For batch and heads. seems correct!\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x) # we apply x + pos, so should be fine!\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device) # NOTE: Causal mask, not attention mask\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 100\n",
        "batch_size = 32\n",
        "seq_len = 20\n",
        "data = torch.randint(0, vocab_size, (1000, seq_len))  # Simple random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "ff_dim = 512\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim).to('cuda')\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 2\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss() # what is the padding token? usually i'd do ignore_index=-100\n",
        "\n",
        "for epoch in range(50):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size].to('cuda')\n",
        "        # REMINDER: whenever shape shifting a tensor, contiguous MUST follow view\n",
        "        inputs = batch[:, :-1] # saves computation by cutting off last token\n",
        "        targets = batch[:, 1:].contiguous()  # No shift for next-token. Fixed! NOTE: memory here is no longer contiguous, because first element points to second!\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1)) # Off-by-one in vocab size\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDFu_DhZvXy_",
        "outputId": "016ba6c0-80b9-46d2-e838-4b1c9fdf5174"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 17.23827362060547\n",
            "Epoch 2, Loss: 9.752243041992188\n",
            "Epoch 3, Loss: 7.852353572845459\n",
            "Epoch 4, Loss: 6.7391510009765625\n",
            "Epoch 5, Loss: 6.114192962646484\n",
            "Epoch 6, Loss: 5.743813991546631\n",
            "Epoch 7, Loss: 5.358801364898682\n",
            "Epoch 8, Loss: 5.3363518714904785\n",
            "Epoch 9, Loss: 5.084201335906982\n",
            "Epoch 10, Loss: 5.142270565032959\n",
            "Epoch 11, Loss: 4.818770408630371\n",
            "Epoch 12, Loss: 4.684993267059326\n",
            "Epoch 13, Loss: 4.5540771484375\n",
            "Epoch 14, Loss: 4.30326509475708\n",
            "Epoch 15, Loss: 4.249999523162842\n",
            "Epoch 16, Loss: 4.105668544769287\n",
            "Epoch 17, Loss: 3.9461934566497803\n",
            "Epoch 18, Loss: 3.8815810680389404\n",
            "Epoch 19, Loss: 3.7875373363494873\n",
            "Epoch 20, Loss: 3.5763373374938965\n",
            "Epoch 21, Loss: 3.6997880935668945\n",
            "Epoch 22, Loss: 3.4020168781280518\n",
            "Epoch 23, Loss: 3.3523342609405518\n",
            "Epoch 24, Loss: 3.209355115890503\n",
            "Epoch 25, Loss: 3.151813268661499\n",
            "Epoch 26, Loss: 2.9953668117523193\n",
            "Epoch 27, Loss: 2.834721326828003\n",
            "Epoch 28, Loss: 2.7074615955352783\n",
            "Epoch 29, Loss: 2.6717967987060547\n",
            "Epoch 30, Loss: 2.5102837085723877\n",
            "Epoch 31, Loss: 2.3549017906188965\n",
            "Epoch 32, Loss: 2.1316328048706055\n",
            "Epoch 33, Loss: 2.2606096267700195\n",
            "Epoch 34, Loss: 2.0066757202148438\n",
            "Epoch 35, Loss: 1.9803147315979004\n",
            "Epoch 36, Loss: 1.7844992876052856\n",
            "Epoch 37, Loss: 1.8043700456619263\n",
            "Epoch 38, Loss: 1.6640061140060425\n",
            "Epoch 39, Loss: 1.4889063835144043\n",
            "Epoch 40, Loss: 1.5592340230941772\n",
            "Epoch 41, Loss: 1.5624643564224243\n",
            "Epoch 42, Loss: 1.402075171470642\n",
            "Epoch 43, Loss: 1.2853994369506836\n",
            "Epoch 44, Loss: 1.1563729047775269\n",
            "Epoch 45, Loss: 1.3461934328079224\n",
            "Epoch 46, Loss: 1.3075599670410156\n",
            "Epoch 47, Loss: 1.041153073310852\n",
            "Epoch 48, Loss: 1.1039552688598633\n",
            "Epoch 49, Loss: 1.1112555265426636\n",
            "Epoch 50, Loss: 0.9327753186225891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: Difficulty Medium\n",
        "# Time taken: 32m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 2\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_linear = nn.Linear(embed_dim, embed_dim * 3) # This is fine, but be careful how this works!\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_k = x.shape # b, m, d\n",
        "        # Better to just use einops TODO\n",
        "        # qkv = self.qkv_linear(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim) # [b, m, d*3] -> [b, m, 3, n, h]\n",
        "        # []: 3, 16, 384\n",
        "        # 3, 16=seq_len, 384=(3 * 4 * 32)\n",
        "        q, k, v = rearrange(self.qkv_linear(x), 'b m (three n h) -> three b n m h', m=seq_len, three=3, n=self.num_heads, h=self.head_dim).unbind(0)\n",
        "        # q, k, v = qkv[:,:,0].transpose(1, 2), qkv[:,:,1].transpose(1, 2), qkv[:,:,2].transpose(1, 2) # [b, m, n, h] -> [b, n, m, h]\n",
        "        assert q.shape == k.shape == v.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # [b, n, q, h] x [b, n, h, k] -> [b, n, q, k]\n",
        "        assert scores.shape == (batch_size, self.num_heads, seq_len, seq_len)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
        "            # scores += mask * -1e9  # Add instead of masked_fill for broadcasting test. Better to multiply with mask position. TODO\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1) # [b, n, q, k]\n",
        "        out = torch.matmul(attn, v) # [b, n, q, k] x [b, n, k, h] -> [b, n, q, h]\n",
        "        assert out.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', n=self.num_heads, h=self.head_dim)\n",
        "        # out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim) # TODO - convert to einops: [b, q, n, h] -> [b, q, d]\n",
        "        out = self.out_linear(out)\n",
        "        # Debug norm:\n",
        "        # print(out.norm())\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x))) # hmm this is correct. not sure what the error is?\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask): # TODO add residuals\n",
        "        # attn_out = self.self_attn(x, mask)\n",
        "        # x = self.norm1(attn_out)  # No residual add\n",
        "        # ff_out = self.ff(x)\n",
        "        # x = self.norm2(x + self.dropout(ff_out))\n",
        "        # return x\n",
        "        attn_out = self.self_attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ff_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n",
        "\n",
        "# LearnablePositionalEncoding\n",
        "# Bugs: 1\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, embed_dim) * 1/(embed_dim ** 0.5)) # Too high of random. maybe scale by sqrt(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :] # [b, m, d] + [1, m, d]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = LearnablePositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.triu(torch.ones(1, 1, seq_len, seq_len), diagonal=1)  # No ==0, broadcasts as is. Fixed!\n",
        "        return mask\n",
        "        # return mask.unsqueeze(0).unsqueeze(0).expand(-1, self.layers[0].self_attn.num_heads, -1, -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x) # x + pos\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 200\n",
        "batch_size = 64\n",
        "seq_len = 30\n",
        "data = torch.randint(0, vocab_size, (200, seq_len))  # Random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "num_layers = 2\n",
        "ff_dim = 64 * 4\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim).to('cuda')\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()  # Wrong ignore. well what's the padding token? they're all guaranteed to be >0. so idk what token represents pad\n",
        "\n",
        "for epoch in range(30):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size].to('cuda')\n",
        "        b = len(batch)\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:].contiguous()  # No shift, wrong shape. Fixed! (1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        assert outputs.shape == (b, seq_len - 1, vocab_size)\n",
        "        assert targets.shape == (b, seq_len - 1)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Generation loop (autoregressive)\n",
        "# Bugs: 2\n",
        "def generate(model, start_token, max_len=10, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([[start_token]]).to(device)\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)  # No [:, -1] # Okay, interestingly we're doubling. this is because we should only be making use of last token\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, data[1, 0].item(), seq_len, device)\n",
        "print(\"Generated:\", generated)\n",
        "print(\"Original:\", data[1])\n",
        "\n",
        "\n",
        "# First pass - 8m\n",
        "# Second pass - 4m\n",
        "# Third pass - blitzkerg fixes. Alright, so we now know that we're not able to overfit. 32m\n",
        "# Loss did go down. But the issue was with your causal mask. Took an additional 20m that should have been resolved first-pass in 32m. Overall, took 52m. Very poor performance!\n",
        "\n",
        "\n",
        "# Lesson learned - 1: when we're generating the same token over and over again. It is almost guaranteed to be related to !causal mask!\n",
        "# Lesson learned -2: Just stick with einops from the getgo. In MHA implementation, get rid of view/transpose and stick with einops. Will make your life easier!\n",
        "\n",
        "\n",
        "# Okay, so i was able to get the forward pass to run properly. Now i noticed that the loss doesn't go down. Looking at it, 3.9 ~= ln(50=vocab_size). which means random chance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKMOQ76ZwRIW",
        "outputId": "7a6eb234-6e50-4ff9-e4d7-d5876953dba6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 5.50458288192749\n",
            "Epoch 2, Loss: 5.31501579284668\n",
            "Epoch 3, Loss: 5.155738353729248\n",
            "Epoch 4, Loss: 4.9510297775268555\n",
            "Epoch 5, Loss: 4.813943862915039\n",
            "Epoch 6, Loss: 4.636119842529297\n",
            "Epoch 7, Loss: 4.5038557052612305\n",
            "Epoch 8, Loss: 4.308445453643799\n",
            "Epoch 9, Loss: 4.1662139892578125\n",
            "Epoch 10, Loss: 4.007351875305176\n",
            "Epoch 11, Loss: 3.8125600814819336\n",
            "Epoch 12, Loss: 3.627694606781006\n",
            "Epoch 13, Loss: 3.470587968826294\n",
            "Epoch 14, Loss: 3.2895007133483887\n",
            "Epoch 15, Loss: 3.1127593517303467\n",
            "Epoch 16, Loss: 2.890537977218628\n",
            "Epoch 17, Loss: 2.7289586067199707\n",
            "Epoch 18, Loss: 2.5613012313842773\n",
            "Epoch 19, Loss: 2.3673408031463623\n",
            "Epoch 20, Loss: 2.2025320529937744\n",
            "Epoch 21, Loss: 2.0141525268554688\n",
            "Epoch 22, Loss: 1.878295660018921\n",
            "Epoch 23, Loss: 1.7419350147247314\n",
            "Epoch 24, Loss: 1.5361849069595337\n",
            "Epoch 25, Loss: 1.4524025917053223\n",
            "Epoch 26, Loss: 1.3472095727920532\n",
            "Epoch 27, Loss: 1.2306327819824219\n",
            "Epoch 28, Loss: 1.1474953889846802\n",
            "Epoch 29, Loss: 1.062679409980774\n",
            "Epoch 30, Loss: 1.0007179975509644\n",
            "Generated: [159, 111, 151, 191, 115, 102, 184, 72, 61, 96, 52, 151, 197, 111, 166, 66, 174, 198, 167, 118, 74, 95, 68, 47, 160, 37, 146, 89, 179, 102, 93]\n",
            "Original: tensor([159,   1,  57,   9,  75, 153,  14,  95, 169, 159,  41, 166,  49, 171,\n",
            "         34, 116,  27, 108,  35,  73,  66, 185,  11,  23, 104, 169,  72,  60,\n",
            "        141,   8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3 - Difficulty Easy\n",
        "# Time taken: 20m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dk = x.shape\n",
        "\n",
        "        # TODO: use einops\n",
        "        q = rearrange(self.q_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        k = rearrange(self.k_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        v = rearrange(self.v_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "\n",
        "        assert q.shape == k.shape == v.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "\n",
        "        # scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # Correct!\n",
        "        scores = torch.einsum('bnqh,bnkh -> bnqk', q, k) / (self.head_dim ** 0.5)\n",
        "        assert scores.shape == (batch_size, self.num_heads, seq_len, seq_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1) # fixed!\n",
        "        out = torch.matmul(attn, v)\n",
        "        assert out.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "        # TODO: use einops\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', h=self.head_dim, n=self.num_heads)\n",
        "        # out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        assert out.shape == (batch_size, seq_len, dk)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU() # Replace with GELU\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # incorrect layernorm and residual connections\n",
        "        # TODO\n",
        "        # x = self.norm1(x)\n",
        "        # attn_out = self.self_attn(x, mask)\n",
        "        # x = x + self.dropout(attn_out)\n",
        "        # x = self.norm2(x)\n",
        "        # ff_out = self.ff(x)\n",
        "        # x = x + self.dropout(ff_out)\n",
        "        # return x\n",
        "        attn_out = self.self_attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ffn_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float() # [m, 1]\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim)).unsqueeze(0) # [1, d/2]\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # [1, m, d]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)] # [b, m, d] + [1, m, d]\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "        # do weight sharing\n",
        "        self.out_linear.weight = self.embedding.weight # no need to do transpose since linear will automatically apply x @ w.T\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        # mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len) # i prefer to use upper triangular\n",
        "        mask = torch.triu(torch.ones(1, 1, seq_len, seq_len), diagonal=1).bool() # must conver to bool for it to work with masked_fill\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "vocab_size = 200\n",
        "batch_size = 64\n",
        "seq_len = 30\n",
        "data = torch.randint(0, vocab_size, (2000, seq_len))\n",
        "# data = torch.arange(0, seq_len).unsqueeze(0).expand(2000, -1) % vocab_size # batch_size, seq_len # [0, 1, 2, 3, ... seq_len], [0, 1, 2, 3, ... seq_len] x batch-size\n",
        "# If you can generate a loss of 0.0 on the above, then clearly the network learned something!\n",
        "\n",
        "\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "ff_dim = 1024\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim).to('cuda')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) # reduce lr\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(20):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size].to('cuda')\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "def generate(model, start_token, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([start_token]).unsqueeze(0).to(device)\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, data[0, 0].item(), seq_len, device)\n",
        "print(\"Generated:\", generated)\n",
        "print('data:', data[0])\n",
        "\n",
        "# First pass - 7m\n",
        "# Okay so we managed to get the loss down to 5. ln(200) = 5.2 This is still basically random.\n",
        "\n",
        "# nice so while we did manage to get the loss down"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmMmnriBwTcI",
        "outputId": "da232cdd-4af0-4675-b351-c81960531701"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 24.91509437561035\n",
            "Epoch 2, Loss: 13.463411331176758\n",
            "Epoch 3, Loss: 10.271132469177246\n",
            "Epoch 4, Loss: 8.690946578979492\n",
            "Epoch 5, Loss: 7.088154315948486\n",
            "Epoch 6, Loss: 6.1104736328125\n",
            "Epoch 7, Loss: 5.6832780838012695\n",
            "Epoch 8, Loss: 4.8486833572387695\n",
            "Epoch 9, Loss: 4.831226825714111\n",
            "Epoch 10, Loss: 4.490036487579346\n",
            "Epoch 11, Loss: 4.2868146896362305\n",
            "Epoch 12, Loss: 3.9010794162750244\n",
            "Epoch 13, Loss: 3.777676820755005\n",
            "Epoch 14, Loss: 3.4722068309783936\n",
            "Epoch 15, Loss: 3.327303171157837\n",
            "Epoch 16, Loss: 2.9304258823394775\n",
            "Epoch 17, Loss: 2.6433820724487305\n",
            "Epoch 18, Loss: 2.3881335258483887\n",
            "Epoch 19, Loss: 2.15696120262146\n",
            "Epoch 20, Loss: 1.9038798809051514\n",
            "Generated: [22, 71, 11, 8, 160, 46, 8, 73, 46, 178, 78, 0, 46, 151, 112, 73, 68, 46, 178, 182, 68, 93, 35, 50, 115, 62, 11, 70, 0, 50, 0]\n",
            "data: tensor([ 22,  71,  84, 117, 146, 177,  81, 190,  32, 150,  49, 114,   1, 172,\n",
            "        184, 105, 127, 131, 190,  40, 197,  35, 106,  47, 106, 173,  51, 155,\n",
            "        166,  81])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4 - Difficulty Hard\n",
        "# Time Taken: 41m (second pass done at 27m)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.constant_(self.out.bias, 0.) # this is good since transformers make use of zero bias initially to make it act like the identity function initially.\n",
        "\n",
        "    def forward(self, x, mask=None, return_attn=False):\n",
        "        B, T, _ = x.shape\n",
        "\n",
        "        # (b, m, d * 3)\n",
        "        q, k, v = rearrange(self.qkv(x), 'b m (n h three) -> three b n m h', three=3, n=self.num_heads, h=self.head_dim).unbind(0)\n",
        "        assert q.shape == k.shape == v.shape == (B, self.num_heads, T, self.head_dim), f\"{q.shape}, {k.shape}, {v.shape} != {B, self.num_heads, T, self.head_dim}\"\n",
        "        # qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, C // self.num_heads)\n",
        "        # qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        # q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = torch.einsum(\"bnqh,bnkh -> bnqk\", q, k) * (1.0 / math.sqrt(k.size(-1))) # Correct!\n",
        "\n",
        "        assert attn.shape == (B, self.num_heads, T, T), f\"{attn.shape} != {B, self.num_heads, T, T}\"\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask = mask.unsqueeze(1) # TODO: do we need this? look into mask construction and shape. Removed\n",
        "            # shape = [1, 1, m, m]\n",
        "            # okay, let's visualize the mask\n",
        "            attn = attn.masked_fill(mask[:, :, :T, :T], float('-inf')) # NOTE: mask_cache once defined\n",
        "            # attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        assert attn.shape == (B, self.num_heads, T, T), f\"{attn.shape} != {B, self.num_heads, T, T}\"\n",
        "        out = torch.einsum('bnqk,bnkh -> bnqh', attn, v) # (1, 8, 1, 1) x (1, 8, 1, 32) -> (1, 8, 32, 1)\n",
        "        assert out.shape == (B, self.num_heads, T, self.head_dim)\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', h=self.head_dim, n=self.num_heads)\n",
        "        assert out.shape == (B, T, self.embed_dim)\n",
        "        # out = out.transpose(1, 2).reshape(B, T, C)\n",
        "        out = self.out(out)\n",
        "\n",
        "        if return_attn:\n",
        "            return out, attn\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # FIXED!\n",
        "        # residual = x\n",
        "        # x = self.norm1(x)\n",
        "        # x = self.attn(x, mask)\n",
        "        # x = self.dropout(x) + residual\n",
        "\n",
        "        # residual = x\n",
        "        # x = self.norm2(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = self.dropout(x) + residual\n",
        "\n",
        "        # return x\n",
        "        attn_out = self.attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ffn_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000, base=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [m, 1]\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() *\n",
        "                           (-math.log(base) / embed_dim)).unsqueeze(0) # [1, d/2]\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0)) # [1, m, d]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return x * math.sqrt(x.size(-1)) + self.pe[:, :x.size(1)] # hmm seems like a weird normalization trick\n",
        "        return x + self.pe[:, :x.size(1)] # made it simpler\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim,\n",
        "                 max_len=512, dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        # weight sharing\n",
        "        self.fc.weight = self.embed.weight\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.register_buffer('mask_cache', torch.empty(0, dtype=torch.bool)) # hmm, sus\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
        "        # nn.init.normal_(self.fc.weight, mean=0.0, std=0.02) # no need since we're dong weight sharing\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        if self.mask_cache.numel() > 0 and self.mask_cache.size(-1) >= sz: # Replace size(0) with size(-1)\n",
        "            return self.mask_cache[:sz, :sz]\n",
        "\n",
        "        mask = torch.triu(torch.ones(1, 1, sz, sz), diagonal=1).bool() # much easier to represent this way\n",
        "        # mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        # mask = mask.masked_fill(mask == 0, float(0.0))\n",
        "        self.mask_cache = mask\n",
        "        return mask\n",
        "\n",
        "    def create_attention_mask(self, x, padding_mask=None):\n",
        "        batch_size, seq_len = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        causal_mask = self._generate_square_subsequent_mask(seq_len).to(device) # [1, 1, m, m]\n",
        "\n",
        "        if padding_mask is None:\n",
        "            # real = 1, pad = 0\n",
        "            # but when doing mask fill 1 is masked out. So should be: padding_mask == 0, s.t. real = 0, pad = 1\n",
        "            padding_mask = (x == self.pad_idx)[:, None, None, :] # convert to boolean [b, m]. need it to be [b, 1, 1, m] because we pad key tokens\n",
        "            combined_mask = causal_mask | padding_mask # logical or saves the day here\n",
        "            return combined_mask\n",
        "        return causal_mask\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        batch_size, seq_len = x.shape\n",
        "        mask = self.create_attention_mask(x, padding_mask)\n",
        "\n",
        "        assert x.shape == (batch_size, seq_len)\n",
        "        x = self.embed(x) * (self.embed_dim ** 0.5) # Scaling trick! TRY THIS: if you comment it out, loss will not go below 6.0\n",
        "        assert x.shape == (batch_size, seq_len, self.embed_dim)\n",
        "        x = self.pos_enc(x)\n",
        "        assert x.shape == (batch_size, seq_len, self.embed_dim)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "            assert x.shape == (batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        x = self.fc(self.norm(x)) # final layer norm\n",
        "        assert x.shape == (batch_size, seq_len, self.vocab_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "def create_batch_mask(lengths, max_len):\n",
        "    batch_size = len(lengths)\n",
        "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
        "    for i, length in enumerate(lengths):\n",
        "        mask[i, :length] = 1\n",
        "    return mask\n",
        "\n",
        "vocab_size = 1000\n",
        "batch_size = 32\n",
        "seq_len = 128\n",
        "num_epochs = 50\n",
        "pad_token = 0\n",
        "\n",
        "def create_padded_batch(batch_size, seq_len, vocab_size, pad_token=0):\n",
        "    data = []\n",
        "    for _ in range(batch_size):\n",
        "        actual_len = torch.randint(seq_len//2, seq_len, (1,)).item()\n",
        "        seq = torch.randint(1, vocab_size, (actual_len,))\n",
        "        padded = torch.full((seq_len,), pad_token)\n",
        "        padded[:actual_len] = seq\n",
        "        data.append(padded)\n",
        "    return torch.stack(data)\n",
        "\n",
        "data = torch.cat([create_padded_batch(batch_size, seq_len, vocab_size, pad_token)\n",
        "                   for _ in range(500 // batch_size + 1)], dim=0)[:500]\n",
        "\n",
        "model = TransformerLM(vocab_size, 256, 8, 4, 1024, max_len=256, dropout=0.1, pad_idx=pad_token)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_data = data[i:min(i+batch_size, len(data))].to(device)\n",
        "\n",
        "        inputs = batch_data[:, :-1]\n",
        "        targets = batch_data[:, 1:].contiguous()\n",
        "\n",
        "        padding_mask = (inputs != pad_token).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, padding_mask)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, start_tokens, max_len=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    tokens = start_tokens.to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Top-k sampling\n",
        "        padding_mask = (tokens != model.pad_idx).float()\n",
        "        outputs = model(tokens, padding_mask)\n",
        "        logits = outputs[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1)\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "        if next_token.item() == pad_token:\n",
        "            break\n",
        "\n",
        "    return tokens\n",
        "\n",
        "start = data[0, :10].unsqueeze(0)\n",
        "generated = generate(model, start, max_len=30)\n",
        "print(f\"Generated: {generated.tolist()[0]}\")\n",
        "print(f\"Target: {data[0].tolist()}\")\n",
        "\n",
        "# 17 minutes - okay so we got the tensor shapes aligned. nan because of padding mask\n",
        "# 27m - alright, so we managed to get loss below ln(1000) - 6.4. still bad\n",
        "# 41m - figured out the issue. The causal mask was blowing up in size because self.cache_mask[:sq, :sq] was always false. Best to just truncate it in MHA calculation\n",
        "\n",
        "# Lesson learned: For sanity sake in MHA always ensure to keep mask[:, :, :seq_len, :seq_len]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsF4ls47w1D8",
        "outputId": "4ea9874f-9b9d-4fcb-c73f-caeb8bb1efd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 6.9468\n",
            "Epoch 2/50, Loss: 6.9293\n",
            "Epoch 3/50, Loss: 6.9061\n",
            "Epoch 4/50, Loss: 6.8803\n",
            "Epoch 5/50, Loss: 6.8262\n",
            "Epoch 6/50, Loss: 6.6979\n",
            "Epoch 7/50, Loss: 6.5456\n",
            "Epoch 8/50, Loss: 6.3875\n",
            "Epoch 9/50, Loss: 6.2176\n",
            "Epoch 10/50, Loss: 6.0372\n",
            "Epoch 11/50, Loss: 5.8472\n",
            "Epoch 12/50, Loss: 5.6468\n",
            "Epoch 13/50, Loss: 5.4281\n",
            "Epoch 14/50, Loss: 5.2059\n",
            "Epoch 15/50, Loss: 4.9686\n",
            "Epoch 16/50, Loss: 4.7307\n",
            "Epoch 17/50, Loss: 4.4823\n",
            "Epoch 18/50, Loss: 4.2393\n",
            "Epoch 19/50, Loss: 4.0102\n",
            "Epoch 20/50, Loss: 3.7919\n",
            "Epoch 21/50, Loss: 3.5760\n",
            "Epoch 22/50, Loss: 3.3734\n",
            "Epoch 23/50, Loss: 3.1988\n",
            "Epoch 24/50, Loss: 3.0262\n",
            "Epoch 25/50, Loss: 2.8676\n",
            "Epoch 26/50, Loss: 2.7261\n",
            "Epoch 27/50, Loss: 2.5966\n",
            "Epoch 28/50, Loss: 2.4767\n",
            "Epoch 29/50, Loss: 2.3735\n",
            "Epoch 30/50, Loss: 2.2682\n",
            "Epoch 31/50, Loss: 2.1829\n",
            "Epoch 32/50, Loss: 2.1008\n",
            "Epoch 33/50, Loss: 2.0335\n",
            "Epoch 34/50, Loss: 1.9561\n",
            "Epoch 35/50, Loss: 1.8990\n",
            "Epoch 36/50, Loss: 1.8494\n",
            "Epoch 37/50, Loss: 1.7918\n",
            "Epoch 38/50, Loss: 1.7605\n",
            "Epoch 39/50, Loss: 1.7279\n",
            "Epoch 40/50, Loss: 1.6888\n",
            "Epoch 41/50, Loss: 1.6610\n",
            "Epoch 42/50, Loss: 1.6492\n",
            "Epoch 43/50, Loss: 1.6203\n",
            "Epoch 44/50, Loss: 1.6169\n",
            "Epoch 45/50, Loss: 1.6146\n",
            "Epoch 46/50, Loss: 1.6029\n",
            "Epoch 47/50, Loss: 1.5910\n",
            "Epoch 48/50, Loss: 1.5822\n",
            "Epoch 49/50, Loss: 1.5899\n",
            "Epoch 50/50, Loss: 1.5884\n",
            "Generated: [211, 31, 157, 141, 65, 88, 823, 130, 708, 971, 124, 130, 763, 986, 271, 518, 386, 244, 10, 413, 717, 981, 981, 981, 25, 299, 13, 27, 27, 384, 691, 38, 328, 178, 945, 449, 101, 456, 524, 601]\n",
            "Target: [211, 31, 157, 141, 65, 88, 823, 130, 708, 971, 784, 313, 415, 746, 12, 631, 438, 357, 128, 868, 306, 326, 425, 835, 299, 785, 13, 509, 297, 863, 672, 669, 271, 527, 889, 838, 391, 3, 185, 606, 324, 814, 560, 799, 680, 343, 578, 186, 708, 112, 683, 402, 432, 697, 179, 851, 238, 668, 551, 266, 29, 824, 396, 303, 834, 91, 448, 38, 657, 415, 738, 402, 917, 86, 207, 530, 697, 641, 383, 678, 506, 732, 80, 692, 3, 652, 885, 420, 377, 720, 303, 677, 791, 372, 481, 367, 590, 483, 431, 396, 796, 411, 954, 355, 538, 370, 156, 168, 840, 550, 727, 687, 853, 177, 399, 954, 639, 972, 433, 925, 166, 246, 859, 730, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 5\n",
        "# Difficulty Easy\n",
        "# Time Taken 14 10s\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim; self.num_heads=num_heads\n",
        "        self.head_dim=embed_dim//num_heads\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "    def forward(self,x,mask=None):\n",
        "        B,T,C=x.shape\n",
        "        q, k, v = rearrange(self.qkv(x), 'b m (three n h) -> three b n m h', three=3, h=self.head_dim, n=self.num_heads)\n",
        "\n",
        "        # qkv=self.qkv(x).view(B,T,3,self.num_heads,self.head_dim)\n",
        "        # q,k,v=qkv[:,:,0].transpose(1,2), qkv[:,:,1].transpose(1,2), qkv[:,:,2].transpose(1,2)\n",
        "\n",
        "        scores=q@k.transpose(-2,-1)/(self.head_dim**0.5) # Seems correct!\n",
        "        assert scores.shape == (B, self.num_heads, T, T)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask[:, :, :T, :T], float('-inf'))\n",
        "            # scores = scores.masked_fill(mask==0,0)\n",
        "\n",
        "        attn = F.softmax(scores,dim=-1)\n",
        "        out = attn @ v # seems correct\n",
        "        assert out.shape == (B, self.num_heads, T, self.head_dim)\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', n=self.num_heads, h=self.head_dim)\n",
        "        # out=out.transpose(1,2).reshape(B,T,C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,embed_dim,ff_dim):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(embed_dim,ff_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.l2=nn.Linear(ff_dim,embed_dim)\n",
        "    def forward(self,x):\n",
        "        return self.l2(self.gelu(self.l1(x)))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,embed_dim,num_heads,ff_dim,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn=MultiHeadSelfAttention(embed_dim,num_heads)\n",
        "        self.ff=FeedForward(embed_dim,ff_dim)\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x,mask):\n",
        "        x = x + self.drop(self.attn(self.norm1(x),mask))\n",
        "        x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self,embed_dim,max_len=5000):\n",
        "        super().__init__()\n",
        "        pe=torch.zeros(max_len,embed_dim)\n",
        "        pos=torch.arange(0,max_len).unsqueeze(1).float() # [m, 1]\n",
        "        div=torch.exp(torch.arange(0,embed_dim,2).float()*-(torch.log(torch.tensor(10000.0))/embed_dim)).unsqueeze(0) # [1, d/2]\n",
        "        pe[:,0::2]=torch.sin(pos * div)\n",
        "        pe[:,1::2]=torch.cos(pos * div)\n",
        "        pe=pe.unsqueeze(0) # [1, m, d]\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return x + self.pe[:,:x.size(1)] # (b, m, d) + (1, m, d)\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,num_heads,num_layers,ff_dim,max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "        self.pos=SinusoidalPositionalEncoding(embed_dim,max_len)\n",
        "        self.layers=nn.ModuleList([DecoderLayer(embed_dim,num_heads,ff_dim) for _ in range(num_layers)])\n",
        "        self.out=nn.Linear(embed_dim,vocab_size)\n",
        "\n",
        "    def mask(self,T):\n",
        "        # return torch.tril(torch.ones(T,T))==1\n",
        "        return torch.triu(torch.ones(1, 1, T, T), diagonal=1).bool()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.embed(x) # if std << 1.0, then scale it up\n",
        "        x = self.pos(x)\n",
        "        m = self.mask(x.size(1)).to(x.device)\n",
        "        for l in self.layers:\n",
        "          x = l(x,m)\n",
        "        return self.out(x)\n",
        "\n",
        "vocab=80; seq=25; batch=16\n",
        "data=torch.randint(0,vocab,(400,seq))\n",
        "# Can we learn rule of 0 thru seq\n",
        "# data = torch.arange(0, seq).unsqueeze(0).expand(400, -1) # 400, seq\n",
        "\n",
        "model=TransformerLM(vocab,128,4,2,256)\n",
        "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(50):\n",
        "    for i in range(0,len(data),batch):\n",
        "        inp = data[i:i+batch]\n",
        "        tgt = inp[:,1:].contiguous()\n",
        "        out = model(inp)\n",
        "        loss = loss_fn(out[:,:-1].contiguous().view(-1,vocab), tgt.view(-1))\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    print(e,loss.item())\n",
        "\n",
        "def generate(model,start,max_len=15):\n",
        "    model.eval(); x=torch.tensor([[start]])\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "            nxt = out[:, -1, :].argmax(-1, keepdim=True)\n",
        "            x = torch.cat([x,nxt],dim=1)\n",
        "    return x\n",
        "print(\"Sample:\",generate(model,data[0, 0].item(),15))\n",
        "print(\"Target:\", data[0].tolist())\n",
        "# Baseline: ln(50) = 4.3\n",
        "# First pass - 11m got loss down to 2.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6csEp3vEcK69",
        "outputId": "b30cf08d-666c-4844-c256-c1b0af39fe46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 4.504523754119873\n",
            "1 4.349405765533447\n",
            "2 4.231853008270264\n",
            "3 4.128266334533691\n",
            "4 4.00361967086792\n",
            "5 3.847914934158325\n",
            "6 3.653205156326294\n",
            "7 3.4135475158691406\n",
            "8 3.1465470790863037\n",
            "9 2.8731279373168945\n",
            "10 2.671621561050415\n",
            "11 2.4217679500579834\n",
            "12 2.171053647994995\n",
            "13 1.869207501411438\n",
            "14 1.7097455263137817\n",
            "15 1.6259340047836304\n",
            "16 1.5204716920852661\n",
            "17 1.384577751159668\n",
            "18 1.2615166902542114\n",
            "19 1.1333891153335571\n",
            "20 1.152176022529602\n",
            "21 0.9692707061767578\n",
            "22 0.930966317653656\n",
            "23 0.8409411907196045\n",
            "24 0.8247535228729248\n",
            "25 0.7447419166564941\n",
            "26 0.7213876843452454\n",
            "27 0.6605327725410461\n",
            "28 0.6374664306640625\n",
            "29 0.6403492093086243\n",
            "30 0.7021710872650146\n",
            "31 0.6350927948951721\n",
            "32 0.5136216878890991\n",
            "33 0.5616019368171692\n",
            "34 0.5053308606147766\n",
            "35 0.49142834544181824\n",
            "36 0.49911239743232727\n",
            "37 0.47817477583885193\n",
            "38 0.5039803385734558\n",
            "39 0.42986440658569336\n",
            "40 0.4588099718093872\n",
            "41 0.3979105055332184\n",
            "42 0.4447345733642578\n",
            "43 0.41500750184059143\n",
            "44 0.41754016280174255\n",
            "45 0.4096030294895172\n",
            "46 0.38025617599487305\n",
            "47 0.40319177508354187\n",
            "48 0.3718743622303009\n",
            "49 0.3476261794567108\n",
            "Sample: tensor([[39,  7, 69, 12, 38,  5, 21, 22, 55, 76, 29, 43, 51, 41, 67, 79]])\n",
            "Target: [39, 6, 16, 16, 60, 31, 18, 45, 21, 30, 65, 45, 8, 12, 58, 61, 25, 14, 55, 42, 67, 4, 17, 71, 69]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 6\n",
        "# Difficulty Medium\n",
        "\n",
        "# Time taken 16m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    # Seems correct!\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # This is good, checks to see if output identity initially\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.h_dim = config.n_embd // config.n_head\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.triu(torch.ones(1, 1, config.block_size, config.block_size), diagonal=1).bool())\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = rearrange(self.c_attn(x), 'b m (three n h) -> three b n m h', three=3, n=self.n_head, h=self.h_dim).unbind(0)\n",
        "        # q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        # k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T], float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # automatic weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        pass\n",
        "        # if isinstance(module, nn.Linear):\n",
        "        #     if module.bias is not None:\n",
        "        #       torch.nn.init.zeros_(module.bias)\n",
        "        # elif isinstance(module, nn.Embedding):\n",
        "        #   torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "        tok_emb = self.transformer.wte(idx) * (self.config.n_embd ** 0.5) # because std is very low, close to 0\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = kwargs.get('n_layer', 6)\n",
        "        self.n_head = kwargs.get('n_head', 6)\n",
        "        self.n_embd = kwargs.get('n_embd', 384)\n",
        "        self.dropout = kwargs.get('dropout', 0.2)\n",
        "        self.bias = kwargs.get('bias', False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = GPTConfig(\n",
        "        vocab_size=50,\n",
        "        block_size=32,\n",
        "        n_layer=3,\n",
        "        n_head=4,\n",
        "        n_embd=128,\n",
        "        dropout=0.1,\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "    model = GPT(config)\n",
        "\n",
        "    batch_size = 8\n",
        "    seq_len = 32\n",
        "    vocab_size = 50\n",
        "    # This is buggy - we should have same input and outputs\n",
        "    # data = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "    data = torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, -1)\n",
        "    x = data[:, :-1].contiguous()\n",
        "    targets = data[:, 1:].contiguous()\n",
        "    logits, loss = model(x, targets)\n",
        "    print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for i in range(50):\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = model(x, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Step {i}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    idx = torch.tensor([[data[0, 0].item()]])\n",
        "    generated = model.generate(idx, max_new_tokens=20)\n",
        "    print(f\"Generated: {generated[0].tolist()}\")\n",
        "    print(f\"Targets: {data[0, :21].tolist()}\")\n",
        "# Okay, so baseline loss is ln(50) = 3.9. Our buggy code does go down to 3.4 which means it's learning something"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5rcHt7Al6b4",
        "outputId": "f7453c70-0e7d-427d-c400-8abc1aeb8811"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.3744\n",
            "Step 0, Loss: 4.3782\n",
            "Step 1, Loss: 3.8186\n",
            "Step 2, Loss: 3.2728\n",
            "Step 3, Loss: 2.7266\n",
            "Step 4, Loss: 2.2558\n",
            "Step 5, Loss: 1.8192\n",
            "Step 6, Loss: 1.4447\n",
            "Step 7, Loss: 1.1198\n",
            "Step 8, Loss: 0.8642\n",
            "Step 9, Loss: 0.6592\n",
            "Step 10, Loss: 0.5037\n",
            "Step 11, Loss: 0.3869\n",
            "Step 12, Loss: 0.3110\n",
            "Step 13, Loss: 0.2552\n",
            "Step 14, Loss: 0.2060\n",
            "Step 15, Loss: 0.1756\n",
            "Step 16, Loss: 0.1550\n",
            "Step 17, Loss: 0.1345\n",
            "Step 18, Loss: 0.1200\n",
            "Step 19, Loss: 0.1092\n",
            "Step 20, Loss: 0.0993\n",
            "Step 21, Loss: 0.0903\n",
            "Step 22, Loss: 0.0822\n",
            "Step 23, Loss: 0.0770\n",
            "Step 24, Loss: 0.0719\n",
            "Step 25, Loss: 0.0662\n",
            "Step 26, Loss: 0.0626\n",
            "Step 27, Loss: 0.0591\n",
            "Step 28, Loss: 0.0554\n",
            "Step 29, Loss: 0.0527\n",
            "Step 30, Loss: 0.0498\n",
            "Step 31, Loss: 0.0482\n",
            "Step 32, Loss: 0.0459\n",
            "Step 33, Loss: 0.0434\n",
            "Step 34, Loss: 0.0419\n",
            "Step 35, Loss: 0.0406\n",
            "Step 36, Loss: 0.0396\n",
            "Step 37, Loss: 0.0374\n",
            "Step 38, Loss: 0.0360\n",
            "Step 39, Loss: 0.0345\n",
            "Step 40, Loss: 0.0336\n",
            "Step 41, Loss: 0.0326\n",
            "Step 42, Loss: 0.0315\n",
            "Step 43, Loss: 0.0308\n",
            "Step 44, Loss: 0.0296\n",
            "Step 45, Loss: 0.0287\n",
            "Step 46, Loss: 0.0278\n",
            "Step 47, Loss: 0.0273\n",
            "Step 48, Loss: 0.0264\n",
            "Step 49, Loss: 0.0257\n",
            "Generated: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "Targets: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ]
        }
      ]
    }
  ]
}