{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN/9MFPijpeaHEa+frMePda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/debugging/nano_GPT_debugging_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target: do each one in less than 30 minutes\n",
        "# This contains solutions to https://github.com/QasimWani/simple-transformer/blob/main/transformers/debugging/nano_GPT_debugging_problems.ipynb"
      ],
      "metadata": {
        "id": "ywngg5Og4mb9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1: Difficulty Medium\n",
        "# Time taken: 35m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Error 1 - weights initialized to zeros\n",
        "        # nn.init.zeros_(self.q_linear.weight)\n",
        "        # nn.init.zeros_(self.k_linear.weight)\n",
        "        # nn.init.zeros_(self.v_linear.weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dk = x.shape\n",
        "        # b, m, d -> b, m, n, h -> (b, n, m, h)\n",
        "        # Better to just be safe and use einops\n",
        "        q = rearrange(self.q_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        k = rearrange(self.k_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        v = rearrange(self.v_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # (b, n, q, h) x (b, n, h, k) -> (b, n, q, k) Correct!\n",
        "\n",
        "        if mask is not None:\n",
        "            # Erorr 2: masked_fill will replace all True with float('inf'). Change to -inf\n",
        "            # mask = 1 is the future tokens --> we need to replace these\n",
        "            # mask = 0 is past and current --> These must stay as-is\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v) # b, n, q, h\n",
        "        # Error 3 - dangeorous broadcasting happening inside view tensor\n",
        "        # out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim) # b, q, n, k -> b, q, d (doing view before transpose is dangerous)\n",
        "        out = rearrange(out, 'b n q h -> b q (n h)', n=self.num_heads, h=self.head_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU() # 1 - Not really a bug, but maybe replace with a GeLU?\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # attn_out = self.self_attn(x, mask)\n",
        "        # x = x + self.dropout(attn_out)\n",
        "        # x = self.norm1(x)\n",
        "        # ff_out = self.ff(x)\n",
        "        # x = x + self.dropout(ff_out)\n",
        "        # x = self.norm2(x)\n",
        "        # return x\n",
        "\n",
        "        # Correct usage of pre-norm\n",
        "        attn_out = self.self_attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ffn_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "# PositionalEncoding\n",
        "# Bugs: 1\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000): # Fair warning: this might be too much and is not a power of 2 which has worse lookup times in memory\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim) # m, d\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float() # m, 1\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim)).unsqueeze(0) # k=d/2. This might lead to unwanted broadcasting. making it explicit.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # (m, 1) x (1, d/2).  = m, d/2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # 1, m, d good idea to avoid broadcasting issues.\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)] # b, m, d + 1, m, d. Correct!\n",
        "        return x\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim) # weight sharing with out_linear\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size, bias=False)  # Off-by-one vocab size. Fixed ;)\n",
        "        # well i'll do you one double here. let's impose a weight sharing\n",
        "        self.out_linear.weight = self.embedding.weight # note: we do not need to explicitly transpose since the linear layer will do: x @ W.T + b\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        # mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) # Error 2\n",
        "        # Well the causal mask is applied to query positions. so the final form must be: [b, n, m, m] -> [1, 1, m, m] (assumes MHA)\n",
        "        # But the bigger problem is that we're masking out all previous token positions because triu will place zeros in all current and previous positions.\n",
        "        # So either get rid of == 0 or do != 0\n",
        "        # Remmber: triu will place zeros in current and previous positions and masked_fill will replace all True values!\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() # Solution 2\n",
        "        return mask.unsqueeze(0).unsqueeze(1)  # For batch and heads. seems correct!\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x) # we apply x + pos, so should be fine!\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device) # NOTE: Causal mask, not attention mask\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 100\n",
        "batch_size = 32\n",
        "seq_len = 20\n",
        "data = torch.randint(0, vocab_size, (1000, seq_len))  # Simple random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "ff_dim = 512\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim).to('cuda')\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 2\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss() # what is the padding token? usually i'd do ignore_index=-100\n",
        "\n",
        "for epoch in range(50):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size].to('cuda')\n",
        "        # REMINDER: whenever shape shifting a tensor, contiguous MUST follow view\n",
        "        inputs = batch[:, :-1] # saves computation by cutting off last token\n",
        "        targets = batch[:, 1:].contiguous()  # No shift for next-token. Fixed! NOTE: memory here is no longer contiguous, because first element points to second!\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1)) # Off-by-one in vocab size\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDFu_DhZvXy_",
        "outputId": "37b0c3fa-74db-4a26-e058-1790550eabde"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 16.65876579284668\n",
            "Epoch 2, Loss: 9.91919231414795\n",
            "Epoch 3, Loss: 7.202060222625732\n",
            "Epoch 4, Loss: 6.663390159606934\n",
            "Epoch 5, Loss: 5.914766788482666\n",
            "Epoch 6, Loss: 5.347503662109375\n",
            "Epoch 7, Loss: 5.191928386688232\n",
            "Epoch 8, Loss: 5.322828769683838\n",
            "Epoch 9, Loss: 4.788407802581787\n",
            "Epoch 10, Loss: 4.530216217041016\n",
            "Epoch 11, Loss: 4.610927581787109\n",
            "Epoch 12, Loss: 4.574163436889648\n",
            "Epoch 13, Loss: 4.445892333984375\n",
            "Epoch 14, Loss: 4.440374851226807\n",
            "Epoch 15, Loss: 4.0369672775268555\n",
            "Epoch 16, Loss: 4.143715858459473\n",
            "Epoch 17, Loss: 3.9437336921691895\n",
            "Epoch 18, Loss: 3.7424821853637695\n",
            "Epoch 19, Loss: 3.8234410285949707\n",
            "Epoch 20, Loss: 3.7176196575164795\n",
            "Epoch 21, Loss: 3.562265157699585\n",
            "Epoch 22, Loss: 3.343305826187134\n",
            "Epoch 23, Loss: 3.1773416996002197\n",
            "Epoch 24, Loss: 3.2731451988220215\n",
            "Epoch 25, Loss: 2.8281917572021484\n",
            "Epoch 26, Loss: 2.8621819019317627\n",
            "Epoch 27, Loss: 2.755774736404419\n",
            "Epoch 28, Loss: 2.6218812465667725\n",
            "Epoch 29, Loss: 2.5086421966552734\n",
            "Epoch 30, Loss: 2.4341561794281006\n",
            "Epoch 31, Loss: 2.3312325477600098\n",
            "Epoch 32, Loss: 2.243271589279175\n",
            "Epoch 33, Loss: 2.307332992553711\n",
            "Epoch 34, Loss: 2.202542543411255\n",
            "Epoch 35, Loss: 1.8029632568359375\n",
            "Epoch 36, Loss: 1.8469860553741455\n",
            "Epoch 37, Loss: 1.7516053915023804\n",
            "Epoch 38, Loss: 1.7939183712005615\n",
            "Epoch 39, Loss: 1.6546566486358643\n",
            "Epoch 40, Loss: 1.4739218950271606\n",
            "Epoch 41, Loss: 1.4305564165115356\n",
            "Epoch 42, Loss: 1.4342825412750244\n",
            "Epoch 43, Loss: 1.404726505279541\n",
            "Epoch 44, Loss: 1.2135566473007202\n",
            "Epoch 45, Loss: 1.2374968528747559\n",
            "Epoch 46, Loss: 1.2629936933517456\n",
            "Epoch 47, Loss: 0.994470477104187\n",
            "Epoch 48, Loss: 1.1553436517715454\n",
            "Epoch 49, Loss: 1.0941734313964844\n",
            "Epoch 50, Loss: 0.9560415148735046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: Difficulty Medium\n",
        "# Time taken: 32m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 2\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_linear = nn.Linear(embed_dim, embed_dim * 3) # This is fine, but be careful how this works!\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_k = x.shape # b, m, d\n",
        "        # Better to just use einops TODO\n",
        "        # qkv = self.qkv_linear(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim) # [b, m, d*3] -> [b, m, 3, n, h]\n",
        "        # []: 3, 16, 384\n",
        "        # 3, 16=seq_len, 384=(3 * 4 * 32)\n",
        "        q, k, v = rearrange(self.qkv_linear(x), 'b m (three n h) -> three b n m h', m=seq_len, three=3, n=self.num_heads, h=self.head_dim).unbind(0)\n",
        "        # q, k, v = qkv[:,:,0].transpose(1, 2), qkv[:,:,1].transpose(1, 2), qkv[:,:,2].transpose(1, 2) # [b, m, n, h] -> [b, n, m, h]\n",
        "        assert q.shape == k.shape == v.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # [b, n, q, h] x [b, n, h, k] -> [b, n, q, k]\n",
        "        assert scores.shape == (batch_size, self.num_heads, seq_len, seq_len)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
        "            # scores += mask * -1e9  # Add instead of masked_fill for broadcasting test. Better to multiply with mask position. TODO\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1) # [b, n, q, k]\n",
        "        out = torch.matmul(attn, v) # [b, n, q, k] x [b, n, k, h] -> [b, n, q, h]\n",
        "        assert out.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', n=self.num_heads, h=self.head_dim)\n",
        "        # out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim) # TODO - convert to einops: [b, q, n, h] -> [b, q, d]\n",
        "        out = self.out_linear(out)\n",
        "        # Debug norm:\n",
        "        # print(out.norm())\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x))) # hmm this is correct. not sure what the error is?\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask): # TODO add residuals\n",
        "        # attn_out = self.self_attn(x, mask)\n",
        "        # x = self.norm1(attn_out)  # No residual add\n",
        "        # ff_out = self.ff(x)\n",
        "        # x = self.norm2(x + self.dropout(ff_out))\n",
        "        # return x\n",
        "        attn_out = self.self_attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ff_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n",
        "\n",
        "# LearnablePositionalEncoding\n",
        "# Bugs: 1\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, embed_dim) * 1/(embed_dim ** 0.5)) # Too high of random. maybe scale by sqrt(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :] # [b, m, d] + [1, m, d]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = LearnablePositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.triu(torch.ones(1, 1, seq_len, seq_len), diagonal=1)  # No ==0, broadcasts as is. Fixed!\n",
        "        return mask\n",
        "        # return mask.unsqueeze(0).unsqueeze(0).expand(-1, self.layers[0].self_attn.num_heads, -1, -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x) # x + pos\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 200\n",
        "batch_size = 64\n",
        "seq_len = 30\n",
        "data = torch.randint(0, vocab_size, (200, seq_len))  # Random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "num_layers = 2\n",
        "ff_dim = 64 * 4\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim).to('cuda')\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()  # Wrong ignore. well what's the padding token? they're all guaranteed to be >0. so idk what token represents pad\n",
        "\n",
        "for epoch in range(30):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size].to('cuda')\n",
        "        b = len(batch)\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:].contiguous()  # No shift, wrong shape. Fixed! (1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        assert outputs.shape == (b, seq_len - 1, vocab_size)\n",
        "        assert targets.shape == (b, seq_len - 1)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Generation loop (autoregressive)\n",
        "# Bugs: 2\n",
        "def generate(model, start_token, max_len=10, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([[start_token]]).to(device)\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)  # No [:, -1] # Okay, interestingly we're doubling. this is because we should only be making use of last token\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, data[1, 0].item(), seq_len, device)\n",
        "print(\"Generated:\", generated)\n",
        "print(\"Original:\", data[1])\n",
        "\n",
        "\n",
        "# First pass - 8m\n",
        "# Second pass - 4m\n",
        "# Third pass - blitzkerg fixes. Alright, so we now know that we're not able to overfit. 32m\n",
        "# Loss did go down. But the issue was with your causal mask. Took an additional 20m that should have been resolved first-pass in 32m. Overall, took 52m. Very poor performance!\n",
        "\n",
        "\n",
        "# Lesson learned - 1: when we're generating the same token over and over again. It is almost guaranteed to be related to !causal mask!\n",
        "# Lesson learned -2: Just stick with einops from the getgo. In MHA implementation, get rid of view/transpose and stick with einops. Will make your life easier!\n",
        "\n",
        "\n",
        "# Okay, so i was able to get the forward pass to run properly. Now i noticed that the loss doesn't go down. Looking at it, 3.9 ~= ln(50=vocab_size). which means random chance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKMOQ76ZwRIW",
        "outputId": "1b7d0451-a734-4465-f2ca-8358d1fc23cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 5.508038520812988\n",
            "Epoch 2, Loss: 5.356257915496826\n",
            "Epoch 3, Loss: 5.183887481689453\n",
            "Epoch 4, Loss: 5.016503810882568\n",
            "Epoch 5, Loss: 4.840149879455566\n",
            "Epoch 6, Loss: 4.662425518035889\n",
            "Epoch 7, Loss: 4.500616073608398\n",
            "Epoch 8, Loss: 4.352596282958984\n",
            "Epoch 9, Loss: 4.176392078399658\n",
            "Epoch 10, Loss: 4.0160746574401855\n",
            "Epoch 11, Loss: 3.8471202850341797\n",
            "Epoch 12, Loss: 3.646458148956299\n",
            "Epoch 13, Loss: 3.4854209423065186\n",
            "Epoch 14, Loss: 3.2571630477905273\n",
            "Epoch 15, Loss: 3.1006343364715576\n",
            "Epoch 16, Loss: 2.9039833545684814\n",
            "Epoch 17, Loss: 2.7305314540863037\n",
            "Epoch 18, Loss: 2.5265450477600098\n",
            "Epoch 19, Loss: 2.3248372077941895\n",
            "Epoch 20, Loss: 2.1876890659332275\n",
            "Epoch 21, Loss: 2.0111308097839355\n",
            "Epoch 22, Loss: 1.822227954864502\n",
            "Epoch 23, Loss: 1.6882132291793823\n",
            "Epoch 24, Loss: 1.5529844760894775\n",
            "Epoch 25, Loss: 1.4647681713104248\n",
            "Epoch 26, Loss: 1.3355042934417725\n",
            "Epoch 27, Loss: 1.2203292846679688\n",
            "Epoch 28, Loss: 1.118321180343628\n",
            "Epoch 29, Loss: 1.0180186033248901\n",
            "Epoch 30, Loss: 0.9751502871513367\n",
            "Generated: [122, 56, 71, 56, 71, 56, 66, 191, 79, 113, 171, 145, 39, 112, 60, 108, 75, 185, 178, 53, 50, 71, 56, 97, 8, 42, 52, 42, 52, 42, 52]\n",
            "Original: tensor([122, 139,  25,   0, 148,  31,  38,  33, 108, 182, 121, 169,   2, 118,\n",
            "        196, 181,  96, 160, 119, 199, 165, 172, 182,  39,  85, 120,  68, 162,\n",
            "        111, 176])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3 - Difficulty Easy\n",
        "# Time taken: 20m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dk = x.shape\n",
        "\n",
        "        # TODO: use einops\n",
        "        q = rearrange(self.q_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        k = rearrange(self.k_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "        v = rearrange(self.v_linear(x), 'b m (n h) -> b n m h', n=self.num_heads, h=self.head_dim)\n",
        "\n",
        "        assert q.shape == k.shape == v.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "\n",
        "        # scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) # Correct!\n",
        "        scores = torch.einsum('bnqh,bnkh -> bnqk', q, k) / (self.head_dim ** 0.5)\n",
        "        assert scores.shape == (batch_size, self.num_heads, seq_len, seq_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1) # fixed!\n",
        "        out = torch.matmul(attn, v)\n",
        "        assert out.shape == (batch_size, self.num_heads, seq_len, self.head_dim)\n",
        "        # TODO: use einops\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', h=self.head_dim, n=self.num_heads)\n",
        "        # out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        assert out.shape == (batch_size, seq_len, dk)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU() # Replace with GELU\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # incorrect layernorm and residual connections\n",
        "        # TODO\n",
        "        # x = self.norm1(x)\n",
        "        # attn_out = self.self_attn(x, mask)\n",
        "        # x = x + self.dropout(attn_out)\n",
        "        # x = self.norm2(x)\n",
        "        # ff_out = self.ff(x)\n",
        "        # x = x + self.dropout(ff_out)\n",
        "        # return x\n",
        "        attn_out = self.self_attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ffn_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float() # [m, 1]\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim)).unsqueeze(0) # [1, d/2]\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # [1, m, d]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)] # [b, m, d] + [1, m, d]\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "        # do weight sharing\n",
        "        self.out_linear.weight = self.embedding.weight # no need to do transpose since linear will automatically apply x @ w.T\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        # mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len) # i prefer to use upper triangular\n",
        "        mask = torch.triu(torch.ones(1, 1, seq_len, seq_len), diagonal=1).bool() # must conver to bool for it to work with masked_fill\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "vocab_size = 200\n",
        "batch_size = 64\n",
        "seq_len = 30\n",
        "data = torch.randint(0, vocab_size, (2000, seq_len))\n",
        "# data = torch.arange(0, seq_len).unsqueeze(0).expand(2000, -1) % vocab_size # batch_size, seq_len # [0, 1, 2, 3, ... seq_len], [0, 1, 2, 3, ... seq_len] x batch-size\n",
        "# If you can generate a loss of 0.0 on the above, then clearly the network learned something!\n",
        "\n",
        "\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "ff_dim = 1024\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim).to('cuda')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) # reduce lr\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(20):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size].to('cuda')\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "def generate(model, start_token, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([start_token]).unsqueeze(0).to(device)\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, data[0, 0].item(), seq_len, device)\n",
        "print(\"Generated:\", generated)\n",
        "print('data:', data[0])\n",
        "\n",
        "# First pass - 7m\n",
        "# Okay so we managed to get the loss down to 5. ln(200) = 5.2 This is still basically random.\n",
        "\n",
        "# nice so while we did manage to get the loss down"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmMmnriBwTcI",
        "outputId": "863e4e3c-f391-4236-c458-479a552a276c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 24.584558486938477\n",
            "Epoch 2, Loss: 13.734915733337402\n",
            "Epoch 3, Loss: 10.068390846252441\n",
            "Epoch 4, Loss: 8.234720230102539\n",
            "Epoch 5, Loss: 6.857690334320068\n",
            "Epoch 6, Loss: 6.384119510650635\n",
            "Epoch 7, Loss: 5.696919918060303\n",
            "Epoch 8, Loss: 5.006101608276367\n",
            "Epoch 9, Loss: 4.768941879272461\n",
            "Epoch 10, Loss: 4.523361682891846\n",
            "Epoch 11, Loss: 4.285684108734131\n",
            "Epoch 12, Loss: 4.05180025100708\n",
            "Epoch 13, Loss: 3.895644187927246\n",
            "Epoch 14, Loss: 3.545987367630005\n",
            "Epoch 15, Loss: 3.055267095565796\n",
            "Epoch 16, Loss: 3.0507569313049316\n",
            "Epoch 17, Loss: 2.6773107051849365\n",
            "Epoch 18, Loss: 2.3442561626434326\n",
            "Epoch 19, Loss: 2.103731632232666\n",
            "Epoch 20, Loss: 1.7644339799880981\n",
            "Generated: [180, 148, 159, 172, 158, 118, 150, 48, 190, 127, 48, 156, 194, 139, 189, 25, 63, 150, 77, 25, 194, 105, 13, 128, 94, 71, 157, 76, 118, 131, 127]\n",
            "data: tensor([180,  98, 127,  39,  69, 140, 133,  89, 159, 142,  73,  41, 182, 189,\n",
            "         62,  57, 124, 157, 193,  91,  74,  65,  70,  12,  86,  17, 129, 183,\n",
            "        166,   8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4 - Difficulty Hard\n",
        "# Time Taken: 41m (second pass done at 27m)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.constant_(self.out.bias, 0.) # this is good since transformers make use of zero bias initially to make it act like the identity function initially.\n",
        "\n",
        "    def forward(self, x, mask=None, return_attn=False):\n",
        "        B, T, _ = x.shape\n",
        "\n",
        "        # (b, m, d * 3)\n",
        "        q, k, v = rearrange(self.qkv(x), 'b m (n h three) -> three b n m h', three=3, n=self.num_heads, h=self.head_dim).unbind(0)\n",
        "        assert q.shape == k.shape == v.shape == (B, self.num_heads, T, self.head_dim), f\"{q.shape}, {k.shape}, {v.shape} != {B, self.num_heads, T, self.head_dim}\"\n",
        "        # qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, C // self.num_heads)\n",
        "        # qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        # q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = torch.einsum(\"bnqh,bnkh -> bnqk\", q, k) * (1.0 / math.sqrt(k.size(-1))) # Correct!\n",
        "\n",
        "        assert attn.shape == (B, self.num_heads, T, T), f\"{attn.shape} != {B, self.num_heads, T, T}\"\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask = mask.unsqueeze(1) # TODO: do we need this? look into mask construction and shape. Removed\n",
        "            # shape = [1, 1, m, m]\n",
        "            # okay, let's visualize the mask\n",
        "            attn = attn.masked_fill(mask[:, :, :T, :T], float('-inf')) # NOTE: mask_cache once defined\n",
        "            # attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        assert attn.shape == (B, self.num_heads, T, T), f\"{attn.shape} != {B, self.num_heads, T, T}\"\n",
        "        out = torch.einsum('bnqk,bnkh -> bnqh', attn, v) # (1, 8, 1, 1) x (1, 8, 1, 32) -> (1, 8, 32, 1)\n",
        "        assert out.shape == (B, self.num_heads, T, self.head_dim)\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', h=self.head_dim, n=self.num_heads)\n",
        "        assert out.shape == (B, T, self.embed_dim)\n",
        "        # out = out.transpose(1, 2).reshape(B, T, C)\n",
        "        out = self.out(out)\n",
        "\n",
        "        if return_attn:\n",
        "            return out, attn\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # FIXED!\n",
        "        # residual = x\n",
        "        # x = self.norm1(x)\n",
        "        # x = self.attn(x, mask)\n",
        "        # x = self.dropout(x) + residual\n",
        "\n",
        "        # residual = x\n",
        "        # x = self.norm2(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = self.dropout(x) + residual\n",
        "\n",
        "        # return x\n",
        "        attn_out = self.attn(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ffn_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000, base=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [m, 1]\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() *\n",
        "                           (-math.log(base) / embed_dim)).unsqueeze(0) # [1, d/2]\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0)) # [1, m, d]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return x * math.sqrt(x.size(-1)) + self.pe[:, :x.size(1)] # hmm seems like a weird normalization trick\n",
        "        return x + self.pe[:, :x.size(1)] # made it simpler\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim,\n",
        "                 max_len=512, dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        # weight sharing\n",
        "        self.fc.weight = self.embed.weight\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.register_buffer('mask_cache', torch.empty(0, dtype=torch.bool)) # hmm, sus\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
        "        # nn.init.normal_(self.fc.weight, mean=0.0, std=0.02) # no need since we're dong weight sharing\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        if self.mask_cache.numel() > 0 and self.mask_cache.size(-1) >= sz: # Replace size(0) with size(-1)\n",
        "            return self.mask_cache[:sz, :sz]\n",
        "\n",
        "        mask = torch.triu(torch.ones(1, 1, sz, sz), diagonal=1).bool() # much easier to represent this way\n",
        "        # mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        # mask = mask.masked_fill(mask == 0, float(0.0))\n",
        "        self.mask_cache = mask\n",
        "        return mask\n",
        "\n",
        "    def create_attention_mask(self, x, padding_mask=None):\n",
        "        batch_size, seq_len = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        causal_mask = self._generate_square_subsequent_mask(seq_len).to(device) # [1, 1, m, m]\n",
        "\n",
        "        if padding_mask is None:\n",
        "            # real = 1, pad = 0\n",
        "            # but when doing mask fill 1 is masked out. So should be: padding_mask == 0, s.t. real = 0, pad = 1\n",
        "            padding_mask = (x == self.pad_idx)[:, None, None, :] # convert to boolean [b, m]. need it to be [b, 1, 1, m] because we pad key tokens\n",
        "            combined_mask = causal_mask | padding_mask # logical or saves the day here\n",
        "            return combined_mask\n",
        "        return causal_mask\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        batch_size, seq_len = x.shape\n",
        "        mask = self.create_attention_mask(x, padding_mask)\n",
        "\n",
        "        assert x.shape == (batch_size, seq_len)\n",
        "        x = self.embed(x) * (self.embed_dim ** 0.5) # Scaling trick! TRY THIS: if you comment it out, loss will not go below 6.0\n",
        "        assert x.shape == (batch_size, seq_len, self.embed_dim)\n",
        "        x = self.pos_enc(x)\n",
        "        assert x.shape == (batch_size, seq_len, self.embed_dim)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "            assert x.shape == (batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        x = self.fc(self.norm(x)) # final layer norm\n",
        "        assert x.shape == (batch_size, seq_len, self.vocab_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "def create_batch_mask(lengths, max_len):\n",
        "    batch_size = len(lengths)\n",
        "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
        "    for i, length in enumerate(lengths):\n",
        "        mask[i, :length] = 1\n",
        "    return mask\n",
        "\n",
        "vocab_size = 1000\n",
        "batch_size = 32\n",
        "seq_len = 128\n",
        "num_epochs = 50\n",
        "pad_token = 0\n",
        "\n",
        "def create_padded_batch(batch_size, seq_len, vocab_size, pad_token=0):\n",
        "    data = []\n",
        "    for _ in range(batch_size):\n",
        "        actual_len = torch.randint(seq_len//2, seq_len, (1,)).item()\n",
        "        seq = torch.randint(1, vocab_size, (actual_len,))\n",
        "        padded = torch.full((seq_len,), pad_token)\n",
        "        padded[:actual_len] = seq\n",
        "        data.append(padded)\n",
        "    return torch.stack(data)\n",
        "\n",
        "data = torch.cat([create_padded_batch(batch_size, seq_len, vocab_size, pad_token)\n",
        "                   for _ in range(500 // batch_size + 1)], dim=0)[:500]\n",
        "\n",
        "model = TransformerLM(vocab_size, 256, 8, 4, 1024, max_len=256, dropout=0.1, pad_idx=pad_token)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_data = data[i:min(i+batch_size, len(data))].to(device)\n",
        "\n",
        "        inputs = batch_data[:, :-1]\n",
        "        targets = batch_data[:, 1:].contiguous()\n",
        "\n",
        "        padding_mask = (inputs != pad_token).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, padding_mask)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, start_tokens, max_len=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    tokens = start_tokens.to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Top-k sampling\n",
        "        padding_mask = (tokens != model.pad_idx).float()\n",
        "        outputs = model(tokens, padding_mask)\n",
        "        logits = outputs[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1)\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "        if next_token.item() == pad_token:\n",
        "            break\n",
        "\n",
        "    return tokens\n",
        "\n",
        "start = data[0, :10].unsqueeze(0)\n",
        "generated = generate(model, start, max_len=30)\n",
        "print(f\"Generated: {generated.tolist()[0]}\")\n",
        "print(f\"Target: {data[0].tolist()}\")\n",
        "\n",
        "# 17 minutes - okay so we got the tensor shapes aligned. nan because of padding mask\n",
        "# 27m - alright, so we managed to get loss below ln(1000) - 6.4. still bad\n",
        "# 41m - figured out the issue. The causal mask was blowing up in size because self.cache_mask[:sq, :sq] was always false. Best to just truncate it in MHA calculation\n",
        "\n",
        "# Lesson learned: For sanity sake in MHA always ensure to keep mask[:, :, :seq_len, :seq_len]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsF4ls47w1D8",
        "outputId": "b418e7a8-a224-437c-d7ba-8aa3467f0027"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 6.9482\n",
            "Epoch 2/50, Loss: 6.9261\n",
            "Epoch 3/50, Loss: 6.9030\n",
            "Epoch 4/50, Loss: 6.8689\n",
            "Epoch 5/50, Loss: 6.7863\n",
            "Epoch 6/50, Loss: 6.6459\n",
            "Epoch 7/50, Loss: 6.4910\n",
            "Epoch 8/50, Loss: 6.3252\n",
            "Epoch 9/50, Loss: 6.1542\n",
            "Epoch 10/50, Loss: 5.9704\n",
            "Epoch 11/50, Loss: 5.7814\n",
            "Epoch 12/50, Loss: 5.5736\n",
            "Epoch 13/50, Loss: 5.3486\n",
            "Epoch 14/50, Loss: 5.1152\n",
            "Epoch 15/50, Loss: 4.8744\n",
            "Epoch 16/50, Loss: 4.6420\n",
            "Epoch 17/50, Loss: 4.4035\n",
            "Epoch 18/50, Loss: 4.1638\n",
            "Epoch 19/50, Loss: 3.9268\n",
            "Epoch 20/50, Loss: 3.7070\n",
            "Epoch 21/50, Loss: 3.4950\n",
            "Epoch 22/50, Loss: 3.2964\n",
            "Epoch 23/50, Loss: 3.1209\n",
            "Epoch 24/50, Loss: 2.9512\n",
            "Epoch 25/50, Loss: 2.8006\n",
            "Epoch 26/50, Loss: 2.6614\n",
            "Epoch 27/50, Loss: 2.5332\n",
            "Epoch 28/50, Loss: 2.4297\n",
            "Epoch 29/50, Loss: 2.3146\n",
            "Epoch 30/50, Loss: 2.2315\n",
            "Epoch 31/50, Loss: 2.1305\n",
            "Epoch 32/50, Loss: 2.0572\n",
            "Epoch 33/50, Loss: 1.9849\n",
            "Epoch 34/50, Loss: 1.9134\n",
            "Epoch 35/50, Loss: 1.8646\n",
            "Epoch 36/50, Loss: 1.8114\n",
            "Epoch 37/50, Loss: 1.7629\n",
            "Epoch 38/50, Loss: 1.7205\n",
            "Epoch 39/50, Loss: 1.6825\n",
            "Epoch 40/50, Loss: 1.6525\n",
            "Epoch 41/50, Loss: 1.6317\n",
            "Epoch 42/50, Loss: 1.6177\n",
            "Epoch 43/50, Loss: 1.5876\n",
            "Epoch 44/50, Loss: 1.5793\n",
            "Epoch 45/50, Loss: 1.5728\n",
            "Epoch 46/50, Loss: 1.5606\n",
            "Epoch 47/50, Loss: 1.5564\n",
            "Epoch 48/50, Loss: 1.5483\n",
            "Epoch 49/50, Loss: 1.5484\n",
            "Epoch 50/50, Loss: 1.5512\n",
            "Generated: [894, 275, 813, 63, 441, 739, 615, 521, 938, 527, 816, 274, 270, 868, 560, 232, 255, 749, 557, 415, 460, 959, 770, 277, 602, 430, 263, 632, 753, 548, 17, 6, 632, 564, 132, 297, 729, 859, 859, 70]\n",
            "Target: [894, 275, 813, 63, 441, 739, 615, 521, 938, 527, 816, 74, 309, 844, 690, 412, 868, 560, 130, 260, 427, 26, 174, 58, 851, 203, 52, 59, 166, 48, 245, 851, 305, 477, 370, 461, 477, 547, 985, 372, 66, 59, 746, 650, 907, 200, 66, 275, 561, 442, 330, 506, 533, 36, 599, 866, 944, 400, 258, 250, 753, 449, 739, 798, 910, 807, 941, 670, 42, 572, 454, 24, 998, 630, 418, 113, 752, 193, 121, 935, 981, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 5\n",
        "# Difficulty Easy\n",
        "# Time Taken 14 10s\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim; self.num_heads=num_heads\n",
        "        self.head_dim=embed_dim//num_heads\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "    def forward(self,x,mask=None):\n",
        "        B,T,C=x.shape\n",
        "        q, k, v = rearrange(self.qkv(x), 'b m (three n h) -> three b n m h', three=3, h=self.head_dim, n=self.num_heads)\n",
        "\n",
        "        # qkv=self.qkv(x).view(B,T,3,self.num_heads,self.head_dim)\n",
        "        # q,k,v=qkv[:,:,0].transpose(1,2), qkv[:,:,1].transpose(1,2), qkv[:,:,2].transpose(1,2)\n",
        "\n",
        "        scores=q@k.transpose(-2,-1)/(self.head_dim**0.5) # Seems correct!\n",
        "        assert scores.shape == (B, self.num_heads, T, T)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask[:, :, :T, :T], float('-inf'))\n",
        "            # scores = scores.masked_fill(mask==0,0)\n",
        "\n",
        "        attn = F.softmax(scores,dim=-1)\n",
        "        out = attn @ v # seems correct\n",
        "        assert out.shape == (B, self.num_heads, T, self.head_dim)\n",
        "        out = rearrange(out, 'b n m h -> b m (n h)', n=self.num_heads, h=self.head_dim)\n",
        "        # out=out.transpose(1,2).reshape(B,T,C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,embed_dim,ff_dim):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(embed_dim,ff_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.l2=nn.Linear(ff_dim,embed_dim)\n",
        "    def forward(self,x):\n",
        "        return self.l2(self.gelu(self.l1(x)))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,embed_dim,num_heads,ff_dim,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn=MultiHeadSelfAttention(embed_dim,num_heads)\n",
        "        self.ff=FeedForward(embed_dim,ff_dim)\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x,mask):\n",
        "        x = x + self.drop(self.attn(self.norm1(x),mask))\n",
        "        x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self,embed_dim,max_len=5000):\n",
        "        super().__init__()\n",
        "        pe=torch.zeros(max_len,embed_dim)\n",
        "        pos=torch.arange(0,max_len).unsqueeze(1).float() # [m, 1]\n",
        "        div=torch.exp(torch.arange(0,embed_dim,2).float()*-(torch.log(torch.tensor(10000.0))/embed_dim)).unsqueeze(0) # [1, d/2]\n",
        "        pe[:,0::2]=torch.sin(pos * div)\n",
        "        pe[:,1::2]=torch.cos(pos * div)\n",
        "        pe=pe.unsqueeze(0) # [1, m, d]\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return x + self.pe[:,:x.size(1)] # (b, m, d) + (1, m, d)\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,num_heads,num_layers,ff_dim,max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "        self.pos=SinusoidalPositionalEncoding(embed_dim,max_len)\n",
        "        self.layers=nn.ModuleList([DecoderLayer(embed_dim,num_heads,ff_dim) for _ in range(num_layers)])\n",
        "        self.out=nn.Linear(embed_dim,vocab_size)\n",
        "\n",
        "    def mask(self,T):\n",
        "        # return torch.tril(torch.ones(T,T))==1\n",
        "        return torch.triu(torch.ones(1, 1, T, T), diagonal=1).bool()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.embed(x) # if std << 1.0, then scale it up\n",
        "        x = self.pos(x)\n",
        "        m = self.mask(x.size(1)).to(x.device)\n",
        "        for l in self.layers:\n",
        "          x = l(x,m)\n",
        "        return self.out(x)\n",
        "\n",
        "vocab=80; seq=25; batch=16\n",
        "data=torch.randint(0,vocab,(400,seq))\n",
        "# Can we learn rule of 0 thru seq\n",
        "# data = torch.arange(0, seq).unsqueeze(0).expand(400, -1) # 400, seq\n",
        "\n",
        "model=TransformerLM(vocab,128,4,2,256)\n",
        "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(50):\n",
        "    for i in range(0,len(data),batch):\n",
        "        inp = data[i:i+batch]\n",
        "        tgt = inp[:,1:].contiguous()\n",
        "        out = model(inp)\n",
        "        loss = loss_fn(out[:,:-1].contiguous().view(-1,vocab), tgt.view(-1))\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    print(e,loss.item())\n",
        "\n",
        "def generate(model,start,max_len=15):\n",
        "    model.eval(); x=torch.tensor([[start]])\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "            nxt = out[:, -1, :].argmax(-1, keepdim=True)\n",
        "            x = torch.cat([x,nxt],dim=1)\n",
        "    return x\n",
        "print(\"Sample:\",generate(model,data[0, 0].item(),15))\n",
        "print(\"Target:\", data[0].tolist())\n",
        "# Baseline: ln(50) = 4.3\n",
        "# First pass - 11m got loss down to 2.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6csEp3vEcK69",
        "outputId": "00ecb995-92e0-4276-bfd9-2e1377bbd3de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 4.5227370262146\n",
            "1 4.3678059577941895\n",
            "2 4.271291255950928\n",
            "3 4.171764373779297\n",
            "4 4.046815395355225\n",
            "5 3.9116032123565674\n",
            "6 3.745391607284546\n",
            "7 3.5111544132232666\n",
            "8 3.3014843463897705\n",
            "9 3.042496681213379\n",
            "10 2.780313491821289\n",
            "11 2.5006587505340576\n",
            "12 2.216636896133423\n",
            "13 2.0774731636047363\n",
            "14 1.9390748739242554\n",
            "15 1.6542234420776367\n",
            "16 1.5063520669937134\n",
            "17 1.3554095029830933\n",
            "18 1.280140995979309\n",
            "19 1.0645058155059814\n",
            "20 1.0304051637649536\n",
            "21 1.035711407661438\n",
            "22 0.8601331114768982\n",
            "23 0.8623487949371338\n",
            "24 0.7634195685386658\n",
            "25 0.730797290802002\n",
            "26 0.76020747423172\n",
            "27 0.6380252838134766\n",
            "28 0.5974095463752747\n",
            "29 0.5660734176635742\n",
            "30 0.5395298600196838\n",
            "31 0.50992751121521\n",
            "32 0.5218378305435181\n",
            "33 0.48669853806495667\n",
            "34 0.5084396600723267\n",
            "35 0.4684503972530365\n",
            "36 0.39678478240966797\n",
            "37 0.4593440294265747\n",
            "38 0.46391162276268005\n",
            "39 0.3630252182483673\n",
            "40 0.38009557127952576\n",
            "41 0.3211081326007843\n",
            "42 0.35254156589508057\n",
            "43 0.3483334481716156\n",
            "44 0.3280523717403412\n",
            "45 0.2834053635597229\n",
            "46 0.3768521845340729\n",
            "47 0.2802039682865143\n",
            "48 0.26974043250083923\n",
            "49 0.3274654448032379\n",
            "Sample: tensor([[66, 45, 14, 26, 41, 55, 52, 73, 74, 57,  6,  4, 30, 15, 52, 22]])\n",
            "Target: [66, 24, 66, 65, 58, 52, 59, 31, 65, 10, 5, 24, 61, 56, 32, 3, 55, 35, 43, 58, 42, 16, 62, 3, 51]\n"
          ]
        }
      ]
    }
  ]
}