{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSkxaipKABNZRfrKCzAbLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/debugging/nano_GPT_debugging_problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        nn.init.zeros_(self.q_linear.weight)\n",
        "        nn.init.zeros_(self.k_linear.weight)\n",
        "        nn.init.zeros_(self.v_linear.weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "# PositionalEncoding\n",
        "# Bugs: 1\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size + 1)  # Off-by-one vocab size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) == 0\n",
        "        return mask.unsqueeze(0).unsqueeze(1)  # For batch and heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 100\n",
        "batch_size = 32\n",
        "seq_len = 20\n",
        "data = torch.randint(0, vocab_size, (1000, seq_len))  # Simple random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "ff_dim = 512\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 2\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        inputs = batch\n",
        "        targets = batch  # No shift for next-token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size + 1), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "oDFu_DhZvXy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 2\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_linear = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        qkv = self.qkv_linear(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv[:,:,0].transpose(1, 2), qkv[:,:,1].transpose(1, 2), qkv[:,:,2].transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores += mask * -1e9  # Add instead of masked_fill for broadcasting test\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = self.norm1(attn_out)  # No residual add\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "# LearnablePositionalEncoding\n",
        "# Bugs: 1\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = LearnablePositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len))  # No ==0, broadcasts as is\n",
        "        return mask.unsqueeze(0).unsqueeze(0).expand(-1, self.layers[0].self_attn.num_heads, -1, -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 50\n",
        "batch_size = 16\n",
        "seq_len = 15\n",
        "data = torch.randint(0, vocab_size, (500, seq_len))  # Random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "num_layers = 3\n",
        "ff_dim = 256\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Wrong ignore\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch  # No shift, wrong shape\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Generation loop (autoregressive)\n",
        "# Bugs: 2\n",
        "def generate(model, start_token, max_len=10, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([[start_token]]).to(device)\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output.argmax(dim=-1)  # No [:, -1]\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, 0, seq_len, device)\n",
        "print(\"Generated:\", generated)"
      ],
      "metadata": {
        "id": "WKMOQ76ZwRIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, -1e9)  # ~mask inverts, potential broadcast issue\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-2)  # Wrong dim\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.norm1(x)\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = x + self.dropout(attn_out)  # Residual after norm\n",
        "        x = self.norm2(x)\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n",
        "\n",
        "# SinusoidalPositionalEncoding\n",
        "# Bugs: 1\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)  # / instead of *\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)  # tril instead of triu\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 200\n",
        "batch_size = 64\n",
        "seq_len = 30\n",
        "data = torch.randint(0, vocab_size, (2000, seq_len))  # Random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "ff_dim = 1024\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 3\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        inputs = batch\n",
        "        targets = batch[:, 1:]  # Shift but no pad for last token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), targets.view(-1))  # reshape instead of contiguous.view\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Generation loop (autoregressive)\n",
        "# Bugs: 2\n",
        "def generate(model, start_token, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([start_token]).unsqueeze(0).to(device)  # No batch dim properly\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, torch.tensor([0]), seq_len, device)\n",
        "print(\"Generated:\", generated)"
      ],
      "metadata": {
        "id": "CmMmnriBwTcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        nn.init.zeros_(self.q.weight)   # bug init\n",
        "        nn.init.zeros_(self.k.weight)   # bug init\n",
        "        nn.init.zeros_(self.v.weight)   # bug init\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "        scores = q @ k.transpose(-2,-1) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask==0, float('inf'))  # wrong sign\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1,2).reshape(B,T,C)\n",
        "        return self.out(out)\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.fc1(x))   # missing relu\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask):\n",
        "        attn_out = self.attn(x, mask)\n",
        "        x = self.norm1(attn_out)    # dropped residual\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x) + self.drop(ff_out)   # wrong residual order\n",
        "        return x\n",
        "\n",
        "# LearnablePositionalEncoding\n",
        "# Bugs: 1\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Parameter(torch.zeros(1, max_len, embed_dim))  # zeros init\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:,:x.size(1)]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos = LearnablePositionalEncoding(max_len, embed_dim)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim,num_heads,ff_dim) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size+1)  # off by one\n",
        "    def gen_mask(self, T):\n",
        "        return torch.triu(torch.ones(T,T),1)   # float not -inf\n",
        "    def forward(self,x):\n",
        "        x = self.embed(x)\n",
        "        x = self.pos(x)\n",
        "        mask = self.gen_mask(x.size(1)).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Training loop\n",
        "vocab_size=100; seq_len=20; batch=32\n",
        "data = torch.randint(0,vocab_size,(500,seq_len))\n",
        "model = TransformerLM(vocab_size,128,4,2,256)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(2):\n",
        "    for i in range(0,len(data),batch):\n",
        "        inp = data[i:i+batch]\n",
        "        tgt = inp   # no shift\n",
        "        opt.zero_grad()\n",
        "        out = model(inp)\n",
        "        loss = loss_fn(out.view(-1,vocab_size+1), tgt.view(-1)) # wrong vocab size\n",
        "        loss.backward(); opt.step()\n",
        "    print(epoch, loss.item())"
      ],
      "metadata": {
        "id": "HsF4ls47w1D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# MHA\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim; self.num_heads=num_heads\n",
        "        self.head_dim=embed_dim//num_heads\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "    def forward(self,x,mask=None):\n",
        "        B,T,C=x.shape\n",
        "        qkv=self.qkv(x).view(B,T,3,self.num_heads,self.head_dim)\n",
        "        q,k,v=qkv[:,:,0].transpose(1,2), qkv[:,:,1].transpose(1,2), qkv[:,:,2].transpose(1,2)\n",
        "        scores=q@k.transpose(-2,-1)/(self.head_dim**0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask==0,0)   # should be -inf\n",
        "        attn=F.softmax(scores,dim=-2)   # wrong dim\n",
        "        out=attn@v\n",
        "        out=out.transpose(1,2).reshape(B,T,C)\n",
        "        return self.proj(out)\n",
        "\n",
        "# FFN\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,embed_dim,ff_dim):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(embed_dim,ff_dim)\n",
        "        self.l2=nn.Linear(ff_dim,embed_dim)\n",
        "    def forward(self,x):\n",
        "        return self.l2(self.l1(x))   # missing nonlinearity\n",
        "\n",
        "# Decoder\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,embed_dim,num_heads,ff_dim,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn=MultiHeadSelfAttention(embed_dim,num_heads)\n",
        "        self.ff=FeedForward(embed_dim,ff_dim)\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x,mask):\n",
        "        x=self.norm1(x)    # pre-norm but missing residual\n",
        "        x=self.attn(x,mask)+x   # residual in wrong spot\n",
        "        ff=self.ff(x)\n",
        "        x=self.norm2(x)+self.drop(ff)   # wrong order\n",
        "        return x\n",
        "\n",
        "# Sinusoidal PE\n",
        "# Bugs: 1\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self,embed_dim,max_len=5000):\n",
        "        super().__init__()\n",
        "        pe=torch.zeros(max_len,embed_dim)\n",
        "        pos=torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div=torch.exp(torch.arange(0,embed_dim,2).float()*-(torch.log(torch.tensor(10000.0))/embed_dim))\n",
        "        pe[:,0::2]=torch.sin(pos/div)   # wrong formula (should be pos*div)\n",
        "        pe[:,1::2]=torch.cos(pos/div)\n",
        "        pe=pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "    def forward(self,x): return x+self.pe[:,:x.size(1)]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,num_heads,num_layers,ff_dim,max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "        self.pos=SinusoidalPositionalEncoding(embed_dim,max_len)\n",
        "        self.layers=nn.ModuleList([DecoderLayer(embed_dim,num_heads,ff_dim) for _ in range(num_layers)])\n",
        "        self.out=nn.Linear(embed_dim,vocab_size)\n",
        "    def mask(self,T):\n",
        "        return torch.tril(torch.ones(T,T))==1   # correct direction but bool only\n",
        "    def forward(self,x):\n",
        "        x=self.embed(x)\n",
        "        x=self.pos(x)\n",
        "        m=self.mask(x.size(1)).to(x.device)\n",
        "        for l in self.layers: x=l(x,m)\n",
        "        return self.out(x)\n",
        "\n",
        "# Data & training\n",
        "vocab=80; seq=25; batch=16\n",
        "data=torch.randint(0,vocab,(400,seq))\n",
        "model=TransformerLM(vocab,128,4,2,256)\n",
        "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(3):\n",
        "    for i in range(0,len(data),batch):\n",
        "        inp=data[i:i+batch]\n",
        "        tgt=inp[:,1:]   # shift but input not shifted\n",
        "        out=model(inp)\n",
        "        loss=loss_fn(out[:,:-1].reshape(-1,vocab),tgt.reshape(-1))\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    print(e,loss.item())\n",
        "\n",
        "# Generation\n",
        "def generate(model,start,max_len=15):\n",
        "    model.eval(); x=torch.tensor([[start]])\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out=model(x)\n",
        "            nxt=out.argmax(-1)   # no [:,-1]\n",
        "            x=torch.cat([x,nxt],dim=1)\n",
        "    return x\n",
        "print(\"Sample:\",generate(model,0,10))"
      ],
      "metadata": {
        "id": "u3EQbygOw3s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# RoPE helper\n",
        "# Bugs: 2\n",
        "def apply_rope(x, cos, sin):\n",
        "    # x: [B, H, T, D]\n",
        "    B,H,T,D = x.shape\n",
        "    x1 = x[..., ::2]   # even\n",
        "    x2 = x[..., 1::2]  # odd\n",
        "    # Wrong formula: swapped cos/sin application\n",
        "    x_rot = torch.cat([x1*cos - x2*sin, x1*sin + x2*cos], dim=-1)\n",
        "    return x_rot\n",
        "\n",
        "def build_rope_cache(max_seq_len, head_dim, device):\n",
        "    theta = 10000 ** (-torch.arange(0, head_dim, 2, device=device).float()/head_dim)\n",
        "    seq_idx = torch.arange(max_seq_len, device=device).float()\n",
        "    angles = seq_idx[:,None]*theta[None,:]\n",
        "    cos = angles.cos()[None,None,:,:]\n",
        "    sin = angles.sin()[None,None,:,:]\n",
        "    return cos, sin\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 2\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, max_len):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.num_heads=num_heads\n",
        "        self.head_dim=embed_dim//num_heads\n",
        "        self.q=nn.Linear(embed_dim,embed_dim)\n",
        "        self.k=nn.Linear(embed_dim,embed_dim)\n",
        "        self.v=nn.Linear(embed_dim,embed_dim)\n",
        "        self.out=nn.Linear(embed_dim,embed_dim)\n",
        "        self.register_buffer(\"cos\",None)\n",
        "        self.register_buffer(\"sin\",None)\n",
        "        self.max_len=max_len\n",
        "        nn.init.zeros_(self.q.weight)   # bad init\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        B,T,C=x.shape\n",
        "        if self.cos is None:\n",
        "            cos,sin=build_rope_cache(self.max_len,self.head_dim,x.device)\n",
        "            self.cos,self.sin=cos,sin\n",
        "        q=self.q(x).view(B,T,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        k=self.k(x).view(B,T,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        v=self.v(x).view(B,T,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        q=apply_rope(q,self.cos[:,:,:T,:],self.sin[:,:,:T,:])   # rope on q\n",
        "        k=apply_rope(k,self.cos[:,:,:T,:],self.sin[:,:,:T,:])   # rope on k\n",
        "        scores=(q@k.transpose(-2,-1))/(self.head_dim**0.5)\n",
        "        if mask is not None:\n",
        "            scores=scores.masked_fill(mask==0,float('inf'))  # wrong inf sign\n",
        "        attn=F.softmax(scores,dim=-1)\n",
        "        out=attn@v\n",
        "        out=out.transpose(1,2).reshape(B,T,C)\n",
        "        return self.out(out)\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,embed_dim,ff_dim):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(embed_dim,ff_dim)\n",
        "        self.l2=nn.Linear(ff_dim,embed_dim)\n",
        "    def forward(self,x):\n",
        "        return self.l2(self.l1(x))  # missing activation\n",
        "\n",
        "# Decoder\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,embed_dim,num_heads,ff_dim,max_len,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn=MultiHeadSelfAttention(embed_dim,num_heads,max_len)\n",
        "        self.ff=FeedForward(embed_dim,ff_dim)\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x,mask):\n",
        "        attn_out=self.attn(x,mask)\n",
        "        x=self.norm1(attn_out)   # missing residual\n",
        "        ff=self.ff(x)\n",
        "        x=self.norm2(x)+self.drop(ff)  # wrong residual order\n",
        "        return x\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 1\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,num_heads,num_layers,ff_dim,max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "        self.layers=nn.ModuleList([DecoderLayer(embed_dim,num_heads,ff_dim,max_len) for _ in range(num_layers)])\n",
        "        self.out=nn.Linear(embed_dim,vocab_size+1)  # off by one vocab\n",
        "    def mask(self,T):\n",
        "        return torch.triu(torch.ones(T,T),1)  # float ones not -inf\n",
        "    def forward(self,x):\n",
        "        x=self.embed(x)\n",
        "        m=self.mask(x.size(1)).to(x.device)\n",
        "        for l in self.layers:\n",
        "            x=l(x,m)\n",
        "        return self.out(x)\n",
        "\n",
        "# Data & train\n",
        "vocab=120; seq=20; batch=16\n",
        "data=torch.randint(0,vocab,(400,seq))\n",
        "model=TransformerLM(vocab,128,4,2,256,seq)\n",
        "opt=optim.AdamW(model.parameters(),lr=1e-3)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(3):\n",
        "    for i in range(0,len(data),batch):\n",
        "        inp=data[i:i+batch]\n",
        "        tgt=inp   # not shifted\n",
        "        out=model(inp)\n",
        "        loss=loss_fn(out.view(-1,vocab+1),tgt.view(-1)) # mismatch vocab\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    print(e,loss.item())\n",
        "\n",
        "# Generation\n",
        "def generate(model,start,max_len=15):\n",
        "    model.eval(); x=torch.tensor([[start]])\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out=model(x)\n",
        "            nxt=out.argmax(-1)   # no [:,-1]\n",
        "            x=torch.cat([x,nxt],dim=1)\n",
        "    return x.squeeze(0).tolist()\n",
        "\n",
        "print(\"Sample:\",generate(model,0,10))"
      ],
      "metadata": {
        "id": "19MYTPCOw6po"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}