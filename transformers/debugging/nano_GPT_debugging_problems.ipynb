{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTbFArt9oMtn2/TQmOacO8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/debugging/nano_GPT_debugging_problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1\n",
        "# Difficulty Medium\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        nn.init.zeros_(self.q_linear.weight)\n",
        "        nn.init.zeros_(self.k_linear.weight)\n",
        "        nn.init.zeros_(self.v_linear.weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "# PositionalEncoding\n",
        "# Bugs: 1\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size + 1)  # Off-by-one vocab size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) == 0\n",
        "        return mask.unsqueeze(0).unsqueeze(1)  # For batch and heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 100\n",
        "batch_size = 32\n",
        "seq_len = 20\n",
        "data = torch.randint(0, vocab_size, (1000, seq_len))  # Simple random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "ff_dim = 512\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 2\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        inputs = batch\n",
        "        targets = batch  # No shift for next-token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size + 1), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "oDFu_DhZvXy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2\n",
        "# Difficulty Medium\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 2\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_linear = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        qkv = self.qkv_linear(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv[:,:,0].transpose(1, 2), qkv[:,:,1].transpose(1, 2), qkv[:,:,2].transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores += mask * -1e9  # Add instead of masked_fill for broadcasting test\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = self.norm1(attn_out)  # No residual add\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "# LearnablePositionalEncoding\n",
        "# Bugs: 1\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = LearnablePositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len))  # No ==0, broadcasts as is\n",
        "        return mask.unsqueeze(0).unsqueeze(0).expand(-1, self.layers[0].self_attn.num_heads, -1, -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 50\n",
        "batch_size = 16\n",
        "seq_len = 15\n",
        "data = torch.randint(0, vocab_size, (500, seq_len))  # Random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "num_layers = 3\n",
        "ff_dim = 256\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Wrong ignore\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch  # No shift, wrong shape\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Generation loop (autoregressive)\n",
        "# Bugs: 2\n",
        "def generate(model, start_token, max_len=10, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([[start_token]]).to(device)\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output.argmax(dim=-1)  # No [:, -1]\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, 0, seq_len, device)\n",
        "print(\"Generated:\", generated)"
      ],
      "metadata": {
        "id": "WKMOQ76ZwRIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3\n",
        "# Difficult Easy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# MultiHeadSelfAttention\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, -1e9)  # ~mask inverts, potential broadcast issue\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-2)  # Wrong dim\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.out_linear(out)\n",
        "        return out\n",
        "\n",
        "# FeedForward\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# DecoderLayer\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.norm1(x)\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = x + self.dropout(attn_out)  # Residual after norm\n",
        "        x = self.norm2(x)\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n",
        "\n",
        "# SinusoidalPositionalEncoding\n",
        "# Bugs: 1\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)  # / instead of *\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=5000, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, seq_len):\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)  # tril instead of triu\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = self.generate_mask(seq_len).to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        out = self.out_linear(x)\n",
        "        return out\n",
        "\n",
        "# Data preparation\n",
        "vocab_size = 200\n",
        "batch_size = 64\n",
        "seq_len = 30\n",
        "data = torch.randint(0, vocab_size, (2000, seq_len))  # Random data\n",
        "\n",
        "# Model instantiation\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "ff_dim = 1024\n",
        "model = TransformerLM(vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Train loop\n",
        "# Bugs: 3\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        inputs = batch\n",
        "        targets = batch[:, 1:]  # Shift but no pad for last token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), targets.view(-1))  # reshape instead of contiguous.view\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Generation loop (autoregressive)\n",
        "# Bugs: 2\n",
        "def generate(model, start_token, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "    input = torch.tensor([start_token]).unsqueeze(0).to(device)  # No batch dim properly\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
        "            input = torch.cat([input, next_token], dim=1)\n",
        "    return input.squeeze(0).tolist()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "generated = generate(model, torch.tensor([0]), seq_len, device)\n",
        "print(\"Generated:\", generated)"
      ],
      "metadata": {
        "id": "CmMmnriBwTcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4\n",
        "# Difficult Hard\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.constant_(self.out.bias, 0.)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attn=False):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, C // self.num_heads)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).reshape(B, T, C)\n",
        "        out = self.out(out)\n",
        "\n",
        "        if return_attn:\n",
        "            return out, attn\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attn(x, mask)\n",
        "        x = self.dropout(x) + residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout(x) + residual\n",
        "\n",
        "        return x\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000, base=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() *\n",
        "                           (-math.log(base) / embed_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term[:pe[:, 1::2].shape[1]])\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * math.sqrt(x.size(-1)) + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim,\n",
        "                 max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(embed_dim, max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "        self.register_buffer('mask_cache', torch.empty(0))\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
        "        nn.init.normal_(self.fc.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        if self.mask_cache.size(0) >= sz:\n",
        "            return self.mask_cache[:sz, :sz]\n",
        "\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        mask = mask.masked_fill(mask == 0, float(0.0))\n",
        "        self.mask_cache = mask\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        if mask is None:\n",
        "            device = x.device\n",
        "            mask = self._generate_square_subsequent_mask(seq_len).to(device)\n",
        "\n",
        "        x = self.embed(x) * math.sqrt(self.embed_dim)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def create_batch_mask(lengths, max_len):\n",
        "    batch_size = len(lengths)\n",
        "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
        "    for i, length in enumerate(lengths):\n",
        "        mask[i, :length] = 1\n",
        "    return mask\n",
        "\n",
        "vocab_size = 1000\n",
        "batch_size = 32\n",
        "seq_len = 128\n",
        "num_epochs = 10\n",
        "\n",
        "data = torch.randint(0, vocab_size, (500, seq_len))\n",
        "lengths = torch.randint(seq_len//2, seq_len, (500,))\n",
        "\n",
        "model = TransformerLM(vocab_size, 256, 8, 4, 1024, max_len=256, dropout=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_data = data[i:min(i+batch_size, len(data))]\n",
        "        batch_lengths = lengths[i:min(i+batch_size, len(lengths))]\n",
        "\n",
        "        inputs = batch_data.to(device)\n",
        "        targets = inputs.clone()\n",
        "        targets[:, :-1] = inputs[:, 1:]\n",
        "        targets[:, -1] = -100\n",
        "\n",
        "        for j, length in enumerate(batch_lengths):\n",
        "            targets[j, length:] = -100\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, start_tokens, max_len=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    tokens = start_tokens.to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        outputs = model(tokens)\n",
        "        logits = outputs[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1)\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "        if next_token.item() == 0:\n",
        "            break\n",
        "\n",
        "    return tokens\n",
        "\n",
        "start = torch.randint(1, vocab_size, (1, 1))\n",
        "generated = generate(model, start, max_len=30)\n",
        "print(f\"Generated: {generated.tolist()[0]}\")"
      ],
      "metadata": {
        "id": "HsF4ls47w1D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 5\n",
        "# Difficulty Easy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# MHA\n",
        "# Bugs: 3\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim; self.num_heads=num_heads\n",
        "        self.head_dim=embed_dim//num_heads\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "    def forward(self,x,mask=None):\n",
        "        B,T,C=x.shape\n",
        "        qkv=self.qkv(x).view(B,T,3,self.num_heads,self.head_dim)\n",
        "        q,k,v=qkv[:,:,0].transpose(1,2), qkv[:,:,1].transpose(1,2), qkv[:,:,2].transpose(1,2)\n",
        "        scores=q@k.transpose(-2,-1)/(self.head_dim**0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask==0,0)   # should be -inf\n",
        "        attn=F.softmax(scores,dim=-2)   # wrong dim\n",
        "        out=attn@v\n",
        "        out=out.transpose(1,2).reshape(B,T,C)\n",
        "        return self.proj(out)\n",
        "\n",
        "# FFN\n",
        "# Bugs: 1\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,embed_dim,ff_dim):\n",
        "        super().__init__()\n",
        "        self.l1=nn.Linear(embed_dim,ff_dim)\n",
        "        self.l2=nn.Linear(ff_dim,embed_dim)\n",
        "    def forward(self,x):\n",
        "        return self.l2(self.l1(x))   # missing nonlinearity\n",
        "\n",
        "# Decoder\n",
        "# Bugs: 2\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,embed_dim,num_heads,ff_dim,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn=MultiHeadSelfAttention(embed_dim,num_heads)\n",
        "        self.ff=FeedForward(embed_dim,ff_dim)\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x,mask):\n",
        "        x=self.norm1(x)    # pre-norm but missing residual\n",
        "        x=self.attn(x,mask)+x   # residual in wrong spot\n",
        "        ff=self.ff(x)\n",
        "        x=self.norm2(x)+self.drop(ff)   # wrong order\n",
        "        return x\n",
        "\n",
        "# Sinusoidal PE\n",
        "# Bugs: 1\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self,embed_dim,max_len=5000):\n",
        "        super().__init__()\n",
        "        pe=torch.zeros(max_len,embed_dim)\n",
        "        pos=torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div=torch.exp(torch.arange(0,embed_dim,2).float()*-(torch.log(torch.tensor(10000.0))/embed_dim))\n",
        "        pe[:,0::2]=torch.sin(pos/div)   # wrong formula (should be pos*div)\n",
        "        pe[:,1::2]=torch.cos(pos/div)\n",
        "        pe=pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "    def forward(self,x): return x+self.pe[:,:x.size(1)]\n",
        "\n",
        "# TransformerLM\n",
        "# Bugs: 2\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,num_heads,num_layers,ff_dim,max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "        self.pos=SinusoidalPositionalEncoding(embed_dim,max_len)\n",
        "        self.layers=nn.ModuleList([DecoderLayer(embed_dim,num_heads,ff_dim) for _ in range(num_layers)])\n",
        "        self.out=nn.Linear(embed_dim,vocab_size)\n",
        "    def mask(self,T):\n",
        "        return torch.tril(torch.ones(T,T))==1   # correct direction but bool only\n",
        "    def forward(self,x):\n",
        "        x=self.embed(x)\n",
        "        x=self.pos(x)\n",
        "        m=self.mask(x.size(1)).to(x.device)\n",
        "        for l in self.layers: x=l(x,m)\n",
        "        return self.out(x)\n",
        "\n",
        "# Data & training\n",
        "vocab=80; seq=25; batch=16\n",
        "data=torch.randint(0,vocab,(400,seq))\n",
        "model=TransformerLM(vocab,128,4,2,256)\n",
        "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(3):\n",
        "    for i in range(0,len(data),batch):\n",
        "        inp=data[i:i+batch]\n",
        "        tgt=inp[:,1:]   # shift but input not shifted\n",
        "        out=model(inp)\n",
        "        loss=loss_fn(out[:,:-1].reshape(-1,vocab),tgt.reshape(-1))\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    print(e,loss.item())\n",
        "\n",
        "# Generation\n",
        "def generate(model,start,max_len=15):\n",
        "    model.eval(); x=torch.tensor([[start]])\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out=model(x)\n",
        "            nxt=out.argmax(-1)   # no [:,-1]\n",
        "            x=torch.cat([x,nxt],dim=1)\n",
        "    return x\n",
        "print(\"Sample:\",generate(model,0,10))"
      ],
      "metadata": {
        "id": "u3EQbygOw3s7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}