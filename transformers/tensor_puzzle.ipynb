{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJlvyU0tuuASMBNf7DMhdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/tensor_puzzle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6kpimupP97eV"
      },
      "outputs": [],
      "source": [
        "# 10 Exercises to improve your working knowledge of einsum and transformers\n",
        "# Easy - come up with answer in 1-2 minute\n",
        "# Medium - come up with answer in 3 minutes\n",
        "# Hard - come up with answer in 4 minutes\n",
        "\n",
        "import torch\n",
        "from einops import rearrange\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple profiler\n",
        "import functools\n",
        "import time\n",
        "\n",
        "def profile(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.perf_counter()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.perf_counter()\n",
        "        print(f\"{func.__name__} took {end - start:.6f} seconds\")\n",
        "        return result\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "sMTFj2V_dPYQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #1 - Sequence dot product (Easy).\n",
        "# Description: Calculate the dot product between two 1-d vectors\n",
        "\n",
        "@profile\n",
        "def seq_dot_spec(a, b, out):\n",
        "    out[0] = 0\n",
        "    for i in range(len(a)):\n",
        "        out[0] += a[i] * b[i]\n",
        "\n",
        "@profile\n",
        "def seq_dot(a, b):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    # doing 'a,b->' would be multiply each value of a with each value of b producing a matrix of axb.\n",
        "    # then taking the sum would produce a very high valuer\n",
        "    return torch.einsum('i, i ->', a, b) # i,i takes element-wise sum.\n",
        "\n",
        "# Test\n",
        "a = torch.randn(1_000)\n",
        "b = torch.randn(1_000)\n",
        "out_spec = torch.zeros(1)\n",
        "seq_dot_spec(a, b, out_spec)\n",
        "your_out = seq_dot(a, b)\n",
        "assert torch.allclose(your_out, out_spec, atol=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP8lHjJZAadg",
        "outputId": "4444bb9b-1519-4a88-e0ba-bdc6d3445f7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_dot_spec took 0.025067 seconds\n",
            "seq_dot took 0.014832 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #2 - Batched Self-Attention Score Computation (Easy)\n",
        "# Description: Calculate self-attention\n",
        "\n",
        "@profile\n",
        "def batched_attn_scores_spec(q, k, out):\n",
        "    for b in range(q.shape[0]):\n",
        "        for i in range(q.shape[1]):\n",
        "            for j in range(k.shape[1]):\n",
        "                dot = 0\n",
        "                for d in range(q.shape[2]):\n",
        "                    dot += q[b][i][d] * k[b][j][d]\n",
        "                out[b][i][j] = dot\n",
        "@profile\n",
        "def batched_attn_scores(q, k):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    B, T, D = q.shape\n",
        "    return torch.einsum('BTD,BKD->BTK', q, k)\n",
        "\n",
        "# Test\n",
        "q = torch.randn(4, 5, 8)\n",
        "k = torch.randn(4, 7, 8)\n",
        "out_spec = torch.zeros(4, 5, 7)\n",
        "batched_attn_scores_spec(q, k, out_spec)\n",
        "your_out = batched_attn_scores(q, k)\n",
        "assert torch.allclose(your_out, out_spec, atol=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztBFuzLTS-vR",
        "outputId": "8f55f50f-a522-4b92-e936-1a2d43a710df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched_attn_scores_spec took 0.043477 seconds\n",
            "batched_attn_scores took 0.000252 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #3: Weighted sum of Values, i.e. Attention output (Easy)\n",
        "# Description: Compute the weighted sum of value vectors given attention weights\n",
        "\n",
        "@profile\n",
        "def weighted_sum_spec(weights, values, out):\n",
        "    for i in range(weights.shape[0]):\n",
        "        for d in range(values.shape[1]):\n",
        "            total = 0\n",
        "            for j in range(weights.shape[1]):\n",
        "                total += weights[i][j] * values[j][d]\n",
        "            out[i][d] = total\n",
        "\n",
        "@profile\n",
        "def weighted_sum(weights, values):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    return torch.einsum('ij,jd->id', weights, values)\n",
        "\n",
        "# Test\n",
        "weights = torch.randn(5, 7)\n",
        "values = torch.randn(7, 8)\n",
        "out_spec = torch.zeros(5, 8)\n",
        "weighted_sum_spec(weights, values, out_spec)\n",
        "your_out = weighted_sum(weights, values)\n",
        "assert torch.allclose(your_out, out_spec, atol=1e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx3kUdpIhMfW",
        "outputId": "aba5b082-87dd-4666-85ab-f632b975feae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weighted_sum_spec took 0.007693 seconds\n",
            "weighted_sum took 0.000128 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #4 - Learned Positional Embedding Addition (Easy)\n",
        "# Description: Add learned positional encodings to token embeddings using position indices\n",
        "\n",
        "@profile\n",
        "def add_positional_spec(tokens, positions, pos_embed_weight):\n",
        "    result = tokens.clone()\n",
        "    batch_size, seq_len = positions.shape\n",
        "    for b in range(batch_size):\n",
        "        for t in range(seq_len):\n",
        "            pos_idx = positions[b, t]\n",
        "            result[b, t] = tokens[b, t] + pos_embed_weight[pos_idx]\n",
        "    return result\n",
        "\n",
        "@profile\n",
        "def add_positional(tokens, positions, pos_embed_weight):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    # tokens: batch_size, seq_len, d_embed\n",
        "    # positions: batch_size, seq_len\n",
        "    # pos_embed_weight = (batch_size x seq_len), d_embed\n",
        "\n",
        "    # Goal: result[b, t] = tokens[b, t] + pos_embed_weigh[ positions[b, t] ]\n",
        "    # return tokens + pos_embed_weight[positions] # SIMPLE SOLUTION (preferred)\n",
        "    one_hot = torch.nn.functional.one_hot(positions, num_classes=pos_embed_weight.size(0)).float()\n",
        "    return tokens + torch.einsum('btp,pd->btd', one_hot, pos_embed_weight)\n",
        "\n",
        "# Test\n",
        "tokens = torch.randn(2, 10, 64)\n",
        "positions = torch.arange(10).unsqueeze(0).repeat(2, 1)\n",
        "pos_embed_weight = torch.randn(20, 64)\n",
        "assert torch.allclose(add_positional(tokens, positions, pos_embed_weight), add_positional_spec(tokens, positions, pos_embed_weight), atol=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soDqPa-rTO2h",
        "outputId": "a27081a5-7049-47fb-cf1d-8742bd2b611d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add_positional took 0.000494 seconds\n",
            "add_positional_spec took 0.000808 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7# Problem #5 - Multi-head QKV projection (Medium)\n",
        "# Description: Transform input embeddings into multi-head query, key, and value projections simultaneously\n",
        "\n",
        "@profile\n",
        "def multi_head_qkv_spec(x, qkv_weight):\n",
        "    batch_size, seq_len, embed_dim = x.shape\n",
        "    _, three, num_heads, head_dim = qkv_weight.shape\n",
        "    result = torch.zeros(batch_size, 3, num_heads, seq_len, head_dim)\n",
        "    for b in range(batch_size):\n",
        "        for t in range(seq_len):\n",
        "            for i in range(embed_dim):\n",
        "                for qkv in range(3):\n",
        "                    for h in range(num_heads):\n",
        "                        for d in range(head_dim):\n",
        "                            result[b, qkv, h, t, d] += x[b, t, i] * qkv_weight[i, qkv, h, d]\n",
        "    return result\n",
        "\n",
        "@profile\n",
        "def multi_head_qkv(x, qkv_weight):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    # batch_size, seq_len, d_embed = x.shape\n",
        "    # d_embed, i, num_heads, head_dim = qkv_weight.shape\n",
        "    return torch.einsum('btd,dinh -> binth', x, qkv_weight)\n",
        "\n",
        "\n",
        "# Test\n",
        "x = torch.randn(2, 4, 4) # B, T, D\n",
        "qkv_weight = torch.randn(4, 3, 8, 16)  # 8 heads, 16 dim each\n",
        "assert torch.allclose(multi_head_qkv(x, qkv_weight), multi_head_qkv_spec(x, qkv_weight), rtol=1e-3) # lower tolerance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nrt_nrbWgU4",
        "outputId": "2b16877d-57d6-4402-ccc3-f256fa29a6aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_head_qkv took 0.000145 seconds\n",
            "multi_head_qkv_spec took 1.495444 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #6: Multi-Head Attention Output Reconstruction (Medium)\n",
        "# Description: Concatenate multi-head attention outputs and apply an output projection.\n",
        "\n",
        "@profile\n",
        "def reconstruct_multihead_spec(multi_head_output, output_weight):\n",
        "    batch_size, num_heads, seq_len, head_dim = multi_head_output.shape\n",
        "    output_dim = output_weight.shape[1]\n",
        "    concatenated = torch.zeros(batch_size, seq_len, num_heads * head_dim)\n",
        "    for b in range(batch_size):\n",
        "        for t in range(seq_len):\n",
        "            for h in range(num_heads):\n",
        "                for d in range(head_dim):\n",
        "                    concatenated[b, t, h * head_dim + d] = multi_head_output[b, h, t, d]\n",
        "    result = torch.zeros(batch_size, seq_len, output_dim)\n",
        "    for b in range(batch_size):\n",
        "        for t in range(seq_len):\n",
        "            for i in range(num_heads * head_dim):\n",
        "                for j in range(output_dim):\n",
        "                    result[b, t, j] += concatenated[b, t, i] * output_weight[i, j]\n",
        "    return result\n",
        "\n",
        "@profile\n",
        "def reconstruct_multihead(multi_head_output, output_weight):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    # batch_size, num_heads, seq_len, head_dim = multi_head_output\n",
        "    # d_embed (num_heads x head_dim), out_channels = output_weight\n",
        "    # output: batch_size, seq_len, out_channels\n",
        "    # return torch.einsum('b', multi_head_output, output_weight)\n",
        "    multi_head_output = rearrange(multi_head_output, 'b n t h -> b t (n h)') # batch_size, seq_len, d_embed\n",
        "    return torch.einsum('btd, do -> bto', multi_head_output, output_weight)\n",
        "\n",
        "# Test\n",
        "multi_head_output = torch.randn(2, 8, 10, 16)  # 8 heads, 16 dim each\n",
        "output_weight = torch.randn(128, 64)  # 8*16=128 input, 64 output\n",
        "assert torch.allclose(reconstruct_multihead(multi_head_output, output_weight),\n",
        "                      reconstruct_multihead_spec(multi_head_output, output_weight),\n",
        "                      rtol=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpv6ZNfEdGQl",
        "outputId": "2b6cb74b-0250-4f62-f033-41112e0dd620"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reconstruct_multihead took 2.458075 seconds\n",
            "reconstruct_multihead_spec took 4.857121 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #7: Multi-Head Attention Score Computation (Medium)\n",
        "# Description: Compute attention scores for multi-head self-attention (batched, with multiple heads).\n",
        "\n",
        "@profile\n",
        "def mha_scores_spec(q, k, out):\n",
        "    for b in range(q.shape[0]):\n",
        "        for h in range(q.shape[1]):\n",
        "            for i in range(q.shape[2]):\n",
        "                for j in range(k.shape[2]):\n",
        "                    dot = 0\n",
        "                    for d in range(q.shape[3]):\n",
        "                        dot += q[b][h][i][d] * k[b][h][j][d]\n",
        "                    out[b][h][i][j] = dot\n",
        "\n",
        "@profile\n",
        "def mha_scores(q, k):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    return torch.einsum('bhqd,bhkd -> bhqk', q, k)\n",
        "\n",
        "# Test\n",
        "q = torch.randn(4, 2, 5, 8) # batch_size, num_head, q-seq_len, head_dim\n",
        "k = torch.randn(4, 2, 7, 8) # batch_size, num_head, k-seq_len, head_dim\n",
        "out_spec = torch.zeros(4, 2, 5, 7) # batch_size, num_head, q-seq_len, k-seq_len\n",
        "mha_scores_spec(q, k, out_spec)\n",
        "your_out = mha_scores(q, k)\n",
        "assert torch.allclose(your_out, out_spec, atol=1e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zRPQ_xllhOp",
        "outputId": "2623bad7-c55d-48d1-96ad-001f60dcba1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mha_scores_spec took 0.075232 seconds\n",
            "mha_scores took 0.000425 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #8: Scaled Dot-Product attention with causal mask (Hard)\n",
        "# Description: Apply the lower triangular mask to compute causal attention\n",
        "\n",
        "@profile\n",
        "def causal_attention_spec(query, key, value, scale):\n",
        "    batch_size, seq_len, head_dim = query.shape\n",
        "    scores = torch.zeros(batch_size, seq_len, seq_len)\n",
        "    for b in range(batch_size):\n",
        "        for i in range(seq_len):\n",
        "            for j in range(seq_len):\n",
        "                if j <= i:\n",
        "                    scores[b, i, j] = torch.dot(query[b, i], key[b, j]) * scale\n",
        "                else:\n",
        "                    scores[b, i, j] = float('-inf')\n",
        "    attention = torch.softmax(scores, dim=-1)\n",
        "    output = torch.zeros_like(value)\n",
        "    for b in range(batch_size):\n",
        "        for i in range(seq_len):\n",
        "            for j in range(seq_len):\n",
        "                output[b, i] += attention[b, i, j] * value[b, j]\n",
        "    return output\n",
        "\n",
        "@profile\n",
        "def causal_attention(query, key, value, scale):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    # Compute Mask-attention\n",
        "    # qkv = 3x(batch_size, seq_len, d_embed)\n",
        "    batch_size, seq_len, d_embed = query.shape\n",
        "    weights = torch.einsum('bqd, bkd -> bqk', query, key) * scale\n",
        "    # apply causal mask\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "    weights = weights.masked_fill(mask, float('-inf'))\n",
        "    scores = torch.softmax(weights, dim=-1)\n",
        "    out = torch.einsum('bqk, bkd -> bqd', scores, value)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Test\n",
        "query = torch.randn(2, 6, 32)\n",
        "key = torch.randn(2, 6, 32)\n",
        "value = torch.randn(2, 6, 32)\n",
        "scale = 1/torch.sqrt(torch.tensor(32.0))\n",
        "assert torch.allclose(causal_attention(query, key, value, scale),\n",
        "                      causal_attention_spec(query, key, value, scale),\n",
        "                      rtol=1e-3) # lower tolerance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSmdy6gPjxwI",
        "outputId": "3ad2e08f-80a7-411f-908f-207c21e3fdd4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "causal_attention took 0.000658 seconds\n",
            "causal_attention_spec took 0.003319 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #9: Masked Attention Score Computation (Hard)\n",
        "# Description: Compute batched attention scores with a given causal mask, and set masked-out positions to zero\n",
        "\n",
        "@profile\n",
        "def masked_attn_scores_spec(q, k, mask, out):\n",
        "    for b in range(q.shape[0]):\n",
        "        for i in range(q.shape[1]):\n",
        "            for j in range(k.shape[1]):\n",
        "                if mask[i][j] == 0:\n",
        "                    out[b][i][j] = 0\n",
        "                    continue\n",
        "                dot = 0\n",
        "                for d in range(q.shape[2]):\n",
        "                    dot += q[b][i][d] * k[b][j][d]\n",
        "                out[b][i][j] = dot\n",
        "@profile\n",
        "def masked_attn_scores(q, k, mask):\n",
        "    \"\"\"Your solution here\"\"\"\n",
        "    # batch_size, seq_len, d_embed = q/k\n",
        "    # seq_len, seq_len = mask\n",
        "    qk = torch.einsum('bqd,bkd->bqk', q, k)\n",
        "    return torch.einsum('bqk,qk->bqk', qk, mask)\n",
        "\n",
        "\n",
        "# Test\n",
        "seq = 5\n",
        "q = torch.randn(4, seq, 8)\n",
        "k = torch.randn(4, seq, 8)\n",
        "mask = torch.tril(torch.ones(seq, seq))\n",
        "out_spec = torch.zeros(4, seq, seq)\n",
        "masked_attn_scores_spec(q, k, mask, out_spec)\n",
        "your_out = masked_attn_scores(q, k, mask)\n",
        "assert torch.allclose(your_out, out_spec, atol=1e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2pZb9_wn7GV",
        "outputId": "7f4b8777-73a7-4b4a-d448-78eb8b15657c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked_attn_scores_spec took 0.011909 seconds\n",
            "masked_attn_scores took 0.000323 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem #10: Block Matmul (Hard)\n",
        "# Description: Compute efficient block matmul, similar to the implementation in FlashAttention\n",
        "\n",
        "@profile\n",
        "def block_matmul_spec(a, b, out):\n",
        "    \"\"\"\n",
        "    a: [num_q_blocks, num_k_blocks, block_q, d]\n",
        "    b: [num_q_blocks, num_k_blocks, d, block_k]\n",
        "    out: [num_q_blocks, num_k_blocks, block_q, block_k]\n",
        "    \"\"\"\n",
        "    for qb in range(a.shape[0]):\n",
        "        for kb in range(a.shape[1]):\n",
        "            for i in range(a.shape[2]):\n",
        "                for j in range(b.shape[3]):\n",
        "                    dot = 0\n",
        "                    for d in range(a.shape[3]):\n",
        "                        dot += a[qb, kb, i, d] * b[qb, kb, d, j]\n",
        "                    out[qb, kb, i, j] = dot\n",
        "    return out\n",
        "\n",
        "@profile\n",
        "def block_matmul(a, b):\n",
        "    \"\"\"Your einsum solution here\"\"\"\n",
        "    # num_q_blocks, num_k_blocks, q_block_size, d = a\n",
        "    # num_q_blocks, num_k_blocks, d, k_block_size\n",
        "    # final output = num_q_blocks, num_k_blocks, q_block_size, k_block_size\n",
        "    return torch.einsum('qkQd,qkdK->qkQK', a, b)\n",
        "\n",
        "\n",
        "num_q_blocks, num_k_blocks, block_q, block_k, d = 2, 3, 4, 5, 8\n",
        "a = torch.randn(num_q_blocks, num_k_blocks, block_q, d)\n",
        "b = torch.randn(num_q_blocks, num_k_blocks, d, block_k)\n",
        "out_spec = torch.zeros(num_q_blocks, num_k_blocks, block_q, block_k)\n",
        "\n",
        "block_matmul_spec(a, b, out_spec)\n",
        "your_out = block_matmul(a, b)\n",
        "\n",
        "assert torch.allclose(your_out, out_spec, atol=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8NY9pZAoXLP",
        "outputId": "3d38f144-b8ce-4c7d-fbe2-a97289a07e98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_matmul_spec took 0.020145 seconds\n",
            "block_matmul took 0.000239 seconds\n"
          ]
        }
      ]
    }
  ]
}