{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOq5prBD1rKwdeQJpSOJ7L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CxvPdfG0zEak"
      },
      "outputs": [],
      "source": [
        "# Implements several common decoding algorithms such as top-k and top-p (nucleus sampling)\n",
        "\n",
        "import torch\n",
        "from transformers.generation import TopKLogitsWarper, TopPLogitsWarper, TemperatureLogitsWarper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k(logits, k: int, temperature: float, generator: torch._C.Generator = None):\n",
        "  '''\n",
        "  Step 1. Retrieve the top k logits\n",
        "  Step 2. Convert logits to probs\n",
        "  Step 3. Sample from top k probs (output shape: batch_size, where each element is in range(0, k))\n",
        "  Step 4. Return top id for each sample in the batch (need to re-align to range(0, vocab_size))\n",
        "  '''\n",
        "\n",
        "  batch_size, vocab_size = logits.shape\n",
        "  k = min(k, vocab_size)\n",
        "\n",
        "  if temperature <= 1e-8: # deterministic sampling\n",
        "    return logits.argmax(dim=-1) # batch_size\n",
        "\n",
        "  top_k_logits, top_k_indices = torch.topk(logits / temperature, k) # batch_size, k\n",
        "\n",
        "  top_k_probs = torch.softmax(top_k_logits, dim=-1) # high temperature indicates more randomness\n",
        "  top_sample_indices = torch.multinomial(top_k_probs, num_samples=1, generator=generator) # (batch_size, 1) - use torch.rand for uniform sampling\n",
        "\n",
        "  # Now we have the top sample indices per sample.\n",
        "  # We would like to match this with respect to the original top k indices (which maintains order relative to logits)\n",
        "  # Gather:\n",
        "  # y[i, j] = x[ idx[i, j], j ] , for all i, j in idx. dim = 0\n",
        "  # y[i, j] = x[ i, idx[i, j] ] , for all i, j in idx. dim = 1\n",
        "  token_idx = torch.gather(top_k_indices, dim=1, index=top_sample_indices).squeeze(1) # batch_size\n",
        "  return token_idx # token idx in this case refers to the index in vocab_size which is equivalent to the actual token id\n"
      ],
      "metadata": {
        "id": "DP3KzwPx12q4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_p(logits: torch.Tensor, p: float, temperature: float, generator: torch._C.Generator = None):\n",
        "  '''\n",
        "  Nucleus sampling where the goal is to pick the smallest subset of logits that is greater than or equal to p\n",
        "  Step 1. Sort logits\n",
        "  Step 2. Convert to probabilities\n",
        "  Step 3. Find the smallest subset s.t. the sum >= p\n",
        "  Step 4. Retrieve the indices that belong to that subset\n",
        "  '''\n",
        "  batch_size, vocab_size = logits.shape\n",
        "\n",
        "  if temperature <= 1e-8: # deterministic sampling\n",
        "    return logits.argmax(dim=-1) # batch_size\n",
        "\n",
        "  # Step 1. Sort logits\n",
        "  sorted_logits, sorted_idx = (logits / temperature).sort(dim=-1, descending=True) # batch_size, vocab_size ; O(nlogn)\n",
        "\n",
        "  # Step 2. Convert to probabilities\n",
        "  sorted_probs = torch.softmax(sorted_logits, dim=-1) # batch_size, vocab_size. higher temperature more uniform aka random\n",
        "\n",
        "  # Step 3. Find the smallest subset s.t. cdf â‰¥ p\n",
        "  # Let's illustrate with an example\n",
        "  # probs: [0.5, 0.3, 0.2] , p = 0.9\n",
        "  # cumsum = [0.5, 0.8, 1.0] # Note: higher p leads to more uniform sampling while lower values of p leads to a more greedy sampling\n",
        "  # mask (if current value less than p? if so, include that as part of sampling) = [True, True, False]. Only sample from index [0, 1]\n",
        "  cdf = torch.cumsum(sorted_probs, dim=-1) # batch_size, vocab_size\n",
        "\n",
        "  counts = (cdf < p).sum(dim=-1) + 1 # count-based indexing. shape: batch_size, k where k is in range(0, vocab_size)\n",
        "  # NOTE: why not do just cdf <= p? Because the rule is that we need to find the smallest set that is greater than or equal to p.\n",
        "  # Suppose you have cdf as [0.5, 0.8, 1.0] and p = 0.7\n",
        "  # The smallest subset that is at greater than or equal to p=0.7 is [0.5, 0.8]. But if we did (cdf <= p) we'd get [0.5]\n",
        "\n",
        "  # We now need to conver this into a mask because as it stands right now the tensor is imbalanced\n",
        "  arange = torch.arange(0, vocab_size).unsqueeze(0) # (1, vocab_size) ; alternative do: torch.arange(0, vocab_size).expand(batch_size, vocab_size)\n",
        "  mask = arange < counts.unsqueeze(-1) # batch_size, vocab_size\n",
        "\n",
        "  top_probs = sorted_probs * mask.float()\n",
        "  denom = top_probs.sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
        "  normalized_top_probs = top_probs / denom # batch_size, vocab_size\n",
        "\n",
        "  # Sample from top probabilites. Note: because we make use of multinomial probabilities we do not need to worry about actually removing samples that are close to 0\n",
        "  top_sample_indices = torch.multinomial(normalized_top_probs, num_samples=1, generator=generator) # batch_size, 1\n",
        "\n",
        "  # Step 4. Retrieve the indices that belong to that subset\n",
        "\n",
        "  # Because top_sample_indices is a position in normalized_top_probs we need to align it back to sorted_idx\n",
        "  # Rule of thumb of when to use gather. X.shape = m, n and idx.shape = m. Use gather.\n",
        "  token_id = torch.gather(sorted_idx, dim=1, index=top_sample_indices).squeeze(1) # batch_size\n",
        "  return token_id # this represents the top token id since token index in range vocab_size is equivalent to the index it belongs to\n"
      ],
      "metadata": {
        "id": "baNBlxQMkYig"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verification(logits, **kwargs):\n",
        "    '''\n",
        "    Apply HuggingFace's top-k or top-p filtering and return sampled tokens.\n",
        "    logits.shape = batch_size, vocab_size\n",
        "    kwargs: Either {'p': float, 'temperature': float} for top-p or {'k': int, 'temperature': float} for top-k\n",
        "    '''\n",
        "    batch_size = logits.shape[0]\n",
        "\n",
        "    # Create dummy input_ids for HF warpers\n",
        "    input_ids = torch.zeros((batch_size, 1), dtype=torch.long)\n",
        "\n",
        "    # Apply temperature\n",
        "    temp_warper = TemperatureLogitsWarper(kwargs['temperature'])\n",
        "    logits = temp_warper(input_ids, logits)\n",
        "\n",
        "    # Apply top-p or top-k\n",
        "    if 'p' in kwargs:\n",
        "        topp_warper = TopPLogitsWarper(top_p=kwargs['p'])\n",
        "        logits = topp_warper(input_ids, logits)\n",
        "    elif 'k' in kwargs:\n",
        "        topk_warper = TopKLogitsWarper(top_k=kwargs['k'])\n",
        "        logits = topk_warper(input_ids, logits)\n",
        "\n",
        "    # Convert to probabilities\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Sample tokens\n",
        "    tokens = torch.multinomial(probs, num_samples=1, generator=kwargs['generator']).squeeze(-1)  # Shape: [batch_size]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "dSoOaWzcfB1m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, vocab_size = 10, 50_000\n",
        "k = 10\n",
        "p = 0.8\n",
        "temperature = 1e-2 # Note: because huggingface implementation has few differences than my version, let's reduce temperature to sample more greedily\n",
        "\n",
        "# Seed for reproducibility\n",
        "gen = lambda: torch.Generator().manual_seed(314159)\n",
        "accuracy = lambda gt, pred: round(((gt == pred).sum() / len(gt)).item(), 3)\n",
        "\n",
        "logits = torch.randn(batch_size, vocab_size, generator=gen()) # Normal distribution: 65% of data from [-1, 1], 95% from [-2, 2], and 99.7% from [-3, 3]"
      ],
      "metadata": {
        "id": "NKIfFdJJfD7V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_top_p_samples = verification(logits, temperature=temperature, p=p, generator=gen())\n",
        "pred_top_p_samples = get_top_p(logits, p=p, temperature=temperature, generator=gen())\n",
        "print(\"Accuracy of Top-p with official implementation:\", accuracy(gt_top_p_samples, pred_top_p_samples))\n",
        "\n",
        "\n",
        "gt_top_k_samples = verification(logits, temperature=temperature, k=k, generator=gen())\n",
        "pred_top_k_samples = get_top_k(logits, k=k, temperature=temperature, generator=gen())\n",
        "print(\"Accuracy of Top-k with official implementation:\", accuracy(gt_top_k_samples, pred_top_k_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC233InOi61E",
        "outputId": "98837722-e7f5-44f9-dc65-f1a9db66f806"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Top-p with official implementation: 1.0\n",
            "Accuracy of Top-k with official implementation: 1.0\n"
          ]
        }
      ]
    }
  ]
}