{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9jtXyyK687sfbZ6I/YoUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/simple-transformer/blob/main/transformers/attention_variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d_xpwFKWtq5X"
      },
      "outputs": [],
      "source": [
        "### This notebook implements several attention variants\n",
        "\n",
        "# Variants:\n",
        "# 1. SingleQuery Dot Attention - 2m\n",
        "# 2. SHA - 5m\n",
        "# 3. MHA - 8m (with causal + attention mask)\n",
        "# 4. SingleQuery MHA - 10m\n",
        "# 5. MQA - 7m\n",
        "# 6. SingleQuery MQA - 10m\n",
        "# 7. GQA - 15m\n",
        "# 8. Sliding Window Attention - 7m\n",
        "# 9. Custom Hand-Rolled QKV attention - 10m\n",
        "# 10. RoPE - 9m"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "GgCpQsNHtq5X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Constants\n",
        "batch_size, seq_len, d_embed, max_seq_len, num_heads = 4, 1024, 128, 4096, 16\n",
        "\n",
        "attention_mask = torch.arange(seq_len)[None, :] < torch.randint(low=1, high=(seq_len+1), size=(batch_size, seq_len))\n",
        "X = torch.randn(batch_size, seq_len, d_embed)\n",
        "QUERY = torch.rand(batch_size, d_embed)"
      ],
      "metadata": {
        "id": "n0TdvN3Ctq5X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleQueryDotAttention(nn.Module):\n",
        "  '''\n",
        "  From https://arxiv.org/pdf/1911.02150, section 2.1\n",
        "  Attention(q, K, V) = âˆ‘ alpha_i * V_i, where alpha_i = softmax(qKi)\n",
        "  '''\n",
        "  def __init__(self, d_embed: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = nn.Linear(d_embed, d_embed)\n",
        "    self.v = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, query: torch.Tensor, x:torch.Tensor) -> torch.Tensor:\n",
        "    # query: [batch_size, d_embed]\n",
        "    # x: [batch_size, seq_len, d_embed]\n",
        "\n",
        "    K = self.k(x) # batch_size, k_seq_len, d_k\n",
        "    V = self.v(x) # batch_size, v_seq_len, d_v\n",
        "\n",
        "    # Attention\n",
        "    logits = torch.einsum('bd,bmd->bm', query, K) # dot product\n",
        "    scores = torch.softmax(logits, dim=-1)\n",
        "    out = torch.einsum('bm,bmd->bd', scores, V)\n",
        "    return out"
      ],
      "metadata": {
        "id": "M1koE4kftq5Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_sqda = SingleQueryDotAttention(d_embed)\n",
        "assert m_sqda(QUERY, X).shape == (batch_size, d_embed)"
      ],
      "metadata": {
        "id": "wTT8xaxQtq5Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SHA(nn.Module):\n",
        "  def __init__(self, d_embed: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.qkv = nn.Linear(d_embed, d_embed * 3)\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    Time complexity: O(bsd^2 + bs^2d), where s = seq_len, d = d_embed, b = batch_size\n",
        "    Space complexity: O(bsd + bs^2)\n",
        "    '''\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "    # How to calculate time complexity? (m x k) @ (k x n) = O(m x k x n)\n",
        "    # How to calculate space complexity? Output shape of resulting tensor: (m x k) @ (k x n) = O(m x n)\n",
        "    # (s x d) x (d x 3d) -> O(bsd^2), batch needs to be multiplied\n",
        "    Q, K, V = rearrange(self.qkv(x), 'batch_size seq_len (d_embed three) -> three batch_size seq_len d_embed', three=3).unbind(0)\n",
        "\n",
        "    # Attention\n",
        "    scale = 1 / (d_embed ** 0.5)\n",
        "    # (s x d) x (s x d) = O(b s^2 d)\n",
        "    weight = torch.einsum('bqd,bkd -> bqk', Q, K) * scale\n",
        "    score = torch.softmax(weight, dim=-1) # apply softmax across the key dimension\n",
        "    # (s x s) x (s x d) -> O(bs^2d), same as above, so ignore\n",
        "    attention = torch.einsum('bqk,bkd->bqd', score, V)\n",
        "    # (s x d) x (d x s) => O(bsd^2), same as above, so ignore\n",
        "    out = self.w_out(attention) # batch_size, seq_len_q, d_embed\n",
        "    return out"
      ],
      "metadata": {
        "id": "aPXfHcLntq5Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_sha = SHA(d_embed)\n",
        "assert m_sha(X).shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "J5OXVKcdtq5Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "  def __init__(self, d_embed: int, num_heads: int, max_seq_len: int):\n",
        "    super().__init__()\n",
        "    assert d_embed % num_heads == 0, f\"d_embed needs to be divisible by number of heads\"\n",
        "\n",
        "    self.head_dim = d_embed // num_heads\n",
        "\n",
        "    self.q = nn.Linear(d_embed, d_embed)\n",
        "    self.k = nn.Linear(d_embed, d_embed)\n",
        "    self.v = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "    causal_mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
        "    self.register_buffer('mask', causal_mask)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    '''\n",
        "    Time complexity: O(bsd^2 + bs^2d), where s = seq_len, d = d_embed, b = batch_size\n",
        "    Space complexity: O(bsd + bs^2)\n",
        "    '''\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "\n",
        "    Q = rearrange(self.q(x), 'batch_size seq_len (num_heads head_dim) -> batch_size num_heads seq_len head_dim', seq_len=seq_len, head_dim=self.head_dim)\n",
        "    K = rearrange(self.k(x), 'batch_size seq_len (num_heads head_dim) -> batch_size num_heads seq_len head_dim', seq_len=seq_len, head_dim=self.head_dim)\n",
        "    V = rearrange(self.v(x), 'batch_size seq_len (num_heads head_dim) -> batch_size num_heads seq_len head_dim', seq_len=seq_len, head_dim=self.head_dim)\n",
        "\n",
        "    # Compute attention\n",
        "    weight = torch.einsum('bhqd,bhkd->bhqk', Q, K) / (self.head_dim ** 0.5)\n",
        "    # apply causal mask\n",
        "    mask = self.mask[:seq_len, :seq_len][None, None, :, :] # (1, 1, seq_len, seq_len)\n",
        "    if attention_mask is not None: # apply pad mask by combining it with causal mask\n",
        "      # 1 = real, 0 = <pad>\n",
        "      # we specifically want to mask out pad tokens, so first need to invert it and then OR it with our existing mask\n",
        "      # batch_size, seq_len. move to batch_size, 1, 1, seq_len\n",
        "      mask = mask | ~attention_mask[:, None, None, :].to(x.device)\n",
        "    weight = weight.masked_fill(mask, float('-inf'))\n",
        "    score = torch.softmax(weight, dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bhqk,bhkd->bhqd', score, V)\n",
        "    attention = rearrange(attention, 'batch_size num_heads seq_len head_dim -> batch_size seq_len (num_heads head_dim)')\n",
        "    out = self.w_out(attention)\n",
        "    return out"
      ],
      "metadata": {
        "id": "X-RlhXQktq5Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_mha = MHA(d_embed, num_heads=num_heads, max_seq_len=max_seq_len)\n",
        "assert m_mha(X, attention_mask).shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "MmJdo_H9tq5Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleQueryMHA(nn.Module):\n",
        "  '''\n",
        "  From https://arxiv.org/pdf/1911.02150, section 2.4\n",
        "  '''\n",
        "  def __init__(self, d_embed: int, num_heads: int):\n",
        "    super().__init__()\n",
        "    assert d_embed % num_heads == 0\n",
        "    self.head_dim = d_embed // num_heads\n",
        "\n",
        "    self.q = nn.Linear(d_embed, d_embed)\n",
        "    self.k = nn.Linear(d_embed, d_embed)\n",
        "    self.v = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, x:torch.Tensor, prev_k, prev_v) -> torch.Tensor:\n",
        "    # x: [batch_size, d_embed], (b, d)\n",
        "    # Note: d = n * h\n",
        "    # prev_k: [batch_size, num_heads, seq_len, head_dim], (b, n, m, h)\n",
        "    # prev_v: [batch_size, num_heads, seq_len, head_dim], (b, n, m, h)\n",
        "\n",
        "    # out: [batch_size, num_heads, seq_len + 1, head_dim], (b, n, m+1, h)\n",
        "    b, d = x.shape\n",
        "    _, n, m, h = prev_k.shape\n",
        "\n",
        "    Q = rearrange(self.q(x), 'b (n h) -> b n h', n=n, h=h) # b, n, h\n",
        "    K = rearrange(self.k(x), 'b (n h) -> b n 1 h', n=n, h=h) # b, n, 1, h\n",
        "    V = rearrange(self.v(x), 'b (n h) -> b n 1 h', n=n, h=h) # b, n, 1, h\n",
        "\n",
        "    K = torch.concat((K, prev_k), dim=2) # b, n, m+1, h\n",
        "    V = torch.concat((V, prev_v), dim=2) # b, n, m+1, h\n",
        "\n",
        "    # Attention\n",
        "    logits = torch.einsum('bnh,bnmh -> bnm', Q, K) / (self.head_dim ** 0.5)\n",
        "    scores = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bnm,bnmh->bnh', scores, V) # b, n, h\n",
        "    attention = rearrange(attention, 'b n h -> b (n h)', n=n, h=h) # b, (n, h)=d\n",
        "\n",
        "    out = self.w_out(attention) # b, d\n",
        "    return out, K, V"
      ],
      "metadata": {
        "id": "KYbDhYB0tq5Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_sqmha = SingleQueryMHA(d_embed, num_heads=num_heads)\n",
        "prev_k, prev_v = torch.rand(2, batch_size, num_heads, seq_len, m_sqmha.head_dim)\n",
        "\n",
        "out = m_sqmha(QUERY, prev_k, prev_v)\n",
        "\n",
        "assert out[0].shape == (batch_size, d_embed)\n",
        "assert out[1].shape == out[2].shape == (batch_size, 16, seq_len + 1, m_sqmha.head_dim)"
      ],
      "metadata": {
        "id": "g_OnE-XYtq5Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MQA(nn.Module):\n",
        "  def __init__(self, d_embed: int, num_heads: int):\n",
        "    '''\n",
        "    Introduced in Section 3 of https://arxiv.org/pdf/1911.02150\n",
        "    '''\n",
        "    super().__init__()\n",
        "    assert d_embed % num_heads == 0\n",
        "    self.head_dim = d_embed // num_heads\n",
        "\n",
        "    self.q = nn.Linear(d_embed, d_embed)\n",
        "    self.k = nn.Linear(d_embed, self.head_dim)\n",
        "    self.v = nn.Linear(d_embed, self.head_dim)\n",
        "\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    Identical to MHA except different heads all share a single set of keys and values\n",
        "    '''\n",
        "    # x: [batch_size, seq_len, d_embed]\n",
        "    # out = [batch_size, d_embed]\n",
        "    b, m, d = x.shape\n",
        "\n",
        "    Q = rearrange(self.q(x), 'b m (n h) -> b m n h', b=b, m=m, h=self.head_dim) # batch_size, seq_len, num_heads, head_dim\n",
        "    K = self.k(x) # batch_size, seq_len, head_dim\n",
        "    V = self.v(x) # batch_size, seq_len, head_dim\n",
        "\n",
        "    # Attention\n",
        "    logits = torch.einsum('bqnh,bmh -> bnqm', Q, K) / (self.head_dim ** 0.5) # batch_size, num_heads, q-seq_len, k-seq_len\n",
        "    weights = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bnqm,bmh -> bqnh', weights, V).flatten(-2, -1) # batch_size, seq_len, (num_heads, head_dim)\n",
        "    out = self.w_out(attention) # batch_size, seq_len, d_embed\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "UYsn9Vojtq5Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_mqa = MQA(d_embed, num_heads=num_heads)\n",
        "out = m_mqa(X)\n",
        "assert out.shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "DrNdWPWS2Xyh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleQueryMQA(nn.Module):\n",
        "  def __init__(self, d_embed: int, num_heads: int):\n",
        "    '''\n",
        "    Combination of SingleQuery MHA and MQA. Introduced in https://arxiv.org/pdf/1911.02150\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "    assert d_embed % num_heads == 0\n",
        "    self.head_dim = d_embed // num_heads\n",
        "\n",
        "    self.q = nn.Linear(d_embed, d_embed)\n",
        "    self.k = nn.Linear(d_embed, self.head_dim)\n",
        "    self.v = nn.Linear(d_embed, self.head_dim)\n",
        "\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, prev_k: torch.Tensor, prev_v: torch.Tensor) -> torch.Tensor:\n",
        "    # x: [batch_size, d_embed], b, d\n",
        "    # prev_k: [batch_size, seq_len, head_dim], b, m, h\n",
        "    # prev_v: [batch_size, seq_len, head_dim], b, m, h\n",
        "\n",
        "    # out_attention = [batch_size, d_embed]\n",
        "    # k: [batch_size, seq_len + 1, head_dim]\n",
        "    # v: [batch_size, seq_len + 1, head_dim]\n",
        "\n",
        "    b, d = x.shape\n",
        "    _, m, h = prev_k.shape # note: head_dim = dk = dv\n",
        "\n",
        "    Q = rearrange(self.q(x), 'b (n h) -> b n h', h=self.head_dim) # b, n h\n",
        "    K = self.k(x).unsqueeze(1) # b, 1, h\n",
        "    V = self.v(x).unsqueeze(1) # b, 1, h\n",
        "\n",
        "    K = torch.concat((prev_k, K), dim=1) # b, m+1, h\n",
        "    V = torch.concat((prev_v, V), dim=1) # b, m+1, h\n",
        "\n",
        "\n",
        "    # Attention\n",
        "    logits = torch.einsum('bnh,bmh -> bnm', Q, K) / (self.head_dim ** 0.5) # batch_size, num_heads, seq_len\n",
        "    scores = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bnm,bmh -> bnh', scores, V).flatten(1, 2) # batch_size, (num_heads, head_dim)\n",
        "    out = self.w_out(attention)\n",
        "\n",
        "    return out, K, V\n"
      ],
      "metadata": {
        "id": "ag5QOV3P3G96"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_sqmqa = SingleQueryMQA(d_embed, num_heads=num_heads)\n",
        "prev_k, prev_v = torch.rand(2, batch_size, seq_len, m_sqmha.head_dim)\n",
        "\n",
        "out = m_sqmqa(QUERY, prev_k, prev_v)\n",
        "\n",
        "assert out[0].shape == (batch_size, d_embed)\n",
        "assert out[1].shape == out[2].shape == (batch_size, seq_len + 1, m_sqmha.head_dim)"
      ],
      "metadata": {
        "id": "qZ30PbQH-urQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GQA(nn.Module):\n",
        "  def __init__(self, d_embed: int, num_heads: int, num_groups: int):\n",
        "    '''\n",
        "    Balance between MQA and MHA.\n",
        "    If GQA(num_groups=1), MQA.\n",
        "    If GQA(num_groups=num_heads), MHA\n",
        "    '''\n",
        "    super().__init__()\n",
        "    assert d_embed % num_heads == 0\n",
        "    self.head_dim = d_embed // num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.num_groups = num_groups\n",
        "\n",
        "    self.q = nn.Linear(d_embed, d_embed)\n",
        "    self.k = nn.Linear(d_embed, num_groups * self.head_dim)\n",
        "    self.v = nn.Linear(d_embed, num_groups * self.head_dim)\n",
        "\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    # x: [batch_size, seq_len, d_embed]\n",
        "\n",
        "    b, m, d = x.shape\n",
        "    num_repeat = self.num_heads // self.num_groups\n",
        "\n",
        "    Q = rearrange(self.q(x), 'b m (n h) -> b n m h', h=self.head_dim, n=self.num_heads) # b, n, m, h\n",
        "    # repeat: how many times you want to repeat per axis?\n",
        "    # expand: what should the final shape look like? Note it works correctly only when size is 1 for that dim.\n",
        "    # otherwise, use repeat or create a new dimension and then reshape later\n",
        "    K = rearrange(self.k(x), 'b m (g h) -> b 1 g m h', h=self.head_dim, g=self.num_groups).expand(b, num_repeat, self.num_groups, m, self.head_dim).flatten(1, 2) # b, g=n, m, h\n",
        "    # Alternative way of doing it, memory intensive since repeats copies it over unlike expand which creates a .view\n",
        "    V = rearrange(self.v(x), 'b m (g h) -> b g m h', h=self.head_dim, g=self.num_groups).repeat(1, num_repeat, 1, 1) # b, g=n, m, h\n",
        "\n",
        "    # Attention\n",
        "    logits = torch.einsum('bnqh,bnkh -> bnqk', Q, K) # b, n, q-seq_len, k-seq_len\n",
        "    scores = torch.softmax(logits / (self.head_dim ** 0.5), dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bnqk,bnkh -> bnqh', scores, V)\n",
        "    attention = rearrange(attention, 'b n m h -> b m (n h)') # b, m, d\n",
        "\n",
        "    out = self.w_out(attention)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "w28W5PlR_CN2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_gqa = GQA(d_embed, num_heads=num_heads, num_groups=2)\n",
        "out = m_gqa(X)\n",
        "assert out.shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "cK6tXXRCXkN1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindowAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_embed: int, window: int):\n",
        "    ''' SHA version that implements sliding window. Trivial to extend to MHA / other variants '''\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(d_embed, d_embed)\n",
        "    self.k = nn.Linear(d_embed, d_embed)\n",
        "    self.v = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "    self.window = window\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    b, m, d = x.shape # batch_size, seq_len, d_embed\n",
        "\n",
        "    Q = self.q(x) # b, m, d\n",
        "    K = self.k(x) # b, m, d\n",
        "    V = self.v(x) # b, m, d\n",
        "\n",
        "    base = torch.ones(m, m)\n",
        "    # diagonal shifts everything post the main diagonal by k units. triu sets everything after diagonal + k as 1\n",
        "    # and triu sets everything diagonal - k as 0. So the span between the window is marked as 1s, everything else is zeros.\n",
        "    mask = torch.triu(base, diagonal=(self.window+1)).bool() | torch.tril(base, diagonal=-self.window).bool()\n",
        "\n",
        "    # Attention (compute rest as-is)\n",
        "    logits = torch.einsum('bqd,bkd -> bqk', Q, K) / (d ** 0.5)\n",
        "    scores = logits.masked_fill(mask, float('-inf'))\n",
        "    scores = torch.softmax(scores, dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bqk,bkd -> bqd', scores, V)\n",
        "    out = self.w_out(attention)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ONMt03S0eu21"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_swa = SlidingWindowAttention(d_embed, window=5)\n",
        "out = m_swa(X)\n",
        "assert out.shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "3Kmr4lCpgfKQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a custom Hand-rolled QKV Attention with Custom Scaling\n",
        "\n",
        "# Problem: Implement a QKV attention module from scratch in PyTorch without using built-in attention functions. For input X (shape [batch_size, seq_len, d_model]),\n",
        "# project to Q, K, V (each [batch_size, seq_len, d_k]). Compute attention as Attn = softmax( (Q @ K^T / sqrt(d_k)) + scaling_matrix ),\n",
        "# where scaling_matrix_{i,j} = log(1 + |i - j|), then output = Attn @ V. Use einsum for all matrix operations to ensure efficiency.\n",
        "\n",
        "# Include a forward pass test with dummy data.\n",
        "\n",
        "# Follow-up ML questions:\n",
        "# 1. Explain why the sqrt(d_k) scaling is typically used in standard attention, and how the added log(1 + |i - j|) might affect long-range dependencies.\n",
        "# Answer: as context window increases, the logits for long-term dependencies will get quite large. this teaches the model to attend to tokens farther apart a lot more\n",
        "#         than tokens that are closer together. Additionally, this will cause an unbalance shift in logits and potentially nullifying the impact of d_k scaling factor.\n",
        "# 2. If this were part of a multi-layer transformer, what gradients might become unstable during backpropagation, and why?\n",
        "# Answer: gradients from q and k will be unstable because we're apply a relative large scaling factor. This is because the softmax is becoming saturated with large shifts"
      ],
      "metadata": {
        "id": "nwgYP5RjGzo9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HandRolledQKV(nn.Module):\n",
        "\n",
        "  def __init__(self, d_embed: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.qkv = nn.Linear(d_embed, d_embed * 3)\n",
        "    self.w_out = nn.Linear(d_embed, d_embed)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    b, m, d = x.shape # batch_size, seq_len, d_embed\n",
        "\n",
        "    Q, K, V = rearrange(self.qkv(x), 'b m (three d) -> three b m d', three=3).unbind(0)\n",
        "\n",
        "    # Attention\n",
        "    logits = torch.einsum('bqd,bkd -> bqk', Q, K) / (d ** 0.5)\n",
        "    # scaling_matrix[i, j] = log(1 + abs(i - j)). I'm assuming i refers to q-seq_len and j refers to k-seq_len\n",
        "    _, q_seq_len, k_seq_len = logits.shape\n",
        "\n",
        "    # Un-vectorized approach\n",
        "    # scaling_matrix = torch.zeros(q_seq_len, k_seq_len)\n",
        "    # for i in range(q_seq_len):\n",
        "    #   for j in range(k_seq_len):\n",
        "    #     scaling_matrix[i, j] = torch.log(1 + torch.abs(i - j))\n",
        "\n",
        "    # Vectorized approach\n",
        "    q_idx = torch.arange(q_seq_len).unsqueeze(1) # Q, 1\n",
        "    k_idx = torch.arange(k_seq_len).unsqueeze(0) # 1, K\n",
        "    scaling_matrix = torch.log1p(torch.abs(q_idx - k_idx))\n",
        "\n",
        "    score = torch.softmax(logits + scaling_matrix, dim=-1)\n",
        "\n",
        "    attention = torch.einsum('bqk,bkd -> bqd', score, V)\n",
        "\n",
        "    out = self.w_out(attention)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "QFLMuMAaG8R-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_customQKV = HandRolledQKV(d_embed)\n",
        "out = m_customQKV(X)\n",
        "assert out.shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "x6U8EqQUJiMe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPE(nn.Module):\n",
        "\n",
        "  def __init__(self, d_embed: int):\n",
        "    super().__init__()\n",
        "\n",
        "    idx = torch.arange(d_embed//2) # range(0, d_embed / 2)\n",
        "    freqs = (1/1e3) ** (idx/d_embed) # 1/10k^idx/d_embed\n",
        "    self.register_buffer('freqs', freqs)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    b, m, d = x.shape # batch_size, seq_len, d_embed\n",
        "\n",
        "    x = rearrange(x, 'b m (d two) -> b m d two', two=2) # b, m, d/2, 2. last dim: first component is reserved for sin, second for cos\n",
        "\n",
        "    positions = torch.arange(m) # seq_len\n",
        "    angles = torch.einsum('m,d->md', positions, self.freqs) # seq_len, d_embed / 2\n",
        "\n",
        "    # Apply the rotation matrix\n",
        "    # [cos x   sin x] [x0] = [x0*cosx  + x1*sinx]\n",
        "    # [-sin x  cos x] [x1]   [-x0*sinx + x1*cosx]\n",
        "\n",
        "    cos = torch.cos(angles).unsqueeze(0) # 1, m, d/2\n",
        "    sin = torch.sin(angles).unsqueeze(0) # 1, m, d/2\n",
        "\n",
        "    rotated_matrix = torch.concat([\n",
        "        cos * x[..., 0] + sin * x[..., 1],\n",
        "        -sin * x[..., 0] + cos * x[..., 1],\n",
        "    ], dim=-1) # b, m, d\n",
        "\n",
        "    return rotated_matrix\n",
        ""
      ],
      "metadata": {
        "id": "1dGVAYoVJsfi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_rope = RoPE(d_embed)\n",
        "out = m_rope(X)\n",
        "\n",
        "assert out.shape == (batch_size, seq_len, d_embed)"
      ],
      "metadata": {
        "id": "wmZcMNXxX_kj"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}